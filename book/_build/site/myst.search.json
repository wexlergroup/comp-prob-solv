{"version":"1","records":[{"hierarchy":{"lvl1":"Welcome to Computational Problem Solving in the Chemical Sciences"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Welcome to Computational Problem Solving in the Chemical Sciences"},"content":"Computational Problem Solving in the Chemical Sciences\n\nPart 1. Introduction to Python Computations\n\nChapter 1: Introduction to Python for the Chemical Sciences\n\nChapter 2: Essential Python Packages for the Chemical Sciences\n\nChapter 3: Control Structures in Python\n\nPart 2. Numerical Methods in Python\n\nChapter 4: Chemical Reaction Equilibria and Roots of Equations\n\nChapter 5: Chemical Bonding and Numerical Integration\n\nChapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations\n\nPart 3. Statistics — Regression and Correlation\n\nChapter 7: Orders of Reaction and Linear Regression Analysis\n\nChapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis\n\nPart 4. Introduction to Molecular Simulations\n\nChapter 9: Classical Thermodynamics\n\nChapter 10: Statistical Thermodynamics\n\nChapter 11: Ensembles and Ergodicity\n\nPart 5. Monte Carlo Simulations\n\nChapter 12: The Monte Carlo Method\n\nChapter 13: Monte Carlo Integration\n\nChapter 14: A Basic Monte Carlo Algorithm\n\nChapter 15: Nanoparticle Shape and Simulated Annealing\n\nChapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.\n\nChapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations\n\nChapter 18: Monte Carlo Simulations of Adsorption on Surfaces\n\nProject 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption\n\nPart 6. Molecular Dynamics Simulations\n\nChapter 19: Molecular Dynamics\n\nChapter 20: Verlet Integration\n\nChapter 21: Thermostatting\n\nChapter 22: Atomic Simulation Environment (ASE)\n\nChapter 23: Radial Distribution Function\n\nProject 2: Molecular Dynamics Simulations of a Polymer Chain\n\nPart 7. Advanced and Emerging Topics in Computational Chemistry\n\nChapter 24: Newton-Raphson Method\n\nChapter 25: Kinetic Monte Carlo\n\nChapter 26: scikit-learn\n\nChapter 27: Nested Sampling","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences"},"type":"lvl1","url":"/lecture-01-introduction","position":0},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences"},"content":"Welcome to Computational Problem Solving in the Chemical Sciences! This course is designed to equip you with the skills to tackle a wide range of problems in chemistry, chemical engineering, chemical physics, chemical biology, and beyond, using modern numerical tools. While we will primarily focus on Python—a language highly valued in today’s workforce—it’s important to recognize that there are many other tools at your disposal for solving problems in the chemical sciences.\n\nThese tools range from spreadsheet software like Excel and Google Sheets to specialized software packages with graphical user interfaces, such as Origin, Mathematica, and MATLAB. Additionally, other programming languages like R, Julia, and C++ are often employed for various scientific computing tasks.\n\nAs we progress through the course, you’ll not only learn the fundamentals of Python but also how to apply it to simulate matter at different scales, including techniques like Monte Carlo and molecular dynamics simulations. We’ll also touch upon Python’s use in quantum chemistry calculations, demonstrating its versatility and power across diverse areas of research.\n\nBy the end of this course, you’ll have a solid foundation in using Python to solve complex problems in the chemical sciences, preparing you for both academic research and industry applications.","type":"content","url":"/lecture-01-introduction","position":1},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-01-introduction#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to:\n\nInstall Python and Jupyter Notebook on your computer.\n\nPerform basic mathematical operations in Python.\n\nUse Python to solve chemical problems, such as calculating molar masses and the number of moles.\n\nCreate basic plots using Python’s matplotlib library.","type":"content","url":"/lecture-01-introduction#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl2":"Step 1: Getting Python Installed"},"type":"lvl2","url":"/lecture-01-introduction#step-1-getting-python-installed","position":4},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl2":"Step 1: Getting Python Installed"},"content":"Before we dive into using Python, you’ll need to have it installed on your computer. Python is available across all major operating systems, and installation is straightforward. Here’s how you can get started:","type":"content","url":"/lecture-01-introduction#step-1-getting-python-installed","position":5},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"1.1 Download and Install Python","lvl2":"Step 1: Getting Python Installed"},"type":"lvl3","url":"/lecture-01-introduction#id-1-1-download-and-install-python","position":6},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"1.1 Download and Install Python","lvl2":"Step 1: Getting Python Installed"},"content":"Visit the official \n\nPython website and download the latest version of Python 3 (we recommend version 3.8 or later).\n\nFollow the installation instructions specific to your operating system.","type":"content","url":"/lecture-01-introduction#id-1-1-download-and-install-python","position":7},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"1.2 Check if Python is Already Installed","lvl2":"Step 1: Getting Python Installed"},"type":"lvl3","url":"/lecture-01-introduction#id-1-2-check-if-python-is-already-installed","position":8},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"1.2 Check if Python is Already Installed","lvl2":"Step 1: Getting Python Installed"},"content":"Mac and Linux Users: Python typically comes pre-installed. To check, open a terminal and type:python3 --version\n\nIf a version number appears, you’re all set. If not, you’ll need to install Python from the website above.\n\nWindows Users: Open a command prompt and type:python --version\n\nIf you see a version number, Python is installed. If you get an error, you’ll need to install Python.","type":"content","url":"/lecture-01-introduction#id-1-2-check-if-python-is-already-installed","position":9},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"1.3 Windows-Specific Note","lvl2":"Step 1: Getting Python Installed"},"type":"lvl3","url":"/lecture-01-introduction#id-1-3-windows-specific-note","position":10},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"1.3 Windows-Specific Note","lvl2":"Step 1: Getting Python Installed"},"content":"During installation on Windows, make sure to check the box that says “Add Python to PATH.” This will make Python accessible from the command line.\n\nOnce Python is installed and verified, you’re ready to start coding!","type":"content","url":"/lecture-01-introduction#id-1-3-windows-specific-note","position":11},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl2":"Step 2: Installing Jupyter Notebook"},"type":"lvl2","url":"/lecture-01-introduction#step-2-installing-jupyter-notebook","position":12},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl2":"Step 2: Installing Jupyter Notebook"},"content":"Next, you’ll need to install Jupyter Notebook, a powerful tool that allows you to create and share documents containing live code, equations, visualizations, and narrative text.","type":"content","url":"/lecture-01-introduction#step-2-installing-jupyter-notebook","position":13},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"2.1 Install Jupyter Notebook","lvl2":"Step 2: Installing Jupyter Notebook"},"type":"lvl3","url":"/lecture-01-introduction#id-2-1-install-jupyter-notebook","position":14},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"2.1 Install Jupyter Notebook","lvl2":"Step 2: Installing Jupyter Notebook"},"content":"For Mac and Linux Users: Open a terminal and type:pip3 install jupyter\n\nIf pip3 doesn’t work, try:python3 -m pip install jupyter\n\nFor Windows Users: Open a command prompt and type:python -m pip install jupyter\n\nThis command ensures that Jupyter is installed using the version of Python you have on your system.","type":"content","url":"/lecture-01-introduction#id-2-1-install-jupyter-notebook","position":15},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"2.2 Launching Jupyter Notebook","lvl2":"Step 2: Installing Jupyter Notebook"},"type":"lvl3","url":"/lecture-01-introduction#id-2-2-launching-jupyter-notebook","position":16},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"2.2 Launching Jupyter Notebook","lvl2":"Step 2: Installing Jupyter Notebook"},"content":"After installation, you can start Jupyter Notebook by typing the following command in your terminal or command prompt:jupyter notebook\n\nThis will automatically open a new tab in your default web browser, presenting the Jupyter interface. From here, you can create new notebooks, organize your projects, and start coding!\n\nWith Jupyter Notebook installed, you’re now ready to start exploring Python interactively!","type":"content","url":"/lecture-01-introduction#id-2-2-launching-jupyter-notebook","position":17},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl2":"Step 3: Let’s Get Started with Python"},"type":"lvl2","url":"/lecture-01-introduction#step-3-lets-get-started-with-python","position":18},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl2":"Step 3: Let’s Get Started with Python"},"content":"","type":"content","url":"/lecture-01-introduction#step-3-lets-get-started-with-python","position":19},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"3.1 Python and Mathematics","lvl2":"Step 3: Let’s Get Started with Python"},"type":"lvl3","url":"/lecture-01-introduction#id-3-1-python-and-mathematics","position":20},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"3.1 Python and Mathematics","lvl2":"Step 3: Let’s Get Started with Python"},"content":"Python is a versatile programming language that excels at solving a wide range of problems in the chemical sciences, particularly when it comes to mathematical computations. Whether you’re performing simple arithmetic or tackling more advanced calculus, Python is equipped to handle it all.\n\nLet’s start with some basic math operations in Python:\n\nAddition: You can add two numbers easily:\n\n2 + 2\n\nMultiplication: Multiplication is just as straightforward:\n\n3 * 4\n\nPython supports a variety of mathematical operations, including:\n\nSubtraction: a - b\n\nDivision: a / b\n\nExponentiation: a ** b (which gives a^b)\n\nNote\n\nThe math module is a built-in library in Python. To use it, you must first import it using the import statement. Once imported, you can access various mathematical functions and constants by prefixing them with math. For more details, refer to the \n\nPython documentation.\n\nPython’s capabilities extend far beyond basic arithmetic. For example, to calculate the square root of a number, you can use the math module:\n\nimport math\nmath.sqrt(9)\n\nIn this example, you’ve seen your first instance of importing a module in Python. The import statement allows you to bring additional functionality into your Python programs. In this case, by importing the math module, you gain access to a suite of mathematical tools that go beyond basic operations.\n\nOnce imported, you can use any function from the module by prefixing it with math. For instance, math.sqrt() is the function to compute the square root.","type":"content","url":"/lecture-01-introduction#id-3-1-python-and-mathematics","position":21},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"3.2 Practice Exercises","lvl2":"Step 3: Let’s Get Started with Python"},"type":"lvl3","url":"/lecture-01-introduction#id-3-2-practice-exercises","position":22},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"3.2 Practice Exercises","lvl2":"Step 3: Let’s Get Started with Python"},"content":"Let’s put this into practice with a couple of exercises:\n\nExercise 1:\nCalculate the value of 2^3 + 4^2 using Python.\n\nHint\n\nYou can use the ** operator for exponentiation.\n\nExercise 2:\nCalculate the value of \\sqrt{16} + \\sqrt{25} using Python.\n\nHint\n\nRemember to use the math.sqrt() function for square roots.","type":"content","url":"/lecture-01-introduction#id-3-2-practice-exercises","position":23},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"3.3 Python Can Do Chemistry","lvl2":"Step 3: Let’s Get Started with Python"},"type":"lvl3","url":"/lecture-01-introduction#id-3-3-python-can-do-chemistry","position":24},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"3.3 Python Can Do Chemistry","lvl2":"Step 3: Let’s Get Started with Python"},"content":"Python is not just a powerful tool for mathematics; it can also be used to solve a wide range of problems in chemistry. Whether you’re performing basic stoichiometric calculations or exploring complex quantum chemistry, Python has you covered.\n\nFor example, let’s calculate the molar mass of carbon dioxide (CO_2):\n\ncarbon_mass = 12.011\noxygen_mass = 16.00\nmolar_mass = carbon_mass + 2 * oxygen_mass\nmolar_mass\n\nIn this example, you’ve encountered your first use of variables in Python. A variable is simply a name that refers to a value. You can use variables to store values and perform calculations. In this case, we stored the atomic masses of carbon (carbon_mass) and oxygen (oxygen_mass) as variables, and then used these to calculate the molar mass of CO_2.\n\nPython can also help you determine the number of moles of a compound:\n\nmass = 10.0\nmolar_mass = 44.01\nmoles = mass / molar_mass\nmoles\n\nAs you can see, Python allows you to easily perform all kinds of chemical calculations. Each variable in Python has a data type, which determines the kind of data it can hold. In the above example, mass and molar_mass are of type float, meaning they represent real numbers. The result of the division operation (moles) is also a float.\n\nYou can determine the data type of any variable using the type() function:\n\ntype(mass)\n\n","type":"content","url":"/lecture-01-introduction#id-3-3-python-can-do-chemistry","position":25},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"3.4 Practice Exercises","lvl2":"Step 3: Let’s Get Started with Python"},"type":"lvl3","url":"/lecture-01-introduction#id-3-4-practice-exercises","position":26},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"3.4 Practice Exercises","lvl2":"Step 3: Let’s Get Started with Python"},"content":"Exercise 1:\nCalculate the molar mass of water (H_2O) using Python.\n\nHint\n\nRemember that the atomic masses of hydrogen and oxygen are approximately 1.008 and 16.00, respectively.\n\nExercise 2:\nCalculate the number of moles of water (H_2O) in 100.0 grams of the compound using Python.\n\nHint\n\nUse the molar mass you calculated in the previous exercise.","type":"content","url":"/lecture-01-introduction#id-3-4-practice-exercises","position":27},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"3.5 Python Can Do Graphing","lvl2":"Step 3: Let’s Get Started with Python"},"type":"lvl3","url":"/lecture-01-introduction#id-3-5-python-can-do-graphing","position":28},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"3.5 Python Can Do Graphing","lvl2":"Step 3: Let’s Get Started with Python"},"content":"Before we start creating plots, you’ll need to ensure that Matplotlib is installed on your computer. Matplotlib is a popular plotting library in Python that you’ll be using frequently for data visualization. To install Matplotlib, open your terminal or command prompt and type the following command:pip install matplotlib\n\nIf you’re using a Jupyter Notebook, you can install Matplotlib directly within the notebook by running:!pip install matplotlib\n\nOnce Matplotlib is installed, you’re ready to start creating plots in Python. Let’s start with a simple example of a line plot:\n\nNote\n\nThe $ symbols around the labels in the xlabel(), ylabel(), and title() functions are used to render the text in LaTeX format, which is commonly used for mathematical expressions. This allows you to include mathematical symbols and equations in your plots. For more information on LaTeX formatting, refer to the \n\nOverleaf documentation.\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [1, 4, 9, 16, 25]\n\nplt.plot(x, y)\n\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title('$y = x^2$')\n\nplt.show()\n\nIn the first line, we encounter a new syntax: import matplotlib.pyplot as plt. This statement means, “import the pyplot module from the matplotlib library and give it the alias plt.” The pyplot module is part of the matplotlib library, which is a powerful tool for creating a wide variety of graphs and plots.\n\nIn this example, we use the plot() function to create a line plot of the function y = x^2. We then use xlabel(), ylabel(), and title() to label the x-axis, y-axis, and add a title to the plot, respectively. Finally, show() is used to display the plot.\n\nPython offers several libraries for creating graphs and plots, each with its own strengths and use cases:\n\nmatplotlib: Great for creating basic plots like line plots, scatter plots, and bar plots.\n\nseaborn: Built on top of matplotlib, it offers enhanced statistical plotting and a more attractive default style.\n\nplotly: Ideal for creating interactive and complex plots, including 3D plots.\n\nYou might find yourself experimenting with different libraries to determine which one best fits your specific needs.","type":"content","url":"/lecture-01-introduction#id-3-5-python-can-do-graphing","position":29},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"3.6 Practice Exercises","lvl2":"Step 3: Let’s Get Started with Python"},"type":"lvl3","url":"/lecture-01-introduction#id-3-6-practice-exercises","position":30},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"3.6 Practice Exercises","lvl2":"Step 3: Let’s Get Started with Python"},"content":"Exercise 1:\nCreate a line plot of the function y = x^3 for x = 1, 2, 3, 4, 5 using Python.\n\nHint\n\nModify the y values to reflect the function y = x^3.\n\nExercise 2:\nCreate a scatter plot of the function y = x^2 for x = 1, 2, 3, 4, 5 using Python.\n\nHint\n\nUse the scatter() function from pyplot to create the scatter plot.\n\nChallenge:\nCreate a plot that shows both y = x^2 and y = x^3 on the same graph. Use different colors and add a legend to differentiate between the two lines.\n\nHint\n\nYou can use the plot() function twice to plot both functions on the same graph. Use the legend() function to add a legend.","type":"content","url":"/lecture-01-introduction#id-3-6-practice-exercises","position":31},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"3.7 Python Can Do More","lvl2":"Step 3: Let’s Get Started with Python"},"type":"lvl3","url":"/lecture-01-introduction#id-3-7-python-can-do-more","position":32},{"hierarchy":{"lvl1":"Chapter 1: Introduction to Python for the Chemical Sciences","lvl3":"3.7 Python Can Do More","lvl2":"Step 3: Let’s Get Started with Python"},"content":"Python’s versatility extends far beyond what we’ll cover in this course. As you continue your journey in the chemical sciences, you’ll find Python to be an invaluable tool for tackling a wide array of problems. Here’s a glimpse of what Python can help you achieve:\n\nStatistical Analysis of Data:\nPython can handle complex statistical analyses with ease, using powerful libraries such as:\n\nnumpy: For numerical operations and array processing.\n\nscipy: For advanced scientific computations, including statistics.\n\npandas: For data manipulation and analysis, particularly with tabular data.\n\nstatsmodels: For statistical modeling and testing.\n\nscikit-learn: For machine learning and predictive modeling.\n\nSimulating Matter:\nPython also excels in simulating matter, which is crucial for research in the chemical sciences. Key libraries include:\n\nase: The Atomic Simulation Environment for setting up, manipulating, running, visualizing, and analyzing atomistic simulations.\n\npyscf: For quantum chemistry and electronic structure calculations.\n\nquacc: An automated workflow engine for quantum chemistry and materials science.\n\npymatgen: The Python Materials Genomics library, useful for materials analysis.\n\ncantera: For thermodynamic, kinetic, and transport properties of chemical reactions.\n\nAnd Much More:\nThe possibilities with Python are vast. For more resources and tools specifically tailored to chemistry, check out the \n\nawesome​-python​-chemistry repository.\n\nThis concludes our first lecture. As you move forward, practice is key to mastering Python’s powerful capabilities. Don’t hesitate to experiment with the code, try out the exercises, and explore additional resources to deepen your understanding. Welcome to the world of computational problem-solving in the chemical sciences!","type":"content","url":"/lecture-01-introduction#id-3-7-python-can-do-more","position":33},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences"},"type":"lvl1","url":"/lecture-02-packages","position":0},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences"},"content":"One of the key strengths of Python is its extensive ecosystem of packages that cater to various scientific needs, including those in the chemical sciences. These packages extend Python’s capabilities, allowing you to perform complex calculations, analyze data, and visualize results with ease. While there are many packages available, in this lecture, we will focus on some of the most fundamental ones that you’ll be using frequently throughout this course:\n\nNumPy: The foundation for numerical computing in Python. NumPy provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays efficiently.\n\nSciPy: Built on top of NumPy, SciPy is a library used for scientific and technical computing. It includes modules for optimization, integration, interpolation, eigenvalue problems, and other advanced mathematical tasks.\n\nMatplotlib: A powerful plotting library that enables you to create a wide variety of static, animated, and interactive visualizations. Matplotlib is particularly useful for generating publication-quality figures in both 2D and 3D.\n\nPandas: A versatile library for data manipulation and analysis. Pandas provides data structures like DataFrames, which allow you to work with structured data easily, making tasks such as data cleaning, transformation, and aggregation straightforward.\n\nIn this lecture, we will explore the core features of each of these packages, with practical examples to help you understand how they can be applied to solve problems in the chemical sciences.","type":"content","url":"/lecture-02-packages","position":1},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-02-packages#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to:\n\nUnderstand the core features and applications of NumPy, SciPy, Matplotlib, and Pandas.\n\nPerform basic numerical operations and matrix manipulations using NumPy.\n\nCreate and customize plots using Matplotlib.\n\nManipulate and analyze data using Pandas DataFrames.","type":"content","url":"/lecture-02-packages#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"type":"lvl2","url":"/lecture-02-packages#section-1-numpy-the-foundation-of-scientific-computing-in-python","position":4},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"content":"NumPy is the cornerstone of scientific computing in Python, providing essential support for large, multi-dimensional arrays and matrices. It also offers a suite of mathematical functions to operate on these arrays, making it indispensable for numerical tasks in the chemical sciences and beyond. Many other scientific libraries, including SciPy, Matplotlib, and Pandas, are built on top of NumPy.","type":"content","url":"/lecture-02-packages#section-1-numpy-the-foundation-of-scientific-computing-in-python","position":5},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"1.1 Key Features of NumPy","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"type":"lvl3","url":"/lecture-02-packages#id-1-1-key-features-of-numpy","position":6},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"1.1 Key Features of NumPy","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"content":"N-dimensional Array Object: NumPy’s array object (ndarray) is a versatile container for data. It can represent vectors, matrices, and higher-dimensional data structures, enabling efficient storage and manipulation of numerical data.\n\nBroadcasting: Perform element-wise operations on arrays of different shapes in a flexible and efficient manner.\n\nLinear Algebra, Fourier Transform, and Random Number Generation: NumPy includes a comprehensive suite of functions for these operations, which are critical in many scientific applications.","type":"content","url":"/lecture-02-packages#id-1-1-key-features-of-numpy","position":7},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"1.2 Working with NumPy Arrays","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"type":"lvl3","url":"/lecture-02-packages#id-1-2-working-with-numpy-arrays","position":8},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"1.2 Working with NumPy Arrays","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"content":"Python Lists\n\nA Python list is an ordered collection of items, which can be of any data type. Lists are created using square brackets, e.g., my_list = [1, 2, 3], and are versatile for storing and manipulating sequences of elements. Lists are indexed, meaning each element can be accessed by its position, starting from 0.\n\nNumPy arrays are the core data structure in NumPy. They are similar to Python lists but are more powerful and efficient, particularly for numerical computations. In the chemical sciences, vectors and matrices are ubiquitous, representing quantities such as atomic positions, molecular orientations, and magnetic field components.","type":"content","url":"/lecture-02-packages#id-1-2-working-with-numpy-arrays","position":9},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Installing NumPy","lvl3":"1.2 Working with NumPy Arrays","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"type":"lvl4","url":"/lecture-02-packages#installing-numpy","position":10},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Installing NumPy","lvl3":"1.2 Working with NumPy Arrays","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"content":"Before you can start using NumPy, you need to ensure it is installed on your system. You can install NumPy by running the following command in your terminal or command prompt:pip install numpy\n\nIf you’re working within a Jupyter Notebook, you can also install NumPy directly by running:!pip install numpy\n\nOnce NumPy is installed, you’re ready to start creating and working with arrays in Python.","type":"content","url":"/lecture-02-packages#installing-numpy","position":11},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Creating and Using Arrays","lvl3":"1.2 Working with NumPy Arrays","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"type":"lvl4","url":"/lecture-02-packages#creating-and-using-arrays","position":12},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Creating and Using Arrays","lvl3":"1.2 Working with NumPy Arrays","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"content":"Vectors, which play a vital role in representing physical quantities in chemistry, can be created easily with NumPy:\n\nimport numpy as np\n\n# Create a 3D vector\nv = np.array([1, 2, 3])\n\n# Print the vector\nprint(v)\n\n# Print the type of v to confirm it's a NumPy array\nprint(type(v))\n\nMatrices are equally important, used to represent systems of equations, Hamiltonians in quantum chemistry, and more. Here’s how you can create a 2x2 matrix:\n\n# Create a 2x2 matrix\nM = np.array([[1, 2], [3, 4]])\n\nprint(M)\n\n","type":"content","url":"/lecture-02-packages#creating-and-using-arrays","position":13},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Matrix and Vector Operations","lvl3":"1.2 Working with NumPy Arrays","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"type":"lvl4","url":"/lecture-02-packages#matrix-and-vector-operations","position":14},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Matrix and Vector Operations","lvl3":"1.2 Working with NumPy Arrays","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"content":"Matrix and vector operations are fundamental in quantum chemistry and many other areas of chemical science. NumPy simplifies these operations:\n\nMatrix-Vector Multiplication: This operation is common in quantum mechanics, where matrices represent operators and vectors represent states.\n\nBest Practice\n\nAlways comment your code to explain the purpose of each section, especially in complex scripts. This makes your code easier to understand and maintain.\n\n# Create a 2x2 matrix\nM = np.array([[1, 2], [3, 4]])\n\n# Create a 2D vector\nv = np.array([1, 2])\n\n# Multiply the matrix by the vector\nw = M @ v\n\nprint(w)\n\nReminder\n\nFor matrix multiplication, the shape of the resulting array is determined by the shapes of the input arrays. For example, if A has shape (m, n) and B has shape (n, p), then C = A @ B will have shape (m, p).\n\nDot Product: The dot product is widely used in quantum chemistry, for example, in verifying the orthonormality of wavefunctions.\n\n# Create two 3D vectors\nv = np.array([1, 2, 3])\nw = np.array([4, 5, 6])\n\n# Compute the dot product\ndot_product = np.dot(v, w)\n\nprint(dot_product)\n\n","type":"content","url":"/lecture-02-packages#matrix-and-vector-operations","position":15},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Advanced Matrix Operations","lvl3":"1.2 Working with NumPy Arrays","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"type":"lvl4","url":"/lecture-02-packages#advanced-matrix-operations","position":16},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Advanced Matrix Operations","lvl3":"1.2 Working with NumPy Arrays","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"content":"In quantum chemistry and other fields, solving linear systems, computing determinants, inverses, and eigenvalues are routine tasks. NumPy makes these operations straightforward:\n\nDeterminant, Inverse, and Eigenvalues:\n\n# Create a 2x2 matrix\nM = np.array([[1, 2], [3, 4]])\n\n# Compute the determinant\ndet = np.linalg.det(M)\nprint(f\"det(M) = {det}\")\n\n# Compute the inverse\nM_inv = np.linalg.inv(M)\nprint(f\"M_inv = \\n{M_inv}\")\n\n# Compute the eigenvalues\neigenvalues = np.linalg.eigvals(M)\nprint(f\"eigenvalues = {eigenvalues}\")\n\n","type":"content","url":"/lecture-02-packages#advanced-matrix-operations","position":17},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Generating Arrays with Specific Properties","lvl3":"1.2 Working with NumPy Arrays","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"type":"lvl4","url":"/lecture-02-packages#generating-arrays-with-specific-properties","position":18},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Generating Arrays with Specific Properties","lvl3":"1.2 Working with NumPy Arrays","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"content":"NumPy also provides convenient functions for creating arrays with predefined properties, such as arrays filled with zeros, ones, or random numbers:\n\n# Create a 3x3 array of zeros\nA = np.zeros((3, 3))\nprint(A)\n\n# Create a 3x3 array of ones\nA = np.ones((3, 3))\nprint(A)\n\n# Create a 3x3 array of random numbers\nA = np.random.rand(3, 3)\nprint(A)\n\n","type":"content","url":"/lecture-02-packages#generating-arrays-with-specific-properties","position":19},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"1.3 Practice Exercises","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"type":"lvl3","url":"/lecture-02-packages#id-1-3-practice-exercises","position":20},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"1.3 Practice Exercises","lvl2":"Section 1: NumPy - The Foundation of Scientific Computing in Python"},"content":"Exercise 1:\nCreate a 3x3 matrix with random integers between 0 and 9.\n\nHint\n\nUse np.random.randint() to generate random integers.\n\nExercise 2:\nCreate a 3x3 identity matrix.\n\nHint\n\nUse np.eye() to create an identity matrix.","type":"content","url":"/lecture-02-packages#id-1-3-practice-exercises","position":21},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl2":"Section 2: SciPy - A Powerful Tool for Scientific Computing"},"type":"lvl2","url":"/lecture-02-packages#section-2-scipy-a-powerful-tool-for-scientific-computing","position":22},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl2":"Section 2: SciPy - A Powerful Tool for Scientific Computing"},"content":"While we won’t delve deeply into SciPy in this lecture, it’s a library we’ll return to periodically throughout this course. SciPy builds on the foundation provided by NumPy, offering elegant and efficient solutions for a variety of complex scientific and engineering problems. However, before relying heavily on SciPy, it’s important to understand the underlying principles by building some of these solutions from scratch. Once you have a solid foundation, SciPy will become an invaluable tool in your computational toolkit.","type":"content","url":"/lecture-02-packages#section-2-scipy-a-powerful-tool-for-scientific-computing","position":23},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"2.1 Key Features of SciPy","lvl2":"Section 2: SciPy - A Powerful Tool for Scientific Computing"},"type":"lvl3","url":"/lecture-02-packages#id-2-1-key-features-of-scipy","position":24},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"2.1 Key Features of SciPy","lvl2":"Section 2: SciPy - A Powerful Tool for Scientific Computing"},"content":"Optimization: SciPy provides robust tools for optimization tasks, such as finding the minimum or maximum of a function. We’ll explore these capabilities in more detail in Lecture 4, where we’ll tackle problems like finding the roots of equations in chemical reaction equilibria.\n\nIntegration: Whether you need to compute definite integrals or solve differential equations, SciPy offers a range of integration techniques. We’ll cover this in Lecture 5, where you’ll learn how to apply these methods to problems such as calculating the overlap integral in chemical bonding.\n\nFourier Transforms: Fourier analysis is a critical tool in many scientific fields, including signal processing and spectroscopy. SciPy makes it easy to perform Fourier transforms, enabling you to analyze signals and decompose them into their constituent frequencies.\n\nAnd Much More: SciPy is a vast library with modules covering topics like linear algebra, statistics, and image processing. As you progress through this course, you’ll find many opportunities to leverage SciPy’s capabilities. For a full overview of what SciPy offers, you can explore the \n\nofficial documentation.","type":"content","url":"/lecture-02-packages#id-2-1-key-features-of-scipy","position":25},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"type":"lvl2","url":"/lecture-02-packages#section-3-matplotlib-creating-publication-quality-visualizations","position":26},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"content":"Matplotlib is a versatile plotting library in Python that allows you to create publication-quality figures across various formats and interactive environments. Whether you need simple plots or complex visualizations, Matplotlib has you covered. With just a few lines of code, you can generate plots, histograms, bar charts, scatterplots, and much more. As a starting point, I highly recommend checking out the “Anatomy of a Figure” graphic from the Matplotlib documentation: \n\nAnatomy of a Figure. This reference will be invaluable as we work through different plotting tasks in this course.","type":"content","url":"/lecture-02-packages#section-3-matplotlib-creating-publication-quality-visualizations","position":27},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"3.1 Key Features of Matplotlib","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"type":"lvl3","url":"/lecture-02-packages#id-3-1-key-features-of-matplotlib","position":28},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"3.1 Key Features of Matplotlib","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"content":"Wide Range of Plot Types: Create various plots, including line plots, scatter plots, histograms, bar charts, and more.\n\nExtensive Customization: Customize every aspect of your plots, from colors and line styles to markers and annotations.\n\nFlexible Output Formats: Save your plots in multiple formats, such as PNG, PDF, and SVG, making it easy to include them in publications, presentations, and reports.","type":"content","url":"/lecture-02-packages#id-3-1-key-features-of-matplotlib","position":29},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"3.2 Creating Basic Plots with Matplotlib","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"type":"lvl3","url":"/lecture-02-packages#id-3-2-creating-basic-plots-with-matplotlib","position":30},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"3.2 Creating Basic Plots with Matplotlib","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"content":"Let’s explore some of the most common plot types you’ll encounter in this course, starting with scatter plots, line plots, and histograms.","type":"content","url":"/lecture-02-packages#id-3-2-creating-basic-plots-with-matplotlib","position":31},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Scatter Plots","lvl3":"3.2 Creating Basic Plots with Matplotlib","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"type":"lvl4","url":"/lecture-02-packages#scatter-plots","position":32},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Scatter Plots","lvl3":"3.2 Creating Basic Plots with Matplotlib","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"content":"Scatter plots are ideal for visualizing relationships between two variables. Here’s how you can create a scatter plot using Matplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create random data\nx = np.random.rand(100)\ny = np.random.rand(100)\n\n# Create a scatter plot\nplt.scatter(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter Plot of Random Data')\nplt.show()\n\n","type":"content","url":"/lecture-02-packages#scatter-plots","position":33},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Line Plots","lvl3":"3.2 Creating Basic Plots with Matplotlib","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"type":"lvl4","url":"/lecture-02-packages#line-plots","position":34},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Line Plots","lvl3":"3.2 Creating Basic Plots with Matplotlib","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"content":"Line plots are commonly used to visualize trends over a range of values. Here’s an example of how to create a simple line plot:\n\n# Create data\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Create a line plot\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.title('Line Plot of Sine Function')\nplt.show()\n\n","type":"content","url":"/lecture-02-packages#line-plots","position":35},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Histograms","lvl3":"3.2 Creating Basic Plots with Matplotlib","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"type":"lvl4","url":"/lecture-02-packages#histograms","position":36},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl4":"Histograms","lvl3":"3.2 Creating Basic Plots with Matplotlib","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"content":"Histograms are useful for visualizing the distribution of data. They show how data is spread across different intervals, providing insights into its distribution:\n\n# Create random data\ndata = np.random.randn(1000)\n\n# Create a histogram\nplt.hist(data, bins=30, color='blue', edgecolor='black')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram of Random Data')\nplt.show()\n\n","type":"content","url":"/lecture-02-packages#histograms","position":37},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"3.3 Customizing Your Plots","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"type":"lvl3","url":"/lecture-02-packages#id-3-3-customizing-your-plots","position":38},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"3.3 Customizing Your Plots","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"content":"One of Matplotlib’s strengths is its ability to customize every aspect of a plot. This includes changing colors, line styles, marker styles, labels, titles, and more. Here’s an example that demonstrates these customization features:\n\n# Create data\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Create a customized line plot\nplt.plot(x, y, color='red', linestyle='--', marker='o', label='sin(x)')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Customized Plot of Sine Function')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nIn this example, we’ve customized the plot by changing the line color to red, using a dashed line style, and adding circular markers. We also included a legend, labeled the axes, added a title, and enabled grid lines for better readability.","type":"content","url":"/lecture-02-packages#id-3-3-customizing-your-plots","position":39},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"3.4 Practice Exercises","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"type":"lvl3","url":"/lecture-02-packages#id-3-4-practice-exercises","position":40},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"3.4 Practice Exercises","lvl2":"Section 3: Matplotlib - Creating Publication-Quality Visualizations"},"content":"Exercise 1:\nCreate a scatter plot of random data with customizations, such as changing the color, marker style, and adding labels.\n\nHint\n\nUse the color and marker parameters in plt.scatter(), and don’t forget to add labels with plt.xlabel() and plt.ylabel().\n\nExercise 2:\nCreate a histogram of random data with customizations, including changing the color, the number of bins, and adding titles and labels.\n\nHint\n\nUse the bins and color parameters in plt.hist() to customize your histogram.","type":"content","url":"/lecture-02-packages#id-3-4-practice-exercises","position":41},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl2":"Section 4: Pandas - Powerful Data Manipulation in Python"},"type":"lvl2","url":"/lecture-02-packages#section-4-pandas-powerful-data-manipulation-in-python","position":42},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl2":"Section 4: Pandas - Powerful Data Manipulation in Python"},"content":"Pandas is like Excel on steroids—think of it as Excel integrated into Python, with far greater flexibility and power. Pandas is a versatile library designed for data manipulation and analysis, providing structures and functions to handle structured data efficiently. It is built on top of NumPy and is particularly useful for working with tabular data, such as spreadsheets and databases.","type":"content","url":"/lecture-02-packages#section-4-pandas-powerful-data-manipulation-in-python","position":43},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"4.1 Key Features of Pandas","lvl2":"Section 4: Pandas - Powerful Data Manipulation in Python"},"type":"lvl3","url":"/lecture-02-packages#id-4-1-key-features-of-pandas","position":44},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"4.1 Key Features of Pandas","lvl2":"Section 4: Pandas - Powerful Data Manipulation in Python"},"content":"Flexible Data Structures: Work with labeled data using Pandas’ two primary data structures: Series (1D) and DataFrame (2D). These structures allow you to easily manipulate and analyze data.\n\nPowerful Data Manipulation: Perform complex operations such as filtering, grouping, merging, and aggregating data with ease.\n\nComprehensive I/O Capabilities: Pandas can read and write data in various formats, including CSV, Excel, and SQL databases, making it easy to integrate with other data sources.","type":"content","url":"/lecture-02-packages#id-4-1-key-features-of-pandas","position":45},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"4.2 Series: The 1D Data Structure","lvl2":"Section 4: Pandas - Powerful Data Manipulation in Python"},"type":"lvl3","url":"/lecture-02-packages#id-4-2-series-the-1d-data-structure","position":46},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"4.2 Series: The 1D Data Structure","lvl2":"Section 4: Pandas - Powerful Data Manipulation in Python"},"content":"A Series in Pandas is a one-dimensional labeled array capable of holding any data type, such as integers, strings, or floating-point numbers. You can think of a Series as a single column in an Excel spreadsheet, with an index to label each row.\n\nHere’s how you can create a Series from a NumPy array:\n\nimport pandas as pd\nimport numpy as np\n\n# Create a Series from a NumPy array\ns = pd.Series(np.random.randn(5))\n\nprint(s)\n\nIn this Series, the first column represents the index (similar to row numbers in Excel), and the second column holds the data.\n\nImportant\n\nBy default, Python uses zero-based indexing, so the first element in a NumPy array or Pandas DataFrame has an index of 0.","type":"content","url":"/lecture-02-packages#id-4-2-series-the-1d-data-structure","position":47},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"4.3 DataFrame: The 2D Data Structure","lvl2":"Section 4: Pandas - Powerful Data Manipulation in Python"},"type":"lvl3","url":"/lecture-02-packages#id-4-3-dataframe-the-2d-data-structure","position":48},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"4.3 DataFrame: The 2D Data Structure","lvl2":"Section 4: Pandas - Powerful Data Manipulation in Python"},"content":"A DataFrame is a two-dimensional labeled data structure, similar to an Excel spreadsheet, where each column can contain different data types. DataFrames are the bread and butter of data manipulation in Pandas, allowing you to organize and manipulate data in powerful ways.\n\nHere’s how to create a DataFrame from a dictionary of NumPy arrays:\n\n# Create a DataFrame from a dictionary of NumPy arrays\ndata = {\n    'A': np.random.randn(5),\n    'B': np.random.rand(5)\n}\n\ndf = pd.DataFrame(data)\n\nprint(df)\n\nIn this DataFrame, the index column functions like the row numbers in Excel, and each key in the dictionary becomes a column. The DataFrame provides the power to manipulate and analyze your data more efficiently than traditional spreadsheet software.","type":"content","url":"/lecture-02-packages#id-4-3-dataframe-the-2d-data-structure","position":49},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"4.4 Reading and Writing Data","lvl2":"Section 4: Pandas - Powerful Data Manipulation in Python"},"type":"lvl3","url":"/lecture-02-packages#id-4-4-reading-and-writing-data","position":50},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"4.4 Reading and Writing Data","lvl2":"Section 4: Pandas - Powerful Data Manipulation in Python"},"content":"Pandas provides functions for reading and writing data in a variety of formats, such as CSV, Excel, and SQL databases. For this example, you can download the data.csv file we’ll be using \n\nhere.\n\n# Read data from a CSV file\ndf = pd.read_csv('data.csv')\n\nprint(df)\n\nThis dataset includes information like element names and their cohesive energies (the energy required to separate atoms in a solid to infinite distances), stored in units of kJ/mol and eV/atom. Pandas also allows you to write this data to a CSV file using the to_csv method:\n\n# Write the DataFrame to a CSV file\ndf.to_csv('output.csv', index=False)\n\nSetting index=False prevents the index from being written to the file, which is useful if you want a clean output.","type":"content","url":"/lecture-02-packages#id-4-4-reading-and-writing-data","position":51},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"4.5 Filtering Data","lvl2":"Section 4: Pandas - Powerful Data Manipulation in Python"},"type":"lvl3","url":"/lecture-02-packages#id-4-5-filtering-data","position":52},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"4.5 Filtering Data","lvl2":"Section 4: Pandas - Powerful Data Manipulation in Python"},"content":"One of Pandas’ strengths is its ability to filter data based on conditions. For example, you can filter a DataFrame to display only the rows where the cohesive energy falls within a specific range:\n\nNote\n\nThe & operator is used for element-wise logical AND operations in Pandas. You can also use | for OR operations, ~ for NOT operations, == for equality, and parentheses for grouping conditions.\n\n# Filter rows where the cohesive energy is between 50 and 100 kJ/mol\nfiltered_df = df[(df['Per Mole'] > 50) & (df['Per Mole'] < 100)]\n\nprint(filtered_df)\n\nThis filtering technique is incredibly useful when working with large datasets, allowing you to focus on subsets of data that meet specific criteria.","type":"content","url":"/lecture-02-packages#id-4-5-filtering-data","position":53},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"4.6 Practice Exercises","lvl2":"Section 4: Pandas - Powerful Data Manipulation in Python"},"type":"lvl3","url":"/lecture-02-packages#id-4-6-practice-exercises","position":54},{"hierarchy":{"lvl1":"Chapter 2: Essential Python Packages for the Chemical Sciences","lvl3":"4.6 Practice Exercises","lvl2":"Section 4: Pandas - Powerful Data Manipulation in Python"},"content":"Exercise 1:\nCreate a DataFrame from a dictionary of NumPy arrays and write it to a CSV file.\n\nHint\n\nUse the pd.DataFrame() function to create the DataFrame, and to_csv() to save it to a file.\n\nExercise 2:\nRead data from a CSV file into a DataFrame, then filter the data based on a condition.\n\nHint\n\nUse pd.read_csv() to load the data and the filtering syntax shown above to filter the DataFrame.\n\nExercise 3:\nFilter a DataFrame based on a different condition. For example, try filtering for rows where the cohesive energy is greater than 150 kJ/mol.\n\nHint\n\nModify the filtering condition in the example provided.\n\nThis concludes our second lecture. As you practice using these essential Python packages, you’ll gain confidence in applying them to solve complex problems in the chemical sciences. The skills you develop here will serve as a strong foundation for your computational work, both in this course and beyond.","type":"content","url":"/lecture-02-packages#id-4-6-practice-exercises","position":55},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python"},"type":"lvl1","url":"/lecture-03-control","position":0},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python"},"content":"","type":"content","url":"/lecture-03-control","position":1},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl2":"Introduction"},"type":"lvl2","url":"/lecture-03-control#introduction","position":2},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl2":"Introduction"},"content":"In this lecture, we will explore one of the most crucial aspects of programming: control structures. Control structures are fundamental building blocks in Python, allowing you to control the flow of execution in your programs. By using these structures, you can make your code more dynamic, flexible, and responsive to different conditions and inputs.","type":"content","url":"/lecture-03-control#introduction","position":3},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"What Are Control Structures?","lvl2":"Introduction"},"type":"lvl3","url":"/lecture-03-control#what-are-control-structures","position":4},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"What Are Control Structures?","lvl2":"Introduction"},"content":"Control structures dictate the order in which individual statements, instructions, or function calls are executed or evaluated. They enable you to:\n\nExecute specific blocks of code based on certain conditions.\n\nRepeat actions or processes efficiently.\n\nOrganize code into reusable sections, making your programs more modular and easier to maintain.","type":"content","url":"/lecture-03-control#what-are-control-structures","position":5},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"Key Control Structures in Python","lvl2":"Introduction"},"type":"lvl3","url":"/lecture-03-control#key-control-structures-in-python","position":6},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"Key Control Structures in Python","lvl2":"Introduction"},"content":"In Python, there are three main types of control structures that we will cover in this lecture:\n\nConditional Statements: These allow you to execute code only when certain conditions are met. This is essential for making decisions in your programs.\n\nLoops: Loops enable you to repeat a block of code multiple times, which is useful for tasks that require iteration, such as processing data sets or performing repetitive calculations.\n\nFunctions: Functions allow you to encapsulate code into reusable blocks. This not only makes your code more organized but also facilitates code reuse and reduces redundancy.","type":"content","url":"/lecture-03-control#key-control-structures-in-python","position":7},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-03-control#learning-objectives","position":8},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you will be able to:\n\nUnderstand and apply conditional statements to control the flow of execution in Python.\n\nUtilize loops to efficiently repeat tasks and process collections of data.\n\nDefine and use functions to create reusable blocks of code.","type":"content","url":"/lecture-03-control#learning-objectives","position":9},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl2":"Section 1: Conditional Statements"},"type":"lvl2","url":"/lecture-03-control#section-1-conditional-statements","position":10},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl2":"Section 1: Conditional Statements"},"content":"Conditional statements are essential in programming as they allow you to control the flow of your code based on specific conditions. In Python, the most common conditional statements are:\n\nif statement\n\nif-else statement\n\nif-elif-else statement\n\nThese structures enable your programs to make decisions and respond accordingly, making your code more dynamic and flexible.","type":"content","url":"/lecture-03-control#section-1-conditional-statements","position":11},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"1.1 The if Statement","lvl2":"Section 1: Conditional Statements"},"type":"lvl3","url":"/lecture-03-control#id-1-1-the-if-statement","position":12},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"1.1 The if Statement","lvl2":"Section 1: Conditional Statements"},"content":"The if statement is the simplest form of a conditional statement. It allows you to execute a block of code only if a specific condition is true. The syntax is straightforward:if condition:\n    # block of code\n\ncondition: This is an expression that evaluates to either True or False.\n\nIf the condition is True, the block of code within the if statement is executed.\n\nIf the condition is False, the code block is skipped.\n\nExample:\n\nx = 10\n\nif x > 5:\n    print(\"x is greater than 5\")\n\n","type":"content","url":"/lecture-03-control#id-1-1-the-if-statement","position":13},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"1.2 The if-else Statement","lvl2":"Section 1: Conditional Statements"},"type":"lvl3","url":"/lecture-03-control#id-1-2-the-if-else-statement","position":14},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"1.2 The if-else Statement","lvl2":"Section 1: Conditional Statements"},"content":"The if-else statement expands on the if statement by providing an alternative block of code to execute if the condition is false. This allows you to handle both possibilities:if condition:\n    # block of code for True condition\nelse:\n    # block of code for False condition\n\nIf the condition is True, the code inside the if block is executed.\n\nIf the condition is False, the code inside the else block is executed.\n\nExample:\n\nx = 1\n\nif x > 5:\n    print(\"x is greater than 5\")\nelse:\n    print(\"x is less than or equal to 5\")\n\n","type":"content","url":"/lecture-03-control#id-1-2-the-if-else-statement","position":15},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"1.3 The if-elif-else Statement","lvl2":"Section 1: Conditional Statements"},"type":"lvl3","url":"/lecture-03-control#id-1-3-the-if-elif-else-statement","position":16},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"1.3 The if-elif-else Statement","lvl2":"Section 1: Conditional Statements"},"content":"The if-elif-else statement is a more complex conditional structure that allows you to check multiple conditions sequentially. This is useful when you need to execute different blocks of code based on different conditions:if condition1:\n    # block of code for condition1\nelif condition2:\n    # block of code for condition2\nelse:\n    # block of code if no conditions are True\n\nThe if block is executed if condition1 is True.\n\nIf condition1 is False but condition2 is True, the elif block is executed.\n\nIf none of the conditions are True, the else block is executed.\n\nExample:\n\nx = 5\n\nif x > 5:\n    print(\"x is greater than 5\")\nelif x < 5:\n    print(\"x is less than 5\")\nelse:\n    print(\"x is equal to 5\")\n\n","type":"content","url":"/lecture-03-control#id-1-3-the-if-elif-else-statement","position":17},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl2":"Section 2: Loops"},"type":"lvl2","url":"/lecture-03-control#section-2-loops","position":18},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl2":"Section 2: Loops"},"content":"Loops are a fundamental concept in programming, allowing you to execute a block of code multiple times, which is especially useful when working with large datasets, repetitive tasks, or iterative processes. In Python, the two most common types of loops are:\n\nfor loop\n\nwhile loop","type":"content","url":"/lecture-03-control#section-2-loops","position":19},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"2.1 The for Loop","lvl2":"Section 2: Loops"},"type":"lvl3","url":"/lecture-03-control#id-2-1-the-for-loop","position":20},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"2.1 The for Loop","lvl2":"Section 2: Loops"},"content":"The for loop is used to iterate over a sequence of elements, such as a list, tuple, string, or other iterable objects, and execute a block of code for each element. This makes it incredibly versatile for processing collections of data.\n\nSyntax:for element in sequence:\n    # block of code\n\nelement: A variable that takes on the value of each element in the sequence one by one.\n\nsequence: Any iterable object (e.g., list, tuple, string, range).\n\nExample:\n\nfor i in range(5):\n    print(i)\n\nIn this example, range(5) generates a sequence of numbers from 0 to 4. The for loop iterates over this sequence, printing each number.","type":"content","url":"/lecture-03-control#id-2-1-the-for-loop","position":21},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl4":"Looping Through a List","lvl3":"2.1 The for Loop","lvl2":"Section 2: Loops"},"type":"lvl4","url":"/lecture-03-control#looping-through-a-list","position":22},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl4":"Looping Through a List","lvl3":"2.1 The for Loop","lvl2":"Section 2: Loops"},"content":"Lists are one of the most common data structures in Python. You can easily loop through a list using a for loop:\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\n\nfor fruit in fruits:\n    print(fruit)\n\nHere, the for loop iterates over the list fruits, printing each fruit in the list.","type":"content","url":"/lecture-03-control#looping-through-a-list","position":23},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl4":"Looping Through a String","lvl3":"2.1 The for Loop","lvl2":"Section 2: Loops"},"type":"lvl4","url":"/lecture-03-control#looping-through-a-string","position":24},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl4":"Looping Through a String","lvl3":"2.1 The for Loop","lvl2":"Section 2: Loops"},"content":"Strings are also iterable, allowing you to loop through each character individually:\n\nfor char in \"hello\":\n    print(char)\n\nThis loop prints each character in the string \"hello\" on a new line.","type":"content","url":"/lecture-03-control#looping-through-a-string","position":25},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl4":"Looping Through a Dictionary","lvl3":"2.1 The for Loop","lvl2":"Section 2: Loops"},"type":"lvl4","url":"/lecture-03-control#looping-through-a-dictionary","position":26},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl4":"Looping Through a Dictionary","lvl3":"2.1 The for Loop","lvl2":"Section 2: Loops"},"content":"Lists vs. Dictionaries\n\nLists ([]): Ordered collections of items. Access elements by their position (index), e.g., my_list[0].\n\nDictionaries ({}): Unordered collections of key-value pairs. Access elements by their key, e.g., my_dict['key']. Keys must be unique.\n\nDictionaries are collections of key-value pairs, and you can loop through both the keys and values:\n\nperson = {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"}\n\nfor key, value in person.items():\n    print(key, value)\n\nIn this example, the for loop iterates over each key-value pair in the dictionary person, printing both the key and its corresponding value.","type":"content","url":"/lecture-03-control#looping-through-a-dictionary","position":27},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl4":"Looping Through a NumPy Array","lvl3":"2.1 The for Loop","lvl2":"Section 2: Loops"},"type":"lvl4","url":"/lecture-03-control#looping-through-a-numpy-array","position":28},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl4":"Looping Through a NumPy Array","lvl3":"2.1 The for Loop","lvl2":"Section 2: Loops"},"content":"NumPy arrays are powerful structures for numerical computations. You can loop through them just like any other iterable:\n\nimport numpy as np\n\narr = np.array([1, 2, 3, 4, 5])\n\nfor element in arr:\n    print(element)\n\nThis example shows how to iterate over each element in a NumPy array.","type":"content","url":"/lecture-03-control#looping-through-a-numpy-array","position":29},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl4":"Looping Through a Pandas DataFrame","lvl3":"2.1 The for Loop","lvl2":"Section 2: Loops"},"type":"lvl4","url":"/lecture-03-control#looping-through-a-pandas-dataframe","position":30},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl4":"Looping Through a Pandas DataFrame","lvl3":"2.1 The for Loop","lvl2":"Section 2: Loops"},"content":"Pandas DataFrames are used for handling tabular data. You can loop through each row of a DataFrame using iterrows():\n\nimport pandas as pd\n\ndata = {\"name\": [\"Alice\", \"Bob\", \"Charlie\"], \"age\": [30, 25, 35]}\ndf = pd.DataFrame(data)\n\nfor index, row in df.iterrows():\n    print(index, row[\"name\"], row[\"age\"])\n\nHere, the loop iterates over each row in the DataFrame df, printing the index, name, and age for each row.\n\nNote\n\nWhen we use syntax like df.iterrows(), we’re calling a method of the DataFrame object df. A method is like a function but is tied to an object (in this case, the DataFrame). The dot notation (df.method_name()) is used to call methods that perform specific actions on the object. So, iterrows() is a method of a DataFrame that returns an iterator over its rows, allowing you to loop through each row.","type":"content","url":"/lecture-03-control#looping-through-a-pandas-dataframe","position":31},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl4":"List Comprehensions","lvl3":"2.1 The for Loop","lvl2":"Section 2: Loops"},"type":"lvl4","url":"/lecture-03-control#list-comprehensions","position":32},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl4":"List Comprehensions","lvl3":"2.1 The for Loop","lvl2":"Section 2: Loops"},"content":"List comprehensions provide a concise way to create lists by applying an expression to each element in an existing iterable. This is an elegant and Pythonic way to create new lists from existing data.\n\nSyntax:new_list = [expression for element in old_list]\n\nExample:\n\nsquares = [x**2 for x in range(5)]\nprint(squares)\n\nIn this example, the list comprehension [x**2 for x in range(5)] creates a new list object, which is then assigned to the variable squares, containing the squares of each element in the range from 0 to 4.","type":"content","url":"/lecture-03-control#list-comprehensions","position":33},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"2.2 The while Loop","lvl2":"Section 2: Loops"},"type":"lvl3","url":"/lecture-03-control#id-2-2-the-while-loop","position":34},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"2.2 The while Loop","lvl2":"Section 2: Loops"},"content":"Infinite Loops\n\nAn infinite loop is a loop that never terminates. It can occur when the loop condition is always True, causing the loop to run indefinitely. To avoid infinite loops, ensure that the loop condition eventually becomes False.\n\nThe while loop is used to execute a block of code as long as a specified condition is true. This type of loop is particularly useful when the number of iterations is not predetermined, but depends on a condition.\n\nSyntax:while condition:\n    # block of code\n\ncondition: An expression that evaluates to either True or False.\n\nThe code block inside the while loop continues to execute as long as the condition is True.\n\nExample:\n\ni = 0\n\nwhile i < 5:\n    print(i)\n    i += 1\n\nIn this example, the variable i is initialized to 0. The while loop executes the block of code repeatedly, printing the value of i and then incrementing i by 1 on each iteration, until i reaches 5.\n\nExercise\n\nWhat happens if you move i = 0 inside the while loop block?Try moving the line i = 0 inside the while loop. What do you expect will happen, and why?\n\nWhat happens if you forget to include i += 1 inside the loop?What do you think will occur if you remove the line i += 1 from the loop? Explain your reasoning and try running the code to see what happens.","type":"content","url":"/lecture-03-control#id-2-2-the-while-loop","position":35},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl2":"Section 3: Functions"},"type":"lvl2","url":"/lecture-03-control#section-3-functions","position":36},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl2":"Section 3: Functions"},"content":"Functions are a core component of Python programming, allowing you to encapsulate a block of code that performs a specific task. This modular approach helps make your code more organized, reusable, and easier to maintain. Functions in Python are defined using the def keyword, followed by the function name and a set of parentheses.","type":"content","url":"/lecture-03-control#section-3-functions","position":37},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"3.1 Defining Functions","lvl2":"Section 3: Functions"},"type":"lvl3","url":"/lecture-03-control#id-3-1-defining-functions","position":38},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"3.1 Defining Functions","lvl2":"Section 3: Functions"},"content":"The basic syntax for defining a function is as follows:def function_name(parameters):\n    # block of code\n    return value\n\nfunction_name: This is the name you give to your function, which should be descriptive of what the function does.\n\nparameters: These are the inputs to the function. You can include multiple parameters, or none at all.\n\nreturn value: The return statement is optional and is used to send back a value from the function to the caller. If you don’t use return, the function will return None by default.\n\nExample:\n\ndef add(x, y):\n    return x + y\n\nresult = add(3, 5)\nprint(result)\n\nIn this example, we define a function add that takes two parameters, x and y, and returns their sum. When we call add(3, 5), the function returns 8, which is then printed.","type":"content","url":"/lecture-03-control#id-3-1-defining-functions","position":39},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"3.2 Functions with Default Parameter Values","lvl2":"Section 3: Functions"},"type":"lvl3","url":"/lecture-03-control#id-3-2-functions-with-default-parameter-values","position":40},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"3.2 Functions with Default Parameter Values","lvl2":"Section 3: Functions"},"content":"Functions can also have default values for their parameters, making some arguments optional when the function is called.\n\nExample:\n\ndef greet(name=\"Alice\"):\n    return \"Hello, \" + name\n\nmessage = greet()\nprint(message)\n\nIn this example, the function greet has a default parameter name=\"Alice\". When the function is called without providing an argument for name, it defaults to \"Alice\", and the message “Hello, Alice” is printed.","type":"content","url":"/lecture-03-control#id-3-2-functions-with-default-parameter-values","position":41},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"3.3 Lambda Functions","lvl2":"Section 3: Functions"},"type":"lvl3","url":"/lecture-03-control#id-3-3-lambda-functions","position":42},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"3.3 Lambda Functions","lvl2":"Section 3: Functions"},"content":"Lambda functions are small, anonymous functions that are defined using the lambda keyword. Unlike regular functions defined with def, lambda functions can have any number of arguments but only one expression. They are often used for short, simple operations that are not reused elsewhere in your code.\n\nSyntax:lambda arguments: expression\n\nExample:\n\nadd = lambda x, y: x + y\n\nresult = add(3, 5)\nprint(result)\n\nHere, we define a lambda function add that takes two arguments, x and y, and returns their sum. The lambda function behaves like a regular function but is written in a more compact form.\n\nNote: add refers to the function itself, while add(3, 5) refers to the result of calling the function with the arguments 3 and 5. In this case, add(3, 5) evaluates to 8.","type":"content","url":"/lecture-03-control#id-3-3-lambda-functions","position":43},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"3.4 Using Lambda Functions with Higher-Order Functions","lvl2":"Section 3: Functions"},"type":"lvl3","url":"/lecture-03-control#id-3-4-using-lambda-functions-with-higher-order-functions","position":44},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"3.4 Using Lambda Functions with Higher-Order Functions","lvl2":"Section 3: Functions"},"content":"Lambda functions are frequently used as arguments to higher-order functions like map(), filter(), and reduce() because of their concise syntax.\n\nSyntax of map():map(function, iterable)\n\nHere, function is the operation to apply, and iterable is the collection of items that the function will be applied to.\n\nExample with map():\n\nnumbers = [1, 2, 3, 4]\nsquares = list(map(lambda x: x**2, numbers))\n\nprint(squares)\n\nIn this example, map() applies the lambda function lambda x: x**2 to each element of the numbers list, producing a list of squares.","type":"content","url":"/lecture-03-control#id-3-4-using-lambda-functions-with-higher-order-functions","position":45},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"3.5 Using Lambda Functions with Pandas","lvl2":"Section 3: Functions"},"type":"lvl3","url":"/lecture-03-control#id-3-5-using-lambda-functions-with-pandas","position":46},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"3.5 Using Lambda Functions with Pandas","lvl2":"Section 3: Functions"},"content":"Lambda functions are also commonly used in Pandas to apply operations across elements in a DataFrame. This is particularly useful for creating new columns or transforming data.\n\nExample:\n\nimport pandas as pd\n\ndata = {\"name\": [\"Alice\", \"Bob\", \"Charlie\"], \"age\": [30, 25, 35]}\ndf = pd.DataFrame(data)\n\ndf[\"age_squared\"] = df[\"age\"].apply(lambda x: x**2)\n\nprint(df)\n\nIn this example, a Pandas DataFrame df is created with columns name and age. The apply() method is used to apply a lambda function that squares each element in the age column. The result is stored in a new column, age_squared.","type":"content","url":"/lecture-03-control#id-3-5-using-lambda-functions-with-pandas","position":47},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"3.6 Best Practices for Using Functions","lvl2":"Section 3: Functions"},"type":"lvl3","url":"/lecture-03-control#id-3-6-best-practices-for-using-functions","position":48},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"3.6 Best Practices for Using Functions","lvl2":"Section 3: Functions"},"content":"Use Descriptive Names: Function names should clearly describe what the function does.\n\nKeep Functions Small and Focused: A function should do one thing and do it well. If your function is getting too long, consider breaking it up into smaller functions.\n\nDocument Your Functions: Use docstrings to explain what your function does, what parameters it takes, and what it returns.\n\nExample of a Well-Documented Function:def calculate_area(radius):\n    \"\"\"\n    Calculate the area of a circle given its radius.\n\n    Parameters:\n    radius (float): The radius of the circle.\n\n    Returns:\n    float: The area of the circle.\n    \"\"\"\n    return 3.14159 * radius ** 2\n\nIn this example, the function calculate_area is well-documented with a docstring that explains what the function does, its parameters, and its return value.","type":"content","url":"/lecture-03-control#id-3-6-best-practices-for-using-functions","position":49},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl2":"Section 4: Hands-on Practice"},"type":"lvl2","url":"/lecture-03-control#section-4-hands-on-practice","position":50},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl2":"Section 4: Hands-on Practice"},"content":"Now that you’ve learned about control structures in Python, it’s time to put your knowledge into practice. Below are a series of exercises designed to help reinforce the concepts you’ve covered. Each exercise includes a hint to guide you if you need a little help getting started.","type":"content","url":"/lecture-03-control#section-4-hands-on-practice","position":51},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"Exercise 1: Check if a Number is Even or Odd","lvl2":"Section 4: Hands-on Practice"},"type":"lvl3","url":"/lecture-03-control#exercise-1-check-if-a-number-is-even-or-odd","position":52},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"Exercise 1: Check if a Number is Even or Odd","lvl2":"Section 4: Hands-on Practice"},"content":"Exercise 1\n\nWrite a Python program to check whether a given number is even or odd.\n\nHint\n\nUse the modulo operator % to check if the number is divisible by 2. If number % 2 == 0, the number is even; otherwise, it’s odd.","type":"content","url":"/lecture-03-control#exercise-1-check-if-a-number-is-even-or-odd","position":53},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"Exercise 2: Sum of All Numbers in a List","lvl2":"Section 4: Hands-on Practice"},"type":"lvl3","url":"/lecture-03-control#exercise-2-sum-of-all-numbers-in-a-list","position":54},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"Exercise 2: Sum of All Numbers in a List","lvl2":"Section 4: Hands-on Practice"},"content":"Exercise 2\n\nWrite a Python program to find the sum of all the numbers in a list.\n\nHint\n\nUse a for loop to iterate over the elements in the list and accumulate the sum in a variable.","type":"content","url":"/lecture-03-control#exercise-2-sum-of-all-numbers-in-a-list","position":55},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"Exercise 3: Factorial of a Number","lvl2":"Section 4: Hands-on Practice"},"type":"lvl3","url":"/lecture-03-control#exercise-3-factorial-of-a-number","position":56},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"Exercise 3: Factorial of a Number","lvl2":"Section 4: Hands-on Practice"},"content":"Exercise 3\n\nWrite a Python program to calculate the factorial of a given number.\n\nHint\n\nUse a for loop to iterate from 1 to n, multiplying the numbers together to calculate the factorial.","type":"content","url":"/lecture-03-control#exercise-3-factorial-of-a-number","position":57},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"Exercise 4: Check if a String is a Palindrome","lvl2":"Section 4: Hands-on Practice"},"type":"lvl3","url":"/lecture-03-control#exercise-4-check-if-a-string-is-a-palindrome","position":58},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"Exercise 4: Check if a String is a Palindrome","lvl2":"Section 4: Hands-on Practice"},"content":"Exercise 4\n\nWrite a Python program to check if a given string is a palindrome.\n\nHint\n\nA palindrome reads the same forward and backward. Use string slicing to reverse the string and compare it to the original.","type":"content","url":"/lecture-03-control#exercise-4-check-if-a-string-is-a-palindrome","position":59},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"Exercise 5: Find the Maximum and Minimum Elements in a List","lvl2":"Section 4: Hands-on Practice"},"type":"lvl3","url":"/lecture-03-control#exercise-5-find-the-maximum-and-minimum-elements-in-a-list","position":60},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"Exercise 5: Find the Maximum and Minimum Elements in a List","lvl2":"Section 4: Hands-on Practice"},"content":"Exercise 5\n\nWrite a Python program to find the maximum and minimum elements in a list.\n\nHint\n\nUse the built-in max() and min() functions to find the maximum and minimum values in the list.","type":"content","url":"/lecture-03-control#exercise-5-find-the-maximum-and-minimum-elements-in-a-list","position":61},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"Additional Exercises","lvl2":"Section 4: Hands-on Practice"},"type":"lvl3","url":"/lecture-03-control#additional-exercises","position":62},{"hierarchy":{"lvl1":"Chapter 3: Control Structures in Python","lvl3":"Additional Exercises","lvl2":"Section 4: Hands-on Practice"},"content":"If you’re looking to challenge yourself further, here are a few more exercises to practice your Python skills:\n\nExercise 6: Count the Number of Vowels in a StringWrite a Python program to count the number of vowels (a, e, i, o, u) in a given string.\n\nHint\n\nUse a for loop to iterate over each character in the string and check if it’s a vowel using the in operator.\n\nExercise 7: Merge Two Lists and Remove DuplicatesWrite a Python program to merge two lists and remove any duplicate elements.\n\nHint\n\nCombine the lists using the + operator, then convert the result to a set to remove duplicates before converting it back to a list.\n\nExercise 8: Generate Fibonacci SequenceWrite a Python program to generate the first n numbers of the Fibonacci sequence.\n\nHint\n\nUse a while loop or a for loop to generate the sequence, starting with 0 and 1, and adding the last two numbers to get the next number.\n\nFeel free to explore these exercises further, and don’t hesitate to experiment with your own ideas. If you have any questions or need clarification, feel free to discuss them in the Slack channel. The more you practice, the more confident you’ll become in your Python programming skills. Good luck!","type":"content","url":"/lecture-03-control#additional-exercises","position":63},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations"},"type":"lvl1","url":"/lecture-04-optimization","position":0},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations"},"content":"","type":"content","url":"/lecture-04-optimization","position":1},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-04-optimization#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you will be able to:\n\nUnderstand and apply the concept of chemical equilibrium.\n\nFormulate and solve equilibrium problems analytically.\n\nUtilize numerical methods, including scipy.optimize.minimize, to find roots of equations in chemical equilibrium problems.","type":"content","url":"/lecture-04-optimization#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Introduction to Chemical Reaction Equilibria"},"type":"lvl2","url":"/lecture-04-optimization#introduction-to-chemical-reaction-equilibria","position":4},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Introduction to Chemical Reaction Equilibria"},"content":"In the chemical sciences, understanding the equilibrium state of a reaction is essential for predicting the final concentrations of reactants and products in a system. The equilibrium state occurs when the \n\nrates of the forward and reverse reactions are equal, resulting in no net change in the concentrations of the species involved. This state is also characterized by the minimization of the system’s free energy, often represented by the \n\nGibbs free energy under conditions of constant temperature and pressure.\n\nConsider a classic example of chemical equilibrium, the dissociation of water vapor into hydrogen and oxygen gases:2 \\text{H}_2\\text{O}(g) \\rightleftharpoons 2 \\text{H}_2(g) + \\text{O}_2(g)\n\nAt equilibrium, the reaction mixture obeys the \n\nlaw of mass action, which relates the concentrations (or partial pressures) of the species to the equilibrium constant, K_P. The equilibrium constant is a quantity characterizing the extent to which a reaction proceeds.","type":"content","url":"/lecture-04-optimization#introduction-to-chemical-reaction-equilibria","position":5},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Mathematical Formulation of Equilibrium Problems"},"type":"lvl2","url":"/lecture-04-optimization#mathematical-formulation-of-equilibrium-problems","position":6},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Mathematical Formulation of Equilibrium Problems"},"content":"The equilibrium constant K_P for a reaction is defined as the ratio of the product of the partial pressures of the products to the reactants, each raised to the power of their respective stoichiometric coefficients. For the reaction given above, K_P can be expressed as:K_P = \\frac{\\left(\\frac{P_{\\text{H}_2}}{P^{\\circ}}\\right)^2 \\left(\\frac{P_{\\text{O}_2}}{P^{\\circ}}\\right)}{\\left(\\frac{P_{\\text{H}_2\\text{O}}}{P^{\\circ}}\\right)^2}\n\nWhere P_{\\text{H}_2}, P_{\\text{O}_2}, and P_{\\text{H}_2\\text{O}} are the partial pressures of hydrogen, oxygen, and water vapor, respectively, and P^{\\circ} is the standard pressure, i.e., 1 bar.\n\nTo find the equilibrium state, we use an ICE (Initial, Change, Equilibrium) table, which helps in setting up the mathematical expressions for the equilibrium concentrations or partial pressures.\n\nExample:\n\nAssume we start with 2 moles of water vapor in a closed system. At equilibrium, the changes in the number of moles of each species can be represented as follows:\n\n\n\n\\text{H}_2\\text{O}\n\n\\text{H}_2\n\n\\text{O}_2\n\nInitial (mol)\n\n2\n\n0\n\n0\n\nChange (mol)\n\n-2x\n\n+2x\n\n+x\n\nEquilibrium (mol)\n\n2 - 2x\n\n2x\n\nx\n\nPartial Pressure (bar)\n\n\\frac{2 - 2x}{2 + x} P\n\n\\frac{2x}{2 + x} P\n\n\\frac{x}{2 + x} P\n\nIn this table:\n\nInitial: The starting number of moles of each species before the reaction reaches equilibrium.\n\nChange: The change in the number of moles of each species as the reaction proceeds toward equilibrium. Here, x represents the extent of the reaction.\n\nEquilibrium: The number of moles of each species at equilibrium.\n\nPartial Pressure: The partial pressure of each species at equilibrium, assuming the total pressure is P.","type":"content","url":"/lecture-04-optimization#mathematical-formulation-of-equilibrium-problems","position":7},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Solving for Equilibrium"},"type":"lvl2","url":"/lecture-04-optimization#solving-for-equilibrium","position":8},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Solving for Equilibrium"},"content":"To solve for the equilibrium partial pressures (or concentrations), you would typically:\n\nWrite the expression for the equilibrium constant K_P using the equilibrium partial pressures.\n\nSubstitute the expressions from the ICE table into the K_P equation.\n\nSolve the resulting equation for x, the extent of the reaction.\n\nThis process often involves solving a nonlinear equation, which can be done analytically for simple cases or numerically for more complex reactions.","type":"content","url":"/lecture-04-optimization#solving-for-equilibrium","position":9},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Numerical Methods for Finding Roots of Equations"},"type":"lvl2","url":"/lecture-04-optimization#numerical-methods-for-finding-roots-of-equations","position":10},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Numerical Methods for Finding Roots of Equations"},"content":"Traditional numerical root-finding algorithms include:\n\nBisection Method: Iteratively narrows down the interval where a root lies by evaluating the midpoint.\n\nNewton-Raphson Method: Uses the derivative of the function to quickly converge to a root, requiring an initial guess.\n\nSecant Method: Approximates the derivative using finite differences, avoiding the need for explicit derivatives.","type":"content","url":"/lecture-04-optimization#numerical-methods-for-finding-roots-of-equations","position":11},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"scipy.optimize.minimize: A Versatile Approach"},"type":"lvl2","url":"/lecture-04-optimization#scipy-optimize-minimize-a-versatile-approach","position":12},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"scipy.optimize.minimize: A Versatile Approach"},"content":"scipy.optimize.minimize is a powerful optimization tool that, while not a traditional root-finding algorithm by design, can be adapted to find roots by minimizing the absolute value of a function.\n\nAdvantages:\n\nBroad Applicability: Suitable for complex, multi-variable problems.\n\nFlexibility: Can work without explicit derivatives and allows for the inclusion of constraints.\n\nVersatility: Effective in a wide range of practical applications, particularly in chemical systems where certain parameters must remain within physical bounds.","type":"content","url":"/lecture-04-optimization#scipy-optimize-minimize-a-versatile-approach","position":13},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Implementing Root-Finding Methods in Python"},"type":"lvl2","url":"/lecture-04-optimization#implementing-root-finding-methods-in-python","position":14},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Implementing Root-Finding Methods in Python"},"content":"To illustrate the process of finding roots using Python, let’s solve a simple quadratic equation both analytically and numerically. Consider the quadratic equation:x^2 - 3x + 2 = 0\n\nUsing the quadratic formula, the roots are:x = \\frac{3 \\pm \\sqrt{(-3)^2 - 4 \\cdot 1 \\cdot 2}}{2 \\cdot 1} = 2, 1\n\nBefore we find these roots numerically, let’s visualize the quadratic function f(x) = x^2 - 3x + 2 using Matplotlib to see where the roots lie. The roots are the points where the curve intersects the x-axis.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the quadratic function\ndef quadratic_equation(x):\n    return x ** 2 - 3 * x + 2\n\n# Generate x values\nx_values = np.linspace(0, 3, 400)\n\n# Plot the function\nplt.plot(x_values, quadratic_equation(x_values), label=r\"$f(x) = x^2 - 3x + 2$\")\nplt.axhline(0, color='gray', linestyle='--')  # x-axis\n\n# Highlight the roots with circles\nroots = [1, 2]\nplt.scatter(roots, [0, 0], color='red', edgecolor='black', s=100, zorder=5, label=\"Roots at $x = 1, 2$\")\n\n# Format and display the plot\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\nplt.title(\"Visualization of the Quadratic Function and Its Roots\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nNow, let’s find these roots numerically using Python’s scipy.optimize.minimize function. We’ll approach this by minimizing the absolute value of the quadratic expression.\n\nfrom scipy.optimize import minimize\n\n# Define the objective function, which is the absolute value of the quadratic equation\ndef objective_function(x):\n    return abs(x ** 2 - 3 * x + 2)\n\n# Perform the minimization starting from an initial guess of x = 0\nresult = minimize(\n    fun=objective_function,  # Objective function to minimize\n    x0=0,                    # Initial guess\n    method=\"Nelder-Mead\",    # Optimization method\n    tol=1e-6                 # Tolerance for convergence\n)\n\nprint(result)\n\nThe output provides information about the optimization process. The message “Optimization terminated successfully” indicates that the method successfully (success: True) found a solution. The value of the objective function at the root fun is close to zero, which is expected because inserting the root into the quadratic equation should yield zero. The root is found to be 1, which matches one of the analytical solutions. The number of iterations nit and function evaluations nfev are also provided.\n\nThe Importance of Initial Guess\n\nHow does the choice of initial guess affect the outcome? For instance, what happens if the initial guess is set to 2.1 instead of 0?\n\nLet’s explore this by changing the initial guess:\n\n# Perform the minimization starting from an initial guess of x = 2.1\nresult = minimize(\n    fun=objective_function,\n    x0=2.1,\n    method=\"Nelder-Mead\",\n    tol=1e-6\n)\n\nprint(result[\"x\"][0])  # Observe how it converges to a different root\n\nYou can index the result dictionary to access the root found by the method. In this case, the root is found to be 2, which is the other analytical solution. This example demonstrates how the choice of initial guess can influence the root found by the numerical method.\n\nNote\n\nThe initial guess plays a crucial role in determining which root is found, especially in equations with multiple roots. For nonlinear or more complex equations, careful consideration of the initial guess is essential.","type":"content","url":"/lecture-04-optimization#implementing-root-finding-methods-in-python","position":15},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Example: Chemical Reaction Equilibrium via Numerical Method"},"type":"lvl2","url":"/lecture-04-optimization#example-chemical-reaction-equilibrium-via-numerical-method","position":16},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Example: Chemical Reaction Equilibrium via Numerical Method"},"content":"Let’s use scipy.optimize.minimize to determine the equilibrium extent of the water-splitting reaction at 4000 K and 1 bar, where the equilibrium constant is K_P = 1.22 \\times 10^{-2}. We aim to find the value of x, the reaction progress, that satisfies the equilibrium condition.","type":"content","url":"/lecture-04-optimization#example-chemical-reaction-equilibrium-via-numerical-method","position":17},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl3":"Step 1: Formulating the Equilibrium Equation","lvl2":"Example: Chemical Reaction Equilibrium via Numerical Method"},"type":"lvl3","url":"/lecture-04-optimization#step-1-formulating-the-equilibrium-equation","position":18},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl3":"Step 1: Formulating the Equilibrium Equation","lvl2":"Example: Chemical Reaction Equilibrium via Numerical Method"},"content":"The equilibrium equation for the reaction is given by:K_P = \\frac{\\left(\\frac{P_{\\text{H}_2}}{P^{\\circ}}\\right)^2 \\left(\\frac{P_{\\text{O}_2}}{P^{\\circ}}\\right)}{\\left(\\frac{P_{\\text{H}_2\\text{O}}}{P^{\\circ}}\\right)^2}\n\nSubstituting the partial pressures in terms of x into the equation, we get:K_P = \\frac{\\left(\\frac{2x}{2 + x}\\frac{P}{P^{\\circ}}\\right)^2 \\left(\\frac{x}{2 + x}\\frac{P}{P^{\\circ}}\\right)}{\\left(\\frac{2 - 2x}{2 + x}\\frac{P}{P^{\\circ}}\\right)^2}\n\nSimplifying this expression, we obtain:K_P = \\frac{4x^3}{(2 - 2x)^2 (2 + x)}\n\nWhere P = P^{\\circ} = 1 bar. The equilibrium equation to be minimized is:(2 - 2x)^2 (2 + x) K_P - 4x^3 = 0","type":"content","url":"/lecture-04-optimization#step-1-formulating-the-equilibrium-equation","position":19},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl3":"Step 2: Minimizing the Equilibrium Equation","lvl2":"Example: Chemical Reaction Equilibrium via Numerical Method"},"type":"lvl3","url":"/lecture-04-optimization#step-2-minimizing-the-equilibrium-equation","position":20},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl3":"Step 2: Minimizing the Equilibrium Equation","lvl2":"Example: Chemical Reaction Equilibrium via Numerical Method"},"content":"First, we define the objective function representing the equilibrium equation to be minimized:\n\ndef objective_function(x, K_P):\n    equilibrium_equation = (2 - 2 * x) ** 2 * (2 + x) * K_P - 4 * x ** 3\n    return abs(equilibrium_equation)\n\nBefore proceeding with the minimization, let’s visualize the objective function to understand its behavior.\n\n# Generate x values\nx_values = np.linspace(0, 1, 400)\n\n# Plot the function\nplt.plot(x_values, objective_function(x_values, 1.22E-02), label=r\"$|(2 - 2x)^2 (2 + x) K_P - 4x^3|$\")\nplt.axhline(0, color='gray', linestyle='--')  # x-axis\n\n# Format and display the plot\nplt.xlabel(\"$x$\")\nplt.ylabel(\"Objective Function\")\nplt.title(\"Visualization of the Objective Function\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nWait, What’s the Expected Solution?\n\nBefore proceeding with the minimization, what value of x do you expect as the equilibrium extent of the reaction? Reflect on this before running the code.\n\nNow, let’s use scipy.optimize.minimize to find the equilibrium extent of the reaction:\n\n# Perform the minimization with an initial guess of x = 0\nresult = minimize(\n    fun=objective_function,\n    x0=0,\n    args=(1.22E-02,),\n    method=\"Nelder-Mead\",\n    tol=1e-6\n)\n\nprint(\"{:.0f}%\".format(result[\"x\"][0] * 100))  # Convert the result to percentage\n\nThe print statement converts the result to a percentage, representing the equilibrium extent of the reaction. The {:.0f} part indicates that the number should be formatted as a floating-point number (f), but with zero decimal places (.0). The .format() part is used to replace the {:.0f} placeholder with the actual value, which is the result of result[\"x\"][0] * 100.\n\nWarning\n\nAlways check that the solution is physically meaningful. For instance, in this context, x must lie between 0 and 1 (representing 0% to 100% reaction progress). This can be enforced using bounds:\n\nresult = minimize(\n    fun=objective_function,\n    x0=2,  # Initial guess outside the expected range\n    args=(1.22E-02,),\n    method=\"Nelder-Mead\",\n    tol=1e-6,\n    bounds=[(0, 1)]\n)\n\nprint(\"{:.0f}%\".format(result[\"x\"][0] * 100))  # The bounds ensure the result stays within the physical limits.\n\nThe warning indicates that the initial guess is outside the specified bounds. However, the method still converges to the correct solution, which is 78% reaction progress.","type":"content","url":"/lecture-04-optimization#step-2-minimizing-the-equilibrium-equation","position":21},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Hands-On Activity"},"type":"lvl2","url":"/lecture-04-optimization#hands-on-activity","position":22},{"hierarchy":{"lvl1":"Chapter 4: Chemical Reaction Equilibria and Roots of Equations","lvl2":"Hands-On Activity"},"content":"Let’s apply the minimize function to solve a cubic equation, which is analogous to solving for the equilibrium in a chemical reaction. Consider the cubic equation:x^3 - 6x^2 + 11x - 6 = 0\n\nThis equation has three real roots. We will use different initial guesses to find these roots using the minimize function.\n\nfrom scipy.optimize import minimize\n\n# Define the cubic equation\ndef cubic_eq(x):\n    return abs(x ** 3 - 6 * x ** 2 + 11 * x - 6)\n\n# Root near x = 1\nlow_root_guess = minimize(\n    fun=cubic_eq,\n    x0=0.9,  # Initial guess close to 1\n    method=\"Nelder-Mead\",\n    tol=1e-6\n)\nprint(\"Root near 1:\", low_root_guess[\"x\"][0])  # Should be close to 1\n\n# Root near x = 2\nmedium_root_guess = minimize(\n    fun=cubic_eq,\n    x0=1.9,  # Initial guess close to 2\n    method=\"Nelder-Mead\",\n    tol=1e-6\n)\nprint(\"Root near 2:\", medium_root_guess[\"x\"][0])  # Should be close to 2\n\n# Root near x = 3\nhigh_root_guess = minimize(\n    fun=cubic_eq,\n    x0=2.9,  # Initial guess close to 3\n    method=\"Nelder-Mead\",\n    tol=1e-6\n)\nprint(\"Root near 3:\", high_root_guess[\"x\"][0])  # Should be close to 3\n\nExercise\n\nExperiment with different initial guesses and observe how they affect the convergence of the method. Consider how close the result is to the expected root and reflect on the importance of choosing a good initial guess.\n\nAdditional Exercise\n\nModify the cubic equation to have different coefficients and use scipy.optimize.minimize to find the new roots. Reflect on how changes in the coefficients affect the roots and the convergence of the method.","type":"content","url":"/lecture-04-optimization#hands-on-activity","position":23},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration"},"type":"lvl1","url":"/lecture-05-integration","position":0},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration"},"content":"The covalent bond plays a central role in the chemical sciences, dictating the chemical and physical properties of organic matter. It also plays an important role in inorganic, solid-state, and materials chemistry. The covalent bond is a quantum mechanical phenomenon where electrons in atomic orbitals are shared between atoms, causing these atomic orbitals to hybridize and form molecular orbitals. The hybridization of atomic orbitals is primarily governed by the energetic similarity of the atomic orbitals involved and their spatial proximity and orientation. In quantum chemistry, the latter is quantified using the overlap integral, which takes the following form:S = \\int_0^{\\infty} \\int_0^{\\pi} \\int_0^{2\\pi} \\psi_i^*(r, \\theta, \\phi) \\psi_j(r, \\theta, \\phi) r^2 \\sin(\\theta) dr d\\theta d\\phi\n\nHere, \\psi_i and \\psi_j are the atomic orbitals of atoms i and j, respectively. These atomic orbitals are written in spherical coordinates, where the volume element of integration is r^2 \\sin(\\theta) dr d\\theta d\\phi. We will come back to this, but first, let us discuss what an integral is.","type":"content","url":"/lecture-05-integration","position":1},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl2":"Analytical vs. Numerical Integration"},"type":"lvl2","url":"/lecture-05-integration#analytical-vs-numerical-integration","position":2},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl2":"Analytical vs. Numerical Integration"},"content":"","type":"content","url":"/lecture-05-integration#analytical-vs-numerical-integration","position":3},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"What Is an Integral?","lvl2":"Analytical vs. Numerical Integration"},"type":"lvl3","url":"/lecture-05-integration#what-is-an-integral","position":4},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"What Is an Integral?","lvl2":"Analytical vs. Numerical Integration"},"content":"The formal definition of a Riemann integral is the limit of the sum of areas of rectangles under a curve. The integral of a function f(x) over an interval [a, b] is given by:\\int_a^b f(x) dx = \\lim_{n \\to \\infty} \\sum_{i=1}^{n} f(x_i) \\Delta x\n\nIn other words, an integral is a sum with really small increments (i.e., as small as possible). Therefore, calculating an integral numerically amounts to computing the area of a series of rectangles along x, where the height of the rectangle is the function y value at that point. There are several methods to achieve an accurate value of the integral with the fewest number of computations, such as the trapezoidal rule, Simpson’s rule, and Gaussian quadrature. In this lecture, we will focus on the Riemann sum when we write our integration codes and then move on to the trapezoidal rule implemented in Scipy and NumPy.","type":"content","url":"/lecture-05-integration#what-is-an-integral","position":5},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"Analytical Integration","lvl2":"Analytical vs. Numerical Integration"},"type":"lvl3","url":"/lecture-05-integration#analytical-integration","position":6},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"Analytical Integration","lvl2":"Analytical vs. Numerical Integration"},"content":"Let’s consider the function f(x) = x^2 over the interval [0, 1]. The integral of this function is:\\int_0^1 x^2 dx = (1/3) x^3 \\Big|_0^1 = 1/3","type":"content","url":"/lecture-05-integration#analytical-integration","position":7},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"Numerical Integration","lvl2":"Analytical vs. Numerical Integration"},"type":"lvl3","url":"/lecture-05-integration#numerical-integration","position":8},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"Numerical Integration","lvl2":"Analytical vs. Numerical Integration"},"content":"Let’s calculate this integral numerically using a Riemann sum with 10, 100, and 1000 rectangles.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function f(x) = x^2\ndef f(x):\n    return x**2\n\n# Define the Riemann sum function\ndef riemann_sum(g, a, b, n):\n    \"\"\"\n    Computes the Riemann sum of a function over a given interval.\n\n    Parameters:\n    g (function): The function to integrate.\n    a (float): The start of the interval.\n    b (float): The end of the interval.\n    n (int): The number of subdivisions.\n\n    Returns:\n    tuple: The Riemann sum, the x-values used for the sum, and the width of each subdivision (dx).\n    \"\"\"\n    x = np.linspace(a, b, n, endpoint=False)\n    dx = (b - a) / n\n    return np.sum(g(x) * dx), x, dx\n\n# Interval [0, 1]\na = 0\nb = 1\n\n# Number of rectangles\nn_values = [10, 100, 1000]\n\n# Prepare the plot\nfig, axs = plt.subplots(1, 3, figsize=(18, 5))  # 3 subplots side by side\nfig.suptitle(\"Visualization of Riemann Sums for $f(x) = x^2$\")\n\n# Calculate and plot the Riemann sums\nfor i, n in enumerate(n_values):\n    riemann_sum_value, x, dx = riemann_sum(f, a, b, n)\n    axs[i].bar(x, f(x), width=dx, align=\"edge\", alpha=0.6, edgecolor=\"black\")\n    axs[i].plot(np.linspace(a, b, 1000), f(np.linspace(a, b, 1000)), \"r-\", label=\"$f(x) = x^2$\")\n    axs[i].set_title(f\"{n} Rectangles\\nRiemann Sum: {riemann_sum_value:.6f}\")\n    axs[i].set_xlabel(\"$x$\")\n    axs[i].set_ylabel(\"$f(x)$\")\n    axs[i].legend()\n\n# Format and display the plot\nplt.tight_layout()\nplt.show()\n\nAs expected from the definition of an integral, the Riemann sum converges to the exact value of the integral as the number of rectangles increases.","type":"content","url":"/lecture-05-integration#numerical-integration","position":9},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"Symmetry and Integration","lvl2":"Analytical vs. Numerical Integration"},"type":"lvl3","url":"/lecture-05-integration#symmetry-and-integration","position":10},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"Symmetry and Integration","lvl2":"Analytical vs. Numerical Integration"},"content":"Now, let’s consider the function f(x) = \\sin(x) over the interval [-\\pi, \\pi]. Let’s calculate this integral numerically using a Riemann sum with 10, 100, and 1000 rectangles.\n\nWait!\n\nBefore we proceed, can you guess the value of this integral? What can you say about the symmetry of the sine function around the center of the interval of integration?\n\nNow that we have a guess, let’s calculate the integral and compare the results.\n\n# Interval [-pi, pi]\na = -np.pi\nb = np.pi\n\n# Define the function f(x) = sin(x)\ndef f(x):\n    return np.sin(x)\n\n# Prepare the plot\nfig, axs = plt.subplots(1, 3, figsize=(18, 5))\n\n# Calculate and plot the Riemann sums\nfor i, n in enumerate(n_values):\n    riemann_sum_value, x, dx = riemann_sum(f, a, b, n)\n    axs[i].bar(x, f(x), width=dx, align=\"edge\", alpha=0.6, edgecolor=\"black\")\n    axs[i].plot(np.linspace(a, b, 1000), f(np.linspace(a, b, 1000)), \"r-\", label=r\"$f(x) = \\sin(x)$\")\n    axs[i].set_title(f\"{n} Rectangles\\nRiemann Sum: {riemann_sum_value:.6f}\")\n    axs[i].set_xlabel(\"$x$\")\n    axs[i].set_ylabel(\"$f(x)$\")\n    axs[i].legend()\n\n# Format and display the plot\nplt.tight_layout()\nplt.show()\n\nNote\n\nBefore integrating, always check the symmetry of the function about the center of the integration range. You can avoid computing the integral if it is symmetric, like the sine function. This type of intuition can prove useful in the chemical sciences.","type":"content","url":"/lecture-05-integration#symmetry-and-integration","position":11},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl2":"Calculating the Overlap Integral of Two H 1s Orbitals"},"type":"lvl2","url":"/lecture-05-integration#calculating-the-overlap-integral-of-two-h-1s-orbitals","position":12},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl2":"Calculating the Overlap Integral of Two H 1s Orbitals"},"content":"","type":"content","url":"/lecture-05-integration#calculating-the-overlap-integral-of-two-h-1s-orbitals","position":13},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"The Hydrogen 1s Orbital","lvl2":"Calculating the Overlap Integral of Two H 1s Orbitals"},"type":"lvl3","url":"/lecture-05-integration#the-hydrogen-1s-orbital","position":14},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"The Hydrogen 1s Orbital","lvl2":"Calculating the Overlap Integral of Two H 1s Orbitals"},"content":"Now, let’s calculate the overlap integral of two hydrogen 1s orbitals, which are given by:\\psi_{1s} = \\frac{1}{\\sqrt{\\pi}} \\left( \\frac{1}{a_0} \\right)^{3/2} e^{-r / a_0}\n\nwhere a_0 is the Bohr radius, which is approximately 0.529 Å. This integral is tough to solve in spherical coordinates, but we can convert it to Cartesian coordinates and use numerical integration. Recall that r is related to x, y, and z as:r = \\sqrt{x^2 + y^2 + z^2}\n\nTherefore, we can rewrite the hydrogen 1s orbital in Cartesian coordinates as:\\psi_{1s} = \\frac{1}{\\sqrt{\\pi}} \\left( \\frac{1}{a_0} \\right)^{3/2} e^{-\\sqrt{x^2 + y^2 + z^2} / a_0}","type":"content","url":"/lecture-05-integration#the-hydrogen-1s-orbital","position":15},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"Computing the Overlap Integral","lvl2":"Calculating the Overlap Integral of Two H 1s Orbitals"},"type":"lvl3","url":"/lecture-05-integration#computing-the-overlap-integral","position":16},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"Computing the Overlap Integral","lvl2":"Calculating the Overlap Integral of Two H 1s Orbitals"},"content":"To determine the total overlap of the hydrogen 1s orbitals of two electrons, we need to integrate over all possible space where those orbitals can overlap, from -\\infty to \\infty in all three dimensions. For two hydrogen atoms separated by a distance of 1.4 a_0 along the x-axis, the overlap integral is given by:S = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\psi_{1s}^*(x + 0.7 a_0, y, z) \\psi_{1s}(x - 0.7 a_0, y, z) dx dy dz","type":"content","url":"/lecture-05-integration#computing-the-overlap-integral","position":17},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"Numerical Integration Using a Riemann Sum","lvl2":"Calculating the Overlap Integral of Two H 1s Orbitals"},"type":"lvl3","url":"/lecture-05-integration#numerical-integration-using-a-riemann-sum","position":18},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"Numerical Integration Using a Riemann Sum","lvl2":"Calculating the Overlap Integral of Two H 1s Orbitals"},"content":"OK, so what do we have to do to compute this integral? First, we must define the x, y, and z ranges over which we will integrate. Then, we need to define the function we are integrating. Finally, we need to integrate the function over the ranges we defined. Let’s do this now.\n\ndef overlap_integral(N, grid_range, separation):\n    \"\"\"\n    Computes the overlap integral using the Riemann sum with a uniform grid.\n\n    Parameters:\n    N (int): Number of grid points along each axis.\n    grid_range (float): The range for x, y, z in Bohr radii.\n    separation (float): Separation distance between the centers of the two atomic orbitals.\n\n    Returns:\n    float: The computed overlap integral.\n    \"\"\"\n\n    # Create grid points and calculate step size\n    x = np.linspace(0, grid_range, N)\n    dx = grid_range / N\n\n    # Create 3D grids for x, y, and z\n    X, Y, Z = np.meshgrid(x, x, x, indexing='ij')\n\n    # Compute the distances r1 and r2\n    r1 = np.sqrt((X + separation / 2) ** 2 + Y ** 2 + Z ** 2)\n    r2 = np.sqrt((X - separation / 2) ** 2 + Y ** 2 + Z ** 2)\n\n    # Compute the integrand for all points in the grid\n    integrand_value = np.exp(-(r1 + r2))\n\n    # Apply the Riemann sum\n    S_sum = np.sum(integrand_value) * dx**3\n\n    # Apply the normalization factor\n    normalization_factor = 8 / np.pi\n    S = normalization_factor * S_sum\n\n    return S\n\n# Example usage\nS = overlap_integral(N=100, grid_range=7, separation=1.4)\nprint(f\"Overlap integral S using the Riemann sum is approximately: {S:.6f}\")\n\nNote\n\nVectorized refers to a method of performing operations on entire arrays or collections of data simultaneously, rather than applying the operation element by element in a loop. In NumPy, vectorization allows you to write cleaner and more efficient code by taking advantage of optimized low-level operations.\n\nThe above code computes the overlap integral of two hydrogen 1s orbitals using a Riemann sum with a uniform grid. The function overlap_integral takes three arguments: N, grid_range, and separation. The N argument specifies the number of grid points along each axis, while the grid_range argument specifies the range for x, y, and z in Bohr radii. The separation argument specifies the separation distance between the centers of the two atomic orbitals. The function returns the computed overlap integral. The code uses NumPy’s vectorized operations to compute the overlap integral efficiently.","type":"content","url":"/lecture-05-integration#numerical-integration-using-a-riemann-sum","position":19},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"Numerical Integration Using the Trapezoidal Rule","lvl2":"Calculating the Overlap Integral of Two H 1s Orbitals"},"type":"lvl3","url":"/lecture-05-integration#numerical-integration-using-the-trapezoidal-rule","position":20},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"Numerical Integration Using the Trapezoidal Rule","lvl2":"Calculating the Overlap Integral of Two H 1s Orbitals"},"content":"The trapezoidal rule is a numerical integration method that approximates the integral of a function by dividing the interval into small trapezoids. The area of each trapezoid is calculated and summed to approximate the integral. The trapezoidal rule is more accurate than the Riemann sum because it approximates the function with straight lines instead of rectangles. The trapezoidal rule can be applied to multidimensional integrals by applying the rule along each axis. The trapezoidal rule is implemented in the scipy.integrate.trapezoid function, which computes the integral of a function using the trapezoidal rule. Let’s rewrite the overlap integral function using the trapezoidal rule.\n\nfrom scipy.integrate import trapezoid\n\ndef overlap_integral_trapezoid(N=100, grid_range=7, separation=0.7):\n    \"\"\"\n    Computes the overlap integral using the trapezoidal rule with a uniform grid.\n\n    Parameters:\n    N (int): Number of grid points along each axis.\n    grid_range (float): The range for x, y, z in Bohr radii.\n    separation (float): Separation distance between the centers of the two atomic orbitals.\n\n    Returns:\n    float: The computed overlap integral.\n    \"\"\"\n\n    # Create grid points and calculate step size\n    x = np.linspace(0, grid_range, N)\n    dx = grid_range / N\n\n    # Create 3D grids for x, y, and z\n    X, Y, Z = np.meshgrid(x, x, x, indexing='ij')\n\n    # Compute the distances r1 and r2\n    r1 = np.sqrt((X + separation / 2) ** 2 + Y ** 2 + Z ** 2)\n    r2 = np.sqrt((X - separation / 2) ** 2 + Y ** 2 + Z ** 2)\n\n    # Compute the integrand for all points in the grid\n    integrand_value = np.exp(-(r1 + r2))\n\n    # Apply the trapezoidal rule along each axis\n    integral_x = trapezoid(integrand_value, x, axis=0)\n    integral_y = trapezoid(integral_x, x, axis=0)\n    S_sum = trapezoid(integral_y, x, axis=0)\n\n    # Apply the normalization factor\n    normalization_factor = 8 / np.pi\n    S = normalization_factor * S_sum\n\n    return S\n\n# Example usage\nS_trapezoid = overlap_integral_trapezoid(N=100, grid_range=7, separation=0.7)\nprint(f\"Overlap integral S using the trapezoidal rule is approximately: {S_trapezoid:.6f}\")\n\nThe trapezoidal rule gives a different result than the Riemann sum because it approximates the function with straight lines instead of rectangles. Ultimately, the trapezoidal rule is more accurate than the Riemann sum because it approximates the function more closely. How can we verify that the trapezoidal rule is more accurate than the Riemann sum? We can compare the results of the two methods to an analytical solution.","type":"content","url":"/lecture-05-integration#numerical-integration-using-the-trapezoidal-rule","position":21},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"Analytical Solution","lvl2":"Calculating the Overlap Integral of Two H 1s Orbitals"},"type":"lvl3","url":"/lecture-05-integration#analytical-solution","position":22},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl3":"Analytical Solution","lvl2":"Calculating the Overlap Integral of Two H 1s Orbitals"},"content":"The analytical solution to the overlap integral of two hydrogen 1s orbitals separated by a distance R is given by:S = \\left( 1 + R + \\frac{R^2}{3} \\right) e^{-R}\n\nLet’s compute the overlap integral using the analytical solution and compare it with the results obtained using the Riemann sum and the trapezoidal rule.\n\nimport pandas as pd\n\n# Separation values between the two hydrogen atoms in Bohr radii\nR_values = np.linspace(0.5, 5, 10)\n\n# Function to compute analytical overlap integral\ndef analytical_overlap_integral(R):\n    \"\"\"\n    Computes the analytical overlap integral.\n\n    Parameters:\n    R (float): Separation distance between the centers of the two atomic orbitals in Bohr radii.\n\n    Returns:\n    float: The analytical overlap integral.\n    \"\"\"\n    return (1 + R + R**2 / 3) * np.exp(-R)\n\n# Lists to store the results\nanalytical_results = []\nriemann_results = []\ntrapezoidal_results = []\n\n# Compute the overlap integral for different separation values\nfor R in R_values:\n    S_analytical = analytical_overlap_integral(R)\n    S_riemann = overlap_integral(N=100, grid_range=7, separation=R)\n    S_trapezoidal = overlap_integral_trapezoid(N=100, grid_range=7, separation=R)\n    analytical_results.append(S_analytical)\n    riemann_results.append(S_riemann)\n    trapezoidal_results.append(S_trapezoidal)\n\n# Create a DataFrame to display the results\nresults_df = pd.DataFrame({\n    \"Separation (Bohr radii)\": R_values,\n    \"Riemann Sum\": riemann_results,\n    \"Trapezoidal Rule\": trapezoidal_results,\n    \"Analytical\": analytical_results\n})\n\n# Display the results\nresults_df\n\nThe trapezoidal rule is more accurate than the Riemann sum for all separation values. Let’s visualize the results to see how the overlap integral changes with the separation distance between the two hydrogen atoms.\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.plot(R_values, analytical_results, \"ro-\", label=\"Analytical\")\nplt.plot(R_values, riemann_results, \"bs-\", label=\"Riemann Sum\")\nplt.plot(R_values, trapezoidal_results, \"g^-\", label=\"Trapezoidal Rule\")\nplt.xlabel(\"Separation (Bohr radii)\")\nplt.ylabel(\"Overlap Integral\")\nplt.title(\"Overlap Integral of Two Hydrogen 1s Orbitals\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nThe plot shows that the overlap integral decreases as the separation distance between the two hydrogen atoms increases. The analytical solution, the Riemann sum, and the trapezoidal rule all give consistent results, with the trapezoidal rule being the most accurate. The analytical solution provides a reference for the accuracy of the numerical methods.","type":"content","url":"/lecture-05-integration#analytical-solution","position":23},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl2":"Hands-On Activity: Overlap of Two He 1s Orbitals"},"type":"lvl2","url":"/lecture-05-integration#hands-on-activity-overlap-of-two-he-1s-orbitals","position":24},{"hierarchy":{"lvl1":"Chapter 5: Chemical Bonding and Numerical Integration","lvl2":"Hands-On Activity: Overlap of Two He 1s Orbitals"},"content":"Now, let’s calculate the overlap integral of two He 1s orbitals, which are given by:\\psi_{1s} = \\frac{1}{\\sqrt{\\pi}} \\left( \\frac{2}{a_0} \\right)^{3/2} e^{-2r / a_0}\n\nwhere a_0 is the Bohr radius, which is approximately 0.529 Å.\n\nWait!\n\nDo you think the overlap integral will decay more slowly or more rapidly for He compared to H? Do particles become more or less localized as the charge increases? In other words, do particles become more classical or more quantum mechanical as the size of the nucleus increases?","type":"content","url":"/lecture-05-integration#hands-on-activity-overlap-of-two-he-1s-orbitals","position":25},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations"},"type":"lvl1","url":"/lecture-06-linalg","position":0},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations"},"content":"","type":"content","url":"/lecture-06-linalg","position":1},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-06-linalg#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to:\n\nBalance chemical equations using algebraic methods and matrix representation.\n\nSolve systems of linear equations for stoichiometric coefficients using Python and NumPy.\n\nApply the null space method to find balanced coefficients for chemical reactions.\n\nInterpret and generalize solutions to balance hydrocarbon combustion reactions and other chemical equations.","type":"content","url":"/lecture-06-linalg#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl2":"Balancing Chemical Equations"},"type":"lvl2","url":"/lecture-06-linalg#balancing-chemical-equations","position":4},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl2":"Balancing Chemical Equations"},"content":"In the chemical sciences, one of the foundational skills is balancing chemical equations to ensure that no mass is lost or created, as dictated by the law of conservation of mass. This process ensures that the number of atoms of each element is the same on both sides of the reaction.\n\nConsider the combustion of methane in oxygen:a \\text{CH}_4(g) + b \\text{O}_2(g) \\rightarrow c \\text{CO}_2(g) + d \\text{H}_2\\text{O}(g)\n\nThe goal is to determine the stoichiometric coefficients a, b, c, and d that balance the equation. We can do this systematically by ensuring the number of each type of atom is equal on both sides of the reaction. A useful approach is to create a table that tracks the number of atoms in the reactants and products for each element:\n\nElement\n\nReactants\n\nProducts\n\nC\n\na\n\nc\n\nH\n\n4a\n\n2d\n\nO\n\n2b\n\n2c + d\n\nFrom this table, we can see the relationships between the variables. First, we know that for carbon to balance, a = c, since there is one carbon atom in both methane and carbon dioxide. Substituting this into the table gives:\n\nElement\n\nReactants\n\nProducts\n\nC\n\na\n\na\n\nH\n\n4a\n\n2d\n\nO\n\n2b\n\n2a + d\n\nNext, we balance hydrogen. Since there are 4 hydrogen atoms in methane, and each water molecule contains 2 hydrogen atoms, we know that 4a = 2d, which simplifies to d = 2a. Updating the table:\n\nElement\n\nReactants\n\nProducts\n\nC\n\na\n\na\n\nH\n\n4a\n\n4a\n\nO\n\n2b\n\n2a + 2a = 4a\n\nFinally, we balance oxygen. We have 2b oxygen atoms on the reactant side and 4a on the product side, so 2b = 4a, meaning b = 2a.\n\nNow, substituting these values into the chemical equation, with a = 1 for simplicity, we get:\\text{CH}_4(g) + 2 \\text{O}_2(g) \\rightarrow \\text{CO}_2(g) + 2 \\text{H}_2\\text{O}(g)\n\nThus, the balanced chemical equation for the combustion of methane is:\\text{CH}_4(g) + 2 \\text{O}_2(g) \\rightarrow \\text{CO}_2(g) + 2 \\text{H}_2\\text{O}(g)\n\nThis systematic approach shows that we can think of balancing chemical equations as solving a set of linear equations for the stoichiometric coefficients.","type":"content","url":"/lecture-06-linalg#balancing-chemical-equations","position":5},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl2":"Systems of Linear Algebraic Equations"},"type":"lvl2","url":"/lecture-06-linalg#systems-of-linear-algebraic-equations","position":6},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl2":"Systems of Linear Algebraic Equations"},"content":"Balancing chemical equations can be thought of as solving a system of linear algebraic equations. If we have the same number of equations as unknowns, we can solve for the unknowns using linear algebra techniques. Let’s revisit the methane combustion problem and express it as a system of equations.\n\nFor the balanced reaction:a \\text{CH}_4(g) + b \\text{O}_2(g) \\rightarrow c \\text{CO}_2(g) + d \\text{H}_2\\text{O}(g)\n\nWe can write the following set of equations based on the element balances:\n\nCarbon balance: a - c = 0 (since there is one carbon atom in both methane and carbon dioxide)\n\nHydrogen balance: 4a - 2d = 0 (4 hydrogen atoms in methane, 2 per water molecule)\n\nOxygen balance: 2b - 2c - d = 0 (2 oxygen atoms in each oxygen molecule, balancing with oxygen in carbon dioxide and water)\n\nThis system of equations can be expressed as:\\begin{align*}\na - c &= 0 \\\\\n4a - 2d &= 0 \\\\\n2b - 2c - d &= 0 \\\\\n\\end{align*}\n\nNext, we can write this in matrix form as:\\begin{align*}\n\\begin{bmatrix}\n1 & 0 & -1 & 0 \\\\\n4 & 0 & 0 & -2 \\\\\n0 & 2 & -2 & -1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\na \\\\\nb \\\\\nc \\\\\nd \\\\\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\\end{align*}\n\nHere, the matrix on the left represents the coefficients of the unknowns a, b, c, and d, and the vector on the right is the zero vector because we are balancing the equation.\n\nMore concisely, this can be written as:\\mathbf{A} \\mathbf{x} = \\mathbf{0}\n\nWhere:\n\n\\mathbf{A} is the matrix of coefficients\n\n\\mathbf{x} is the vector of unknowns [a, b, c, d]\n\n\\mathbf{0} is the zero vector.","type":"content","url":"/lecture-06-linalg#systems-of-linear-algebraic-equations","position":7},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl2":"Solving the System of Equations"},"type":"lvl2","url":"/lecture-06-linalg#solving-the-system-of-equations","position":8},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl2":"Solving the System of Equations"},"content":"Now, let’s apply what we’ve learned to solve the combustion of methane using Python. We’ll employ the numpy library for matrix operations and the scipy.linalg.null_space function to solve the system by finding the null space of the coefficient matrix.","type":"content","url":"/lecture-06-linalg#solving-the-system-of-equations","position":9},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Step 1: Import the Necessary Libraries","lvl2":"Solving the System of Equations"},"type":"lvl3","url":"/lecture-06-linalg#step-1-import-the-necessary-libraries","position":10},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Step 1: Import the Necessary Libraries","lvl2":"Solving the System of Equations"},"content":"We’ll first import the libraries needed for our computation.\n\nimport numpy as np\nfrom scipy.linalg import null_space\n\n","type":"content","url":"/lecture-06-linalg#step-1-import-the-necessary-libraries","position":11},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Step 2: Define the Coefficient Matrix, \\mathbf{A}","lvl2":"Solving the System of Equations"},"type":"lvl3","url":"/lecture-06-linalg#step-2-define-the-coefficient-matrix-mathbf-a","position":12},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Step 2: Define the Coefficient Matrix, \\mathbf{A}","lvl2":"Solving the System of Equations"},"content":"Next, we define the matrix \\mathbf{A}, which represents the coefficients of the unknowns a, b, c, and d in the system of equations:\n\nA = np.array([[1, 0, -1, 0],\n              [4, 0, 0, -2],\n              [0, 2, -2, -1]])\n\nThis matrix corresponds to the carbon, hydrogen, and oxygen balances, respectively, for the combustion reaction.","type":"content","url":"/lecture-06-linalg#step-2-define-the-coefficient-matrix-mathbf-a","position":13},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Step 3: Compute the Null Space","lvl2":"Solving the System of Equations"},"type":"lvl3","url":"/lecture-06-linalg#step-3-compute-the-null-space","position":14},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Step 3: Compute the Null Space","lvl2":"Solving the System of Equations"},"content":"To solve for the stoichiometric coefficients, we find the null space of matrix \\mathbf{A}. This will provide the ratios between a, b, c, and d that satisfy the system of linear equations:\n\n# Calculate the null space of matrix A\nnull_vec = null_space(A)\n\nThe result is a vector of relative coefficients. However, we need to convert these into the smallest possible integer values to balance the equation correctly.","type":"content","url":"/lecture-06-linalg#step-3-compute-the-null-space","position":15},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Step 4: Normalize and Convert to Integer Coefficients","lvl2":"Solving the System of Equations"},"type":"lvl3","url":"/lecture-06-linalg#step-4-normalize-and-convert-to-integer-coefficients","position":16},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Step 4: Normalize and Convert to Integer Coefficients","lvl2":"Solving the System of Equations"},"content":"We can normalize the null space vector by dividing by its smallest positive value and then round to obtain integer coefficients:\n\n# Normalize the coefficients to the smallest integer values\ncoefficients = null_vec[:, 0] / np.min(null_vec[null_vec > 0])\ncoefficients = np.round(coefficients).astype(int)\n\ncoefficients\n\nThe resulting coefficients correspond to the stoichiometric coefficients for the reaction:\n\na = 1\n\nb = 2\n\nc = 1\n\nd = 2","type":"content","url":"/lecture-06-linalg#step-4-normalize-and-convert-to-integer-coefficients","position":17},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Step 5: The Balanced Chemical Equation","lvl2":"Solving the System of Equations"},"type":"lvl3","url":"/lecture-06-linalg#step-5-the-balanced-chemical-equation","position":18},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Step 5: The Balanced Chemical Equation","lvl2":"Solving the System of Equations"},"content":"The balanced chemical equation for the combustion of methane is:\\text{CH}_4(g) + 2\\text{O}_2(g) \\rightarrow \\text{CO}_2(g) + 2\\text{H}_2\\text{O}(g)\n\nThis method can be generalized to balance any chemical equation using matrix algebra and Python.\n\nGeneral Case for Hydrocarbon Combustion\n\nFor the combustion of any alkane, the general form for balancing the equation is:\\text{C}_n\\text{H}_{2n+2}(g) + (3n+1)\\text{O}_2(g) \\rightarrow n\\text{CO}_2(g) + (n+1)\\text{H}_2\\text{O}(g)\n\nMore generally, for any hydrocarbon (saturated or unsaturated), the formula for the stoichiometric coefficients is:\\text{C}_x\\text{H}_y + \\left(x + \\frac{y}{4}\\right)\\text{O}_2 \\rightarrow x\\text{CO}_2 + \\frac{y}{2}\\text{H}_2\\text{O}","type":"content","url":"/lecture-06-linalg#step-5-the-balanced-chemical-equation","position":19},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Summary","lvl2":"Solving the System of Equations"},"type":"lvl3","url":"/lecture-06-linalg#summary","position":20},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Summary","lvl2":"Solving the System of Equations"},"content":"In this section, we demonstrated how to solve the combustion reaction of methane by representing it as a system of linear equations. By using Python, we computed the null space of the coefficient matrix and found the stoichiometric coefficients needed to balance the equation. This same approach can be extended to balance more complex chemical reactions.","type":"content","url":"/lecture-06-linalg#summary","position":21},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl2":"Example: Reduction of Tin(IV) Oxide by Hydrogen"},"type":"lvl2","url":"/lecture-06-linalg#example-reduction-of-tin-iv-oxide-by-hydrogen","position":22},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl2":"Example: Reduction of Tin(IV) Oxide by Hydrogen"},"content":"Next, let’s explore the reduction of tin(IV) oxide by hydrogen to produce tin and water. The unbalanced chemical equation for this reaction is:\\text{SnO}_2(s) + \\text{H}_2(g) \\rightarrow \\text{Sn}(s) + \\text{H}_2\\text{O}(g)","type":"content","url":"/lecture-06-linalg#example-reduction-of-tin-iv-oxide-by-hydrogen","position":23},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Balancing the Equation by Hand","lvl2":"Example: Reduction of Tin(IV) Oxide by Hydrogen"},"type":"lvl3","url":"/lecture-06-linalg#balancing-the-equation-by-hand","position":24},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Balancing the Equation by Hand","lvl2":"Example: Reduction of Tin(IV) Oxide by Hydrogen"},"content":"Before we jump into solving this problem using Python, take a moment to try balancing this equation manually. Start by creating a table that tracks the number of atoms of each element on both sides of the reaction.\n\nElement\n\nReactants\n\nProducts\n\nSn\n\n1\n\n1\n\nO\n\n2\n\n1\n\nH\n\n2\n\n2\n\nFrom this, you can see that the oxygen atoms are not balanced. We have 2 oxygen atoms on the left (from SnO₂) and only 1 oxygen atom on the right (in H₂O). Therefore, we’ll need to add a coefficient of 2 to the water molecule on the product side.\\text{SnO}_2(s) + \\text{H}_2(g) \\rightarrow \\text{Sn}(s) + 2\\text{H}_2\\text{O}(g)\n\nNow, check the hydrogen atoms. We now have 4 hydrogen atoms on the right side (from 2 H₂O molecules), so we need 2 H₂ molecules on the left side to balance the hydrogen.\n\nThe balanced equation is:\\text{SnO}_2(s) + 2\\text{H}_2(g) \\rightarrow \\text{Sn}(s) + 2\\text{H}_2\\text{O}(g)\n\nTake a Moment\n\nDid you try balancing the equation by hand? Understanding the manual process will help you see how Python can automate and streamline this process.","type":"content","url":"/lecture-06-linalg#balancing-the-equation-by-hand","position":25},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Solving the Equation Using Python","lvl2":"Example: Reduction of Tin(IV) Oxide by Hydrogen"},"type":"lvl3","url":"/lecture-06-linalg#solving-the-equation-using-python","position":26},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Solving the Equation Using Python","lvl2":"Example: Reduction of Tin(IV) Oxide by Hydrogen"},"content":"Once you’ve manually balanced the equation, let’s confirm the result using Python. By writing the system of equations for the element balances, we can solve for the stoichiometric coefficients using matrix algebra.\n\nimport numpy as np\nfrom scipy.linalg import null_space\n\n# Define the matrix of coefficients\nA = np.array([[1, 0, -1, 0],   # Sn balance\n              [2, 0, 0, -1],   # O balance\n              [0, 2, 0, -2]])  # H balance\n\n# Compute the null space of the matrix A\nnull_vec = null_space(A)\n\n# Normalize and convert to integer coefficients\ncoefficients = null_vec[:, 0]\ncoefficients = coefficients / np.min(coefficients[coefficients > 0])\ncoefficients = np.round(coefficients).astype(int)\n\n# Output the coefficients\ncoefficients\n\nThis gives us the same result: 1 SnO₂ reacts with 2 H₂ to produce 1 Sn and 2 H₂O.","type":"content","url":"/lecture-06-linalg#solving-the-equation-using-python","position":27},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Recap","lvl2":"Example: Reduction of Tin(IV) Oxide by Hydrogen"},"type":"lvl3","url":"/lecture-06-linalg#recap","position":28},{"hierarchy":{"lvl1":"Chapter 6: Balancing Chemical Equations and Systems of Linear Algebraic Equations","lvl3":"Recap","lvl2":"Example: Reduction of Tin(IV) Oxide by Hydrogen"},"content":"In this example, we manually balanced the reduction of tin(IV) oxide by hydrogen and confirmed our result using Python. You can apply this process to more complex reactions where balancing by hand might be more challenging.\n\nBy leveraging matrix algebra and Python, we can efficiently balance chemical equations, saving time and ensuring accuracy, especially for reactions involving multiple reactants and products. ## Example: Reduction of Tin(IV) Oxide by Hydrogen\n\nLet's consider the reduction of tin(IV) oxide by hydrogen to form tin and water.\n\n$$\n\\begin{align*}\n\\text{SnO}_2(s) + \\text{H}_2(g) &\\rightarrow \\text{Sn}(s) + \\text{H}_2\\text{O}(g) \\\\\n\\end{align*}\n$$\n\n```{admonition} Wait a Minute!\n:class: warning\nCan you try to do this by hand before you do it in Python?\n``` ","type":"content","url":"/lecture-06-linalg#recap","position":29},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis"},"type":"lvl1","url":"/lecture-07-regression","position":0},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis"},"content":"","type":"content","url":"/lecture-07-regression","position":1},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-07-regression#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to:\n\nIdentify reaction orders and rate laws based on experimental data and understand their significance in chemical kinetics.\n\nPerform linear regression analysis to determine reaction rate constants using Python and NumPy.\n\nUse integrated rate laws to linearize reaction data and apply regression techniques to find rate constants.\n\nVisualize and interpret reaction kinetics by plotting concentration vs. time and log-concentration vs. time graphs.","type":"content","url":"/lecture-07-regression#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl2":"Introduction"},"type":"lvl2","url":"/lecture-07-regression#introduction","position":4},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl2":"Introduction"},"content":"Recall that in Lecture 4 we determined how to use numerical methods to compute the equilibrium progress of a chemical reaction. This prediction corresponded to the final state of the reaction, but we did not consider the rate at which the reaction reached equilibrium. In this lecture, we will discuss how to determine the rate constant of a reaction using linear regression analysis.","type":"content","url":"/lecture-07-regression#introduction","position":5},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl3":"A Refresher or Primer on Rate Laws","lvl2":"Introduction"},"type":"lvl3","url":"/lecture-07-regression#a-refresher-or-primer-on-rate-laws","position":6},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl3":"A Refresher or Primer on Rate Laws","lvl2":"Introduction"},"content":"The rate of a chemical reaction is the rate at which the reactants are consumed and the products are formed. It is described by a “rate law,” which is an equation that relates the rate of a reaction to the concentration of the various chemical species present at time t. Rate laws are determined experimentally and cannot, in general, be derived from the stoichiometry of the reaction. Rate laws are typically expressed as:\\text{rate} = k[A]^m[B]^n\n\nwhere k is the “rate constant,” m and n are the “orders of the reaction” with respect to reactants A and B, respectively, and [A] and [B] are the concentrations of reactants A and B, respectively.","type":"content","url":"/lecture-07-regression#a-refresher-or-primer-on-rate-laws","position":7},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl3":"Orders of Reaction","lvl2":"Introduction"},"type":"lvl3","url":"/lecture-07-regression#orders-of-reaction","position":8},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl3":"Orders of Reaction","lvl2":"Introduction"},"content":"The “order of reaction” with respect to a reactant is the power to which the concentration of that reactant is raised in the rate law. The overall order of the reaction is the sum of the orders of the reaction with respect to each reactant. The order of reaction is determined experimentally and is not related to the stoichiometry of the reaction.\n\nRate law\n\nOrder\n\nUnits of k\n\nIntegrated Rate Law\n\n\\text{rate} = k\n\n0\n\nM/s\n\n[A] = -kt + [A]_0\n\n\\text{rate} = k[A]\n\n1\n\n1/s\n\n[A] = [A]_0e^{-kt}\n\n\\text{rate} = k[A]^2\n\n2\n\n1/(M s)\n\n1/[A] = kt + 1/[A]_0\n\n\\text{rate} = k[A][B]\n\n2\n\n1/(M s)\n\n1/[A] = kt + 1/[A]_0 (assuming [A]_0 = [B]_0)\n\n\\text{rate} = k[A]^{1/2}\n\n1/2\n\nM^{1/2}/s\n\n[A]^{1/2} = [A]_0^{1/2} - \\frac{k}{2}t\n\n\\text{rate} = k[A][B]^{1/2}\n\n3/2\n\nM^{-1/2} s^{-1}\n\n[A] = [A]_0 e^{-k't} (pseudo-first order approximation*)\n\n*The pseudo-first order approximation can be used when [B] is much greater than [A].","type":"content","url":"/lecture-07-regression#orders-of-reaction","position":9},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl2":"Determining the Rate Constant of a Reaction"},"type":"lvl2","url":"/lecture-07-regression#determining-the-rate-constant-of-a-reaction","position":10},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl2":"Determining the Rate Constant of a Reaction"},"content":"Imagine that your lab has purchased a container of 1.24 M N_2O_5(g). You retrieved the container from the freezer and were about to start the experiment when you realized you had a meeting with your PI. You left the container on the bench and forgot about it. When you returned to the lab, you found that the N_2O_5(g) had decomposed. Realizing the importance of the decomposition kinetics of N_2O_5(g) on the experiment you were planning, you suggested to your PI that you could determine the rate constant of the decomposition reaction. Your PI agreed and asked you to determine the rate constant of the decomposition of N_2O_5(g) at 318 K. You collected the following data:\n\nTime (min)\n\n[N_2O_5] (M)\n\n0\n\n1.24\n\n10\n\n0.92\n\n20\n\n0.68\n\n30\n\n0.50\n\n40\n\n0.37\n\n50\n\n0.28\n\n60\n\n0.20\n\n70\n\n0.15\n\n80\n\n0.11\n\n90\n\n0.08\n\n100\n\n0.06\n\nLet’s plot the data using matplotlib and numpy to determine how the concentration of N_2O_5(g) changes with time.\n\n# Plot the concentration of N2O5(g) as a function of time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Data\ntime = np.array([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\nconcentration = np.array([1.24, 0.92, 0.68, 0.50, 0.37, 0.28, 0.20, 0.15, 0.11, 0.08, 0.06])\n\n# Plot\nplt.plot(time, concentration, 'ro-')\nplt.xlabel('Time (s)')\nplt.ylabel('Concentration of N$_2$O$_5$ (mol/L)')\nplt.title('Decomposition of N$_2$O$_5$(g)')\nplt.show()\n\nThe concentration of N_2O_5(g) as a function of time shows a clear exponential decay. This suggests that the decomposition of N_2O_5(g) is a first-order reaction. The rate law for a first-order reaction is:\\text{rate} = k[N_2O_5]\n\nwhere k is the rate constant and [N_2O_5] is the concentration of N_2O_5(g). The integrated rate law for a first-order reaction is:[N_2O_5] = [N_2O_5]_0 e^{-kt}\n\nwhere [N_2O_5]_0 is the initial concentration of N_2O_5(g) and t is the time. Alternatively, we can write the integrated rate law as:\\ln([N_2O_5]) = \\ln([N_2O_5]_0) - kt\n\nThis equation is in the form of a linear equation, y = mx + b, where y = \\ln([N_2O_5]), m = -k, x = t, and b = \\ln([N_2O_5]_0). We can use linear regression analysis to determine the rate constant of the reaction.","type":"content","url":"/lecture-07-regression#determining-the-rate-constant-of-a-reaction","position":11},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl2":"Linear Regression Analysis"},"type":"lvl2","url":"/lecture-07-regression#linear-regression-analysis","position":12},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl2":"Linear Regression Analysis"},"content":"Linear regression analysis is a statistical method used to determine the relationship between two variables. In this case, we will use linear regression analysis to determine the rate constant of the decomposition of N_2O_5(g). We will focus on a type of linear regression called ordinary least squares (OLS).","type":"content","url":"/lecture-07-regression#linear-regression-analysis","position":13},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl3":"Ordinary Least Squares","lvl2":"Linear Regression Analysis"},"type":"lvl3","url":"/lecture-07-regression#ordinary-least-squares","position":14},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl3":"Ordinary Least Squares","lvl2":"Linear Regression Analysis"},"content":"OLS is a method used to estimate the parameters of a linear regression model. The goal of OLS is to minimize the sum of the squared differences between the observed values and the predicted values. The equation for a generic linear regression model with one independent variable is:y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\nwhere i is the index of the data points [for example, the time points in our N_2O_5(g) decomposition experiment], y_i is the dependent variable [for example, the concentration of N_2O_5(g)], \\beta_0 is the intercept, \\beta_1 is the slope, x_i is the independent variable [for example, time], and \\epsilon_i is the error term. The goal of OLS is to minimize the sum of the squared differences between the observed values and the predicted values. We can do this by rewriting the equation in terms of the error term:\\epsilon_i = y_i - \\beta_0 - \\beta_1 x_i\n\nThe sum of the squared errors is:\\text{SSE} = \\sum_{i=1}^{n} \\epsilon_i^2 = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2\n\nThe OLS estimates of \\beta_0 and \\beta_1 are:\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\nwhere \\bar{x} and \\bar{y} are the means of the independent and dependent variables, respectively.","type":"content","url":"/lecture-07-regression#ordinary-least-squares","position":15},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl3":"A Practical Example","lvl2":"Linear Regression Analysis"},"type":"lvl3","url":"/lecture-07-regression#a-practical-example","position":16},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl3":"A Practical Example","lvl2":"Linear Regression Analysis"},"content":"Let’s apply these formulas to a simple data set. Suppose we have the following data:\n\nx\n\ny\n\n1\n\n2\n\n2\n\n3\n\n3\n\n4\n\n4\n\n5\n\n5\n\n6\n\nWe can use the formulas above to determine the slope and intercept of the linear regression model. Let’s calculate the slope and intercept using numpy and matplotlib. First, let’s convert this table in numpy arrays.\n\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 3, 4, 5, 6])\n\nNow, let’s write a function that computes the OLS estimates of the slope.\n\ndef ols_slope(x, y):\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    numerator = np.sum((x - x_mean) * (y - y_mean))\n    denominator = np.sum((x - x_mean) ** 2)\n    return numerator / denominator\n\nNow, let’s write a function that computes the OLS estimates of the intercept.\n\ndef ols_intercept(x, y):\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    slope = ols_slope(x, y)\n    return y_mean - slope * x_mean\n\nFinally, let’s put these together into a function that computes the slope and intercept of the linear regression model using OLS.\n\ndef ols(x, y):\n    slope = ols_slope(x, y)\n    intercept = ols_intercept(x, y)\n    return slope, intercept\n\nNow, let’s use this function to determine the slope and intercept of the linear regression model for the data set.\n\nslope, intercept = ols(x, y)\nprint(f\"Slope: {slope}\")\nprint(f\"Intercept: {intercept}\")\n\nVery cool! We have successfully calculated the slope and intercept of the linear regression model using the OLS method. We can now use these values to determine the rate constant of the decomposition of N_2O_5(g).\n\n","type":"content","url":"/lecture-07-regression#a-practical-example","position":17},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl2":"Back to the N_2O_5(g) Decomposition Experiment"},"type":"lvl2","url":"/lecture-07-regression#back-to-the-n-2o-5-g-decomposition-experiment","position":18},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl2":"Back to the N_2O_5(g) Decomposition Experiment"},"content":"Let’s use the linear regression analysis to determine the rate constant of the decomposition of N_2O_5(g). We will use the functions we defined above to calculate the slope and intercept of the linear regression model for the N_2O_5(g) decomposition data.\n\nNow, let’s calculate the slope and intercept of the linear regression model for the N_2O_5(g) decomposition data.\n\n# Compute the natural log of the concentration\nln_concentration = np.log(concentration)\n\n# Perform linear regression\nslope, intercept = ols(time, ln_concentration)\nprint(f\"Slope: {slope}\")\nprint(f\"Intercept: {intercept}\")\n\nSince the slope of the linear regression model is equal to -k, the rate constant of the decomposition of N_2O_5(g) is 3.04 \\times 10^{-2} min^{-1}. Let’s plot the linear regression model and the data to visualize the fit.\n\n# Plot the linear regression\nplt.plot(time, ln_concentration, 'ro')\nplt.plot(time, slope * time + intercept, 'b-')\nplt.xlabel('Time (s)')\nplt.ylabel('ln[Concentration of N$_2$O$_5$ (mol/L)]')\nplt.title('Decomposition of N$_2$O$_5$(g)')\nplt.show()\n\nThat’s an exceptional fit! Now you can confidently report the rate constant of the decomposition of N_2O_5(g) to your PI.\n\n","type":"content","url":"/lecture-07-regression#back-to-the-n-2o-5-g-decomposition-experiment","position":19},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl2":"Hands-On Activity"},"type":"lvl2","url":"/lecture-07-regression#hands-on-activity","position":20},{"hierarchy":{"lvl1":"Chapter 7: Orders of Reaction and Linear Regression Analysis","lvl2":"Hands-On Activity"},"content":"Your lab plans to start working with NOBr(g) in the near future. You have been asked to determine the rate constant of the decomposition of NOBr(g) at 298 K. You collected the following data:\n\nTime (s)\n\n[NOBr] (M)\n\n0\n\n0.0250\n\n6.2\n\n0.0191\n\n10.8\n\n0.0162\n\n14.7\n\n0.0144\n\n20.0\n\n0.0125\n\n24.6\n\n0.0112\n\nHint\n\nTo determine the rate constant, you first need to determine the order of the reaction. You can do this by plotting the data and determining the order of the reaction based on the shape of the curve. Once you have determined the order of the reaction, you can use linear regression analysis to determine the rate constant.","type":"content","url":"/lecture-07-regression#hands-on-activity","position":21},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis"},"type":"lvl1","url":"/lecture-08-calibration","position":0},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis"},"content":"","type":"content","url":"/lecture-08-calibration","position":1},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-08-calibration#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to\n\nDevelop and interpret calibration curves using ordinary least squares (OLS) regression for experimental data.\n\nCalculate and interpret confidence intervals for the slope and intercept of linear models to assess the precision of estimates.\n\nPerform correlation analysis to evaluate the strength and direction of relationships between variables in calibration data.","type":"content","url":"/lecture-08-calibration#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl2":"Introduction"},"type":"lvl2","url":"/lecture-08-calibration#introduction","position":4},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl2":"Introduction"},"content":"Calibration data is a set of data used to establish a relationship between two variables. In calibration, this relationship is typically linear and is used to predict one variable’s value based on the other variable’s value. In this lecture, we will discuss how to analyze calibration data, including calculating confidence intervals for the slope and intercept of the calibration curve and performing correlation analysis to assess the strength of the relationship between the two variables.","type":"content","url":"/lecture-08-calibration#introduction","position":5},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl2":"Calibration Data"},"type":"lvl2","url":"/lecture-08-calibration#calibration-data","position":6},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl2":"Calibration Data"},"content":"Imagine you are an analytical chemist working for a major brewing company. You have been tasked with developing a new method for measuring the diacetyl concentration in beer. Diacetyl is a compound produced during fermentation responsible for a beer’s buttery flavor. The company wants to ensure that the diacetyl concentration in its beer is below a certain threshold to maintain the desired flavor profile.\n\nAfter doing some research, you recommend to your company that an ultraviolet-visible (UV-Vis) spectrometer be used to measure the diacetyl concentration. You have found that diacetyl has a strong absorbance peak at 530 nm. You have also found that the absorbance of diacetyl is linearly related to its concentration (i.e., follows Beer’s Law). You have also found that the absorbance of diacetyl is not affected by the presence of other compounds in beer.\n\nYou have collected the following data.\n\nSample concentration (mg/L)\n\nAbsorbance value (530 nm)\n\n0.5\n\n0.004\n\n1.0\n\n0.007\n\n1.5\n\n0.013\n\n3.0\n\n0.026\n\n4.0\n\n0.032\n\nYou want to use this data to develop a calibration curve that can predict the diacetyl concentration in beer based on the absorbance value measured by the UV-Vis spectrometer.","type":"content","url":"/lecture-08-calibration#calibration-data","position":7},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl2":"Calibration Curve"},"type":"lvl2","url":"/lecture-08-calibration#calibration-curve","position":8},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl2":"Calibration Curve"},"content":"The first step in analyzing calibration data is plotting it and fitting a line. This line is called the calibration curve and represents the relationship between the two variables. In this case, the calibration curve represents the relationship between the diacetyl concentration and the absorbance value.\n\nLet us plot the data and fit a line to it.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef ols_slope(x, y):\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    numerator = np.sum((x - x_mean) * (y - y_mean))\n    denominator = np.sum((x - x_mean) ** 2)\n    return numerator / denominator\n\ndef ols_intercept(x, y):\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    slope = ols_slope(x, y)\n    return y_mean - slope * x_mean\n\ndef ols(x, y):\n    slope = ols_slope(x, y)\n    intercept = ols_intercept(x, y)\n    return slope, intercept\n\n# Data\nconcentration = np.array([0.5, 1.0, 1.5, 3.0, 4.0])\nabsorbance = np.array([0.004, 0.007, 0.013, 0.026, 0.032])\n\n# Fit a line to the data\nslope, intercept = ols(concentration, absorbance)\nline = slope * concentration + intercept\n\n# Plot the calibration curve with the residuals\nplt.scatter(concentration, absorbance, color='blue', label='Data')\nplt.plot(concentration, line, color='red', label='Calibration curve')\nfor i in range(len(concentration)):\n    plt.plot([concentration[i], concentration[i]], [absorbance[i], line[i]], color='gray')\nplt.xlabel('Concentration (mg/L)')\nplt.ylabel('Absorbance value (530 nm)')\nplt.legend()\nplt.show()\n\nIsn’t it great that we already have OLS functions to do this?","type":"content","url":"/lecture-08-calibration#calibration-curve","position":9},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl2":"Confidence Intervals"},"type":"lvl2","url":"/lecture-08-calibration#confidence-intervals","position":10},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl2":"Confidence Intervals"},"content":"Fitting a calibration curve is not the end of the story. We need to know how confident we should be in the slope and intercept of the calibration curve, which is where confidence intervals come in. A confidence interval is a range of values likely to contain a parameter’s true value. In the case of the calibration curve, we are interested in the confidence intervals for the slope and intercept of the line.","type":"content","url":"/lecture-08-calibration#confidence-intervals","position":11},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl3":"A Theoretical Interlude","lvl2":"Confidence Intervals"},"type":"lvl3","url":"/lecture-08-calibration#a-theoretical-interlude","position":12},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl3":"A Theoretical Interlude","lvl2":"Confidence Intervals"},"content":"In OLS, the sum of squared errors (SSE) or residuals (SSR) is key in determining the confidence intervals for the slope and intercept. The SSR is defined asSSR = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\nwhere y_i is the observed value of the dependent variable, \\hat{y}_i is the predicted value of the dependent variable, and n is the number of data points. Looking at the plot above, this would correspond to summing the squares of the vertical distances (gray lines) between the observed data points and the line. The SSR is related to the variance of the residuals, which is defined as\n\nNote\n\nWe divide SSR by n-2 (not n) because estimating the slope and intercept uses up two degrees of freedom. This adjustment accounts for the parameters estimated, providing an unbiased estimate of the residual variance \\sigma^2.\\sigma^2 = \\frac{SSR}{n-2}\n\nwhere n is the number of data points. The variance of the residuals is used to calculate the standard errors of the slope and intercept, which are then used to calculate the confidence intervals. The standard errors of the slope and intercept are defined asSE(\\hat{\\beta}_1) = \\sqrt{\\frac{\\sigma^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}}SE(\\hat{\\beta}_0) = \\sqrt{\\sigma^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\right)}\n\nwhere \\hat{\\beta}_1 is the estimated slope, \\hat{\\beta}_0 is the estimated intercept, x_i is the value of the independent variable, \\bar{x} is the mean of the independent variable, and n is the number of data points. The confidence intervals for the slope and intercept are then calculated asCI(\\hat{\\beta}_1) = \\hat{\\beta}_1 \\pm t_{\\alpha/2} SE(\\hat{\\beta}_1)CI(\\hat{\\beta}_0) = \\hat{\\beta}_0 \\pm t_{\\alpha/2} SE(\\hat{\\beta}_0)\n\nwhere t_{\\alpha/2} is the critical value of the t-distribution with n-2 degrees of freedom and a significance level of \\alpha/2. The confidence intervals give us a range of values likely to contain the true value of the slope and intercept with a certain level of confidence.","type":"content","url":"/lecture-08-calibration#a-theoretical-interlude","position":13},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl3":"Back to the Real World","lvl2":"Confidence Intervals"},"type":"lvl3","url":"/lecture-08-calibration#back-to-the-real-world","position":14},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl3":"Back to the Real World","lvl2":"Confidence Intervals"},"content":"Let’s calculate the confidence intervals for the calibration curve’s slope and intercept. First, we will write our own functions. Then, we will use the statsmodels library to do the same thing.\n\n# Calculate the residuals\nresiduals = absorbance - line\n\n# Calculate the sum of the squared residuals\ndef sse(residuals):\n    return np.sum(residuals ** 2)\n\n# Test the function\nprint(sse(residuals))\n\nNow, let us write a function to compute the variance of the residuals.\n\n# Calculate the variance of the residuals\ndef variance(residuals):\n    return sse(residuals) / (len(residuals) - 2)\n\n# Test the function\nprint(variance(residuals))\n\nOK, now we can calculate the standard errors of the slope and intercept.\n\n# Calculate the standard error of the slope\ndef se_slope(x, residuals):\n    # numerator\n    numerator = variance(residuals)\n    # denominator\n    x_mean = np.mean(x)\n    denominator = np.sum((x - x_mean) ** 2)\n    return np.sqrt(numerator / denominator)\n\n# Test the function\nprint(se_slope(concentration, residuals))\n\n# Calculate the standard error of the intercept\ndef se_intercept(x, residuals):\n    # numerator\n    numerator = variance(residuals)\n    # denominator\n    x_mean = np.mean(x)\n    denominator = len(x) * np.sum((x - x_mean) ** 2)\n    return np.sqrt(numerator / denominator)\n\n# Test the function\nprint(se_intercept(concentration, residuals))\n\nBringing it all together, we can calculate the confidence intervals for the slope and intercept.\n\n# Calculate the confidence interval\nfrom scipy.stats import t\n\ndef confidence_interval_slope(x, residuals, confidence_level):\n    # Calculate the standard error of the slope\n    se = se_slope(x, residuals)\n\n    # Calculate the critical t-value\n    n_data_points = len(x)\n    df = n_data_points - 2  # degrees of freedom\n    alpha = 1 - confidence_level\n    critical_t_value = t.ppf(1 - alpha/2, df)\n\n    # Calculate the confidence interval\n    return critical_t_value * se\n\n# Calculate the 95% confidence interval for the slope\nprint(f\"slope: {slope:.3f} +/- {confidence_interval_slope(concentration, residuals, 0.95):.3f}\")\n\nNow for the intercept.\n\n# Calculate the confidence interval for the intercept\ndef confidence_interval_intercept(x, residuals, confidence_level):\n    # Calculate the standard error of the intercept\n    se = se_intercept(x, residuals)\n\n    # Calculate the critical t-value\n    n_data_points = len(x)\n    df = n_data_points - 2  # degrees of freedom\n    alpha = 1 - confidence_level\n    critical_t_value = t.ppf(1 - alpha/2, df)\n\n    # Calculate the confidence interval\n    return critical_t_value * se\n\n# Calculate the 95% confidence interval for the intercept\nprint(f\"intercept: {intercept:.3f} +/- {confidence_interval_intercept(concentration, residuals, 0.95):.3f}\")\n\n","type":"content","url":"/lecture-08-calibration#back-to-the-real-world","position":15},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl2":"Correlation Analysis"},"type":"lvl2","url":"/lecture-08-calibration#correlation-analysis","position":16},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl2":"Correlation Analysis"},"content":"The last step in analyzing calibration data is to perform correlation analysis. Correlation analysis assesses the strength of the relationship between the two variables. In this case, we are interested in the correlation between the diacetyl concentration and the absorbance value. The correlation coefficient measures the strength and direction of the relationship between two variables. It ranges from -1 to 1, with 1 indicating a perfect positive relationship, -1 indicating a perfect negative relationship, and 0 indicating no relationship. The correlation coefficient is calculated asr = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n\nwhere x_i is the value of the independent variable, \\bar{x} is the mean of the independent variable, y_i is the value of the dependent variable, and \\bar{y} is the mean of the dependent variable. The correlation coefficient gives us an indication of how well the two variables are related. A correlation coefficient close to 1 or -1 indicates a strong relationship, while a correlation coefficient close to 0 indicates a weak relationship.\n\n# Generate sample data\nnp.random.seed(0)\nx = np.random.rand(10) * 10  # Independent variable\ny = 3 * x + np.random.randn(10) * 5  # Dependent variable\n\n# Calculate the mean of x and y\nmean_x = np.mean(x)\nmean_y = np.mean(y)\n\n# Calculate the deviations from the mean\ndeviations_x = x - mean_x\ndeviations_y = y - mean_y\n\n# Calculate the numerator and denominator of the correlation coefficient formula\nnumerator = np.sum(deviations_x * deviations_y)\ndenominator = np.sqrt(np.sum(deviations_x**2) * np.sum(deviations_y**2))\n\n# Calculate the correlation coefficient\ncorrelation_coefficient = numerator / denominator\n\n# Create a scatter plot\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, color='blue', label='Data points')\n\n# Plot the mean lines\nplt.axhline(mean_y, color='red', linestyle='--', label='Mean of y')\nplt.axvline(mean_x, color='green', linestyle='--', label='Mean of x')\n\n# Annotate deviations\nfor i in range(len(x)):\n    plt.plot([x[i], x[i]], [mean_y, y[i]], color='gray', linestyle=':')\n    plt.plot([mean_x, x[i]], [y[i], y[i]], color='gray', linestyle=':')\n\n# Display the correlation coefficient on the plot\nplt.title('Correlation Analysis')\nplt.xlabel('Independent Variable (x)')\nplt.ylabel('Dependent Variable (y)')\nplt.legend()\nplt.text(1, 25, f'Correlation Coefficient: {correlation_coefficient:.2f}', fontsize=12, color='red')\n\n# Show the plot\nplt.grid(True)\nplt.show()\n\nLet us calculate the correlation coefficient for the diacetyl concentration and the absorbance value.\n\n# Calculate the correlation coefficient\ndef correlation_coefficient(x, y):\n    # Calculate the mean of x and y\n    mean_x = np.mean(x)\n    mean_y = np.mean(y)\n\n    # Calculate the deviations from the mean\n    deviations_x = x - mean_x\n    deviations_y = y - mean_y\n\n    # Calculate the numerator and denominator of the correlation coefficient formula\n    numerator = np.sum(deviations_x * deviations_y)\n    denominator = np.sqrt(np.sum(deviations_x**2) * np.sum(deviations_y**2))\n\n    # Calculate the correlation coefficient\n    return numerator / denominator\n\n# Test the function\nprint(correlation_coefficient(x, y))\n\nApplying this to our data, we get the following correlation coefficient.\n\n# Calculate the correlation coefficient of the concentration and absorbance data\nprint(correlation_coefficient(concentration, absorbance))\n\nThe correlation coefficient is close to 1, which indicates a strong positive relationship between the diacetyl concentration and the absorbance value. This means that the absorbance value can be used to predict the diacetyl concentration in beer with a high degree of accuracy, reinforcing the physical validity of the Beer-Lambert Law and the utility of the UV-Vis spectrometer for measuring diacetyl concentration accurately.\n\nA Familiar Form of the Correlation Coefficient\n\nDid you know that the R^2 value is the square of the correlation coefficient? R^2 value measures the proportion of the variance in the dependent variable that is predictable from the independent variable. In other words, it measures how well the independent variable predicts the dependent variable. The R^2 value ranges from 0 to 1, with 1 indicating a perfect fit and 0 indicating no fit.","type":"content","url":"/lecture-08-calibration#correlation-analysis","position":17},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl2":"Hands-On Activity"},"type":"lvl2","url":"/lecture-08-calibration#hands-on-activity","position":18},{"hierarchy":{"lvl1":"Chapter 8: Calibration Data, Confidence Intervals, and Correlation Analysis","lvl2":"Hands-On Activity"},"content":"Now that you have learned how to analyze calibration data, it is time to test your skills. Here is a hands-on activity for you to try.\n\nAn ideal diatomic gas has a temperature-independent constant pressure heat capacity of C_V = 7R/2, where R is the gas constant. To assess deviations from this ideal behavior, a series of measurements of the heat capacity of a gas as a function of temperature were made. The data are as follows.\n\nTemperature (K)\n\nHeat Capacity (J/mol-K)\n\n600\n\n30.93\n\n650\n\n31.54\n\n700\n\n31.32\n\n750\n\n32.18\n\n800\n\n32.25\n\n850\n\n32.27\n\n900\n\n33.41\n\n950\n\n33.21\n\n1000\n\n33.97\n\nPlot the data and fit a line to obtain the calibration curve.\n\nCalculate the confidence intervals for the slope and intercept of the calibration curve.\n\nCalculate the correlation coefficient between the temperature and heat capacity.\n\nGood luck!","type":"content","url":"/lecture-08-calibration#hands-on-activity","position":19},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics"},"type":"lvl1","url":"/lecture-09-thermo","position":0},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics"},"content":"","type":"content","url":"/lecture-09-thermo","position":1},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-09-thermo#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to\n\nSet up a thermodynamic system and its associated variables.\n\nUnderstand qualitatively the laws of thermodynamics.\n\nAppreciate the role of free energy in establishing equilibrium.","type":"content","url":"/lecture-09-thermo#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl2":"Why Should You Care About Thermodynamics?"},"type":"lvl2","url":"/lecture-09-thermo#why-should-you-care-about-thermodynamics","position":4},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl2":"Why Should You Care About Thermodynamics?"},"content":"Will two molecules react? If so, how much heat will they give off? How far will the reaction proceed? Will precipitation bring rain or snow? These and many other questions are central to our lived experience in the chemical sciences and, more broadly, in the natural world. The answers to these questions are governed by the laws of thermodynamics.","type":"content","url":"/lecture-09-thermo#why-should-you-care-about-thermodynamics","position":5},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl2":"Thermodynamic Systems"},"type":"lvl2","url":"/lecture-09-thermo#thermodynamic-systems","position":6},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl2":"Thermodynamic Systems"},"content":"Before we discuss the laws of thermodynamics, which will give greater insight into the etymology of the word “thermodynamics,” we should define a thermodynamic system.\n\nNote\n\nA useful list of thermodynamic properties can be found here: \n\nList of thermodynamic properties.\n\nAs shown above, a thermodynamic system consists of three parts: the system, its surroundings, and the boundary separating them. The state of the system is defined by the values of its state variables, which include pressure P, volume V, number of moles n or number of particles N (where “particles” can refer to atoms, molecules, ions, clusters, etc.), and temperature T. There are two types of state variables: extensive and intensive. Extensive variables, like V and n, depend on the size of the system, while intensive variables, like P and T, do not. The equation of state relates the state variables of a system. For an ideal gas, the equation of state is the ideal gas law, PV = nRT.\n\nThere are three main types of systems that differ in the boundary between them and their surroundings: isolated, closed, and open systems.\n\nIsolated systems have boundaries that are impermeable to matter and energy. Closed systems have boundaries that are impermeable to matter but not energy. Open systems have boundaries that are permeable to both matter and energy.","type":"content","url":"/lecture-09-thermo#thermodynamic-systems","position":7},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl2":"The Laws of Thermodynamics"},"type":"lvl2","url":"/lecture-09-thermo#the-laws-of-thermodynamics","position":8},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl2":"The Laws of Thermodynamics"},"content":"","type":"content","url":"/lecture-09-thermo#the-laws-of-thermodynamics","position":9},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl3":"The Zeroth Law","lvl2":"The Laws of Thermodynamics"},"type":"lvl3","url":"/lecture-09-thermo#the-zeroth-law","position":10},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl3":"The Zeroth Law","lvl2":"The Laws of Thermodynamics"},"content":"The zeroth law of thermodynamics states that if two systems are in thermal equilibrium with a third system, then they are in thermal equilibrium with each other.","type":"content","url":"/lecture-09-thermo#the-zeroth-law","position":11},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl3":"The First Law","lvl2":"The Laws of Thermodynamics"},"type":"lvl3","url":"/lecture-09-thermo#the-first-law","position":12},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl3":"The First Law","lvl2":"The Laws of Thermodynamics"},"content":"","type":"content","url":"/lecture-09-thermo#the-first-law","position":13},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl4":"Internal Energy, Work, and Heat","lvl3":"The First Law","lvl2":"The Laws of Thermodynamics"},"type":"lvl4","url":"/lecture-09-thermo#internal-energy-work-and-heat","position":14},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl4":"Internal Energy, Work, and Heat","lvl3":"The First Law","lvl2":"The Laws of Thermodynamics"},"content":"Note\n\nInternal energy is a state function, meaning that its value depends only on the state of the system and not on how the system arrived at that state. It has an exact differential, dU.\n\nInternal energy U is the sum of the kinetic and potential energies of the particles in a system. For classical particles, the internal energy is given byU = \\sum_i \\frac{1}{2} m_i v_i^2 + V \\left( \\{ r_i \\} \\right)\n\nwhere m_i is the mass of particle i, v_i is its velocity, and V is the potential energy of the particles. For example, in the case of two particles interacting via a spring, V = \\frac{1}{2} k \\left( r - r_0 \\right)^2, where k is the spring constant, r is the distance between the particles, and r_0 is the distance between the particles that corresponds to the minimum potential energy.\n\nThe internal energy of this system isU = \\frac{1}{2} m_1 v_1^2 + \\frac{1}{2} m_2 v_2^2 + \\frac{1}{2} k \\left( r - r_0 \\right)^2\n\nNote\n\nWork and heat are path functions, meaning that their values depend on the path taken to reach a particular state. They have inexact differentials, \\delta w and \\delta q.\n\nThe work w done on a system is the energy transferred to the system by a force acting on it. The work done by a force \\mathbf{F} on a system isw = \\int \\mathbf{F} \\cdot d\\mathbf{r}\n\nwhere \\mathbf{r} is the position vector of the system. The work done by a force on a system is positive if the force is in the same direction as the displacement of the system and negative if the force is in the opposite direction. For example, the work done by a gas expanding against a piston isw = -\\int P dV\n\nwhere P is the pressure of the gas and V is the volume of the gas.\n\nThe heat q transferred to a system is the energy transferred to the system by a temperature difference. The heat transferred to a system is positive if the system gains energy and negative if the system loses energy.","type":"content","url":"/lecture-09-thermo#internal-energy-work-and-heat","position":15},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl4":"Statement of the First Law","lvl3":"The First Law","lvl2":"The Laws of Thermodynamics"},"type":"lvl4","url":"/lecture-09-thermo#statement-of-the-first-law","position":16},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl4":"Statement of the First Law","lvl3":"The First Law","lvl2":"The Laws of Thermodynamics"},"content":"The first law of thermodynamics states that the change in internal energy of a system is equal to the heat added to the system plus the work done on the system\\Delta U = q + w\n\nFor infinitesimal changes in internal energy, heat, and work, the first law can be written asdU = \\delta q + \\delta w","type":"content","url":"/lecture-09-thermo#statement-of-the-first-law","position":17},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl3":"The Second Law","lvl2":"The Laws of Thermodynamics"},"type":"lvl3","url":"/lecture-09-thermo#the-second-law","position":18},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl3":"The Second Law","lvl2":"The Laws of Thermodynamics"},"content":"Note\n\nEntropy is a state function, meaning that its value depends only on the state of the system and not on how the system arrived at that state. It has an exact differential, dS.\n\nNote\n\nA reversible process is a quasistatic process that can be reversed without net changes (e.g., in entropy) in the system or its surroundings. A quasistatic process is a process that proceeds through a series of equilibrium states.\n\nThe second law of thermodynamics states that the entropy of an isolated system never decreases. The entropy of a system is a measure of the number of ways the system can be arranged. The second law can be stated in many ways, including\\Delta S \\geq \\frac{q}{T}\n\nwhere \\Delta S is the change in entropy of the system, q is the heat added to the system, and T is the temperature of the system. For infinitesimal changes in entropy and heat, the second law can be written asdS \\geq \\frac{\\delta q}{T}\n\nThe equality holds for reversible processes, while the inequality holds for irreversible processes.","type":"content","url":"/lecture-09-thermo#the-second-law","position":19},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl4":"Fundamental Thermodynamic Relation","lvl3":"The Second Law","lvl2":"The Laws of Thermodynamics"},"type":"lvl4","url":"/lecture-09-thermo#fundamental-thermodynamic-relation","position":20},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl4":"Fundamental Thermodynamic Relation","lvl3":"The Second Law","lvl2":"The Laws of Thermodynamics"},"content":"By combining the first and second laws of thermodynamics, we can derive the fundamental thermodynamic relation for a reversible processdU = T dS + \\delta w\n\nFor mechanical work, the fundamental thermodynamic relation becomes dU = T dS - P dV.","type":"content","url":"/lecture-09-thermo#fundamental-thermodynamic-relation","position":21},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl3":"The Third Law","lvl2":"The Laws of Thermodynamics"},"type":"lvl3","url":"/lecture-09-thermo#the-third-law","position":22},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl3":"The Third Law","lvl2":"The Laws of Thermodynamics"},"content":"Note\n\nA perfect crystal is a crystal in which all atoms are in a perfect, ordered arrangement.\n\nThe third law of thermodynamics states that the entropy of a perfect crystal at absolute zero is zero.","type":"content","url":"/lecture-09-thermo#the-third-law","position":23},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl2":"Thermodynamic Potentials"},"type":"lvl2","url":"/lecture-09-thermo#thermodynamic-potentials","position":24},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl2":"Thermodynamic Potentials"},"content":"Potential\n\nDefinition\n\nEquation\n\nInternal energy\n\nSum of kinetic and potential energies of particles\n\nU = \\sum_i \\frac{1}{2} m_i v_i^2 + V \\left( \\{ r_i \\} \\right)\n\nEnthalpy\n\nInternal energy plus pressure-volume work\n\nH = U + PV\n\nHelmholtz free energy\n\nEnergy available to do work at constant volume\n\nA = U - TS\n\nGibbs free energy\n\nEnergy available to do work at constant pressure\n\nG = U + PV - TS","type":"content","url":"/lecture-09-thermo#thermodynamic-potentials","position":25},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl3":"Equilibrium","lvl2":"Thermodynamic Potentials"},"type":"lvl3","url":"/lecture-09-thermo#equilibrium","position":26},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl3":"Equilibrium","lvl2":"Thermodynamic Potentials"},"content":"A system is in equilibrium when its relevant thermodynamic potential is minimized. For a system at constant temperature and volume, the equilibrium condition is dA = 0. For a system at constant temperature and pressure, the equilibrium condition is dG = 0.","type":"content","url":"/lecture-09-thermo#equilibrium","position":27},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl4":"Phase Equilibria","lvl3":"Equilibrium","lvl2":"Thermodynamic Potentials"},"type":"lvl4","url":"/lecture-09-thermo#phase-equilibria","position":28},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl4":"Phase Equilibria","lvl3":"Equilibrium","lvl2":"Thermodynamic Potentials"},"content":"A phase is a region of space in which the properties of a material are uniform. A phase transition is a change in the properties of a material as it moves from one phase to another. The phase diagram of a material shows the regions of space in which the material is in a particular phase.\n\nThe figure above shows the P-T phase diagram of carbon dioxide. The phase boundary between two phases is the line at which the free energies of the two phases are equal. The triple point is the point at which the free energies of all three phases are equal.","type":"content","url":"/lecture-09-thermo#phase-equilibria","position":29},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl2":"Summary"},"type":"lvl2","url":"/lecture-09-thermo#summary","position":30},{"hierarchy":{"lvl1":"Chapter 9: Classical Thermodynamics","lvl2":"Summary"},"content":"In this lecture, we have introduced the concept of a thermodynamic system and its associated variables. We have discussed the laws of thermodynamics and their implications for the behavior of systems. We have also introduced the concept of thermodynamic potentials and their role in establishing equilibrium. Finally, we have discussed phase equilibria and the phase diagram of a material.","type":"content","url":"/lecture-09-thermo#summary","position":31},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics"},"type":"lvl1","url":"/lecture-10-stat-thermo","position":0},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics"},"content":"","type":"content","url":"/lecture-10-stat-thermo","position":1},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-10-stat-thermo#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to\n\nDefine the microstate and macrostate of a system.\n\nApply the Boltzmann distribution to calculate the probability of a system being in a given microstate.\n\nExplain the role of the partition function in calculating the thermodynamic properties of a system.","type":"content","url":"/lecture-10-stat-thermo#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl2":"Introduction to Statistical Thermodynamics"},"type":"lvl2","url":"/lecture-10-stat-thermo#introduction-to-statistical-thermodynamics","position":4},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl2":"Introduction to Statistical Thermodynamics"},"content":"In the previous lecture, we discussed the laws of thermodynamics, which describe the behavior of macroscopic systems in terms of energy, work, and heat. In this lecture, we will take a different approach to understanding thermodynamics by considering the behavior of individual particles that make up a system. This approach is known as statistical thermodynamics or statistical mechanics. Statistical thermodynamics provides a microscopic view of thermodynamic systems by considering the statistical distribution of particles in different energy states. By understanding the behavior of individual particles, we can derive macroscopic thermodynamic properties such as temperature, pressure, and entropy.","type":"content","url":"/lecture-10-stat-thermo#introduction-to-statistical-thermodynamics","position":5},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl2":"Microstates and Macrostates"},"type":"lvl2","url":"/lecture-10-stat-thermo#microstates-and-macrostates","position":6},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl2":"Microstates and Macrostates"},"content":"In statistical thermodynamics, we distinguish between two important concepts: microstates and macrostates.\n\nAs shown above, a microstate refers to the specific configuration of a system at the microscopic level, including the positions and velocities of individual particles. For example, in a gas, a microstate would specify the position (center of each arrow) and velocity (direction of each arrow) of each gas molecule. A macrostate, on the other hand, refers to the overall properties of the system that can be observed and measured macroscopically, such as temperature, pressure, and volume. The macrostate of a system is determined by the collection of microstates that are consistent with the macroscopic properties of the system.","type":"content","url":"/lecture-10-stat-thermo#microstates-and-macrostates","position":7},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl2":"Boltzmann Distribution"},"type":"lvl2","url":"/lecture-10-stat-thermo#boltzmann-distribution","position":8},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl2":"Boltzmann Distribution"},"content":"Note\n\nA heat bath is a large reservoir of particles at a constant temperature T that can exchange energy with the system under consideration.\n\nFor a closed system of fixed volume V, in thermal equilibrium with a heat bath at temperature T, the probability of finding the system in a particular microstate i with energy E_i is given by the Boltzmann distributionP_i = \\frac{e^{-E_i/k_{\\rm B}T}}{Z}\n\nwhere k_{\\rm B} is the Boltzmann constant, T is the temperature of the heat bath, and Z is the partition function defined asZ = \\sum_i e^{-E_i/k_{\\rm B}T}\n\nThe partition function Z is a normalization constant that ensures that the probabilities P_i sum to 1 over all microstates of the system.","type":"content","url":"/lecture-10-stat-thermo#boltzmann-distribution","position":9},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl3":"Example: Two-State System","lvl2":"Boltzmann Distribution"},"type":"lvl3","url":"/lecture-10-stat-thermo#example-two-state-system","position":10},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl3":"Example: Two-State System","lvl2":"Boltzmann Distribution"},"content":"Consider a simple two-state system with energies E_1 = 0 and E_2 = \\epsilon. The partition function for this system is given byZ = e^{-0/k_{\\rm B}T} + e^{-\\epsilon/k_{\\rm B}T} = 1 + e^{-\\epsilon/k_{\\rm B}T}\n\nThe probability of the system being in the ground state (E_1 = 0) isP_1 = \\frac{e^{-0/k_{\\rm B}T}}{Z} = \\frac{1}{1 + e^{-\\epsilon/k_{\\rm B}T}}\n\nand the probability of the system being in the excited state (E_2 = \\epsilon) isP_2 = \\frac{e^{-\\epsilon/k_{\\rm B}T}}{Z} = \\frac{e^{-\\epsilon/k_{\\rm B}T}}{1 + e^{-\\epsilon/k_{\\rm B}T}}\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.constants import k, eV\n\n# Constants\nk_B = k / eV  # Boltzmann constant in eV/K\nepsilon = 0.01 # Energy difference in eV\n\n# Temperature range (in Kelvin)\nT = np.linspace(1, 500, 500)\n\n# Partition function Z\nZ = 1 + np.exp(-epsilon / (k_B * T))\n\n# Probabilities of ground state (P1) and excited state (P2)\nP1 = 1 / Z\nP2 = np.exp(-epsilon / (k_B * T)) / Z\n\n# Plotting\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Plotting probabilities\nax1.plot(T, P1, label='Ground state probability ($P_1$)')\nax1.plot(T, P2, label='Excited state probability ($P_2$)')\nax1.set_xlabel('Temperature (K)')\nax1.set_ylabel('Probability')\nax1.set_title('Probabilities of Ground and Excited States')\nax1.legend()\nax1.grid(True)\n\n# Plotting partition function\nax2.plot(T, Z, label='Partition function ($Z$)', color='purple', linestyle='--')\nax2.annotate('One available microstate', xy=(30, 1), xytext=(100, 1.01),\n             arrowprops=dict(facecolor='black', arrowstyle='->'))\nax2.set_xlabel('Temperature (K)')\nax2.set_ylabel('Partition Function ($Z$)')\nax2.set_title('Partition Function')\nax2.legend()\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nThe plot above shows the probabilities of the ground state (P_1) and excited state (P_2) as a function of temperature, as well as the partition function Z. As the temperature increases, the probability of the system being in the excited state increases, while the probability of the system being in the ground state decreases. The partition function Z is a measure of the total number of microstates available to the system. As the temperature increases, the partition function also increases, indicating that the system has more available microstates at higher temperatures.\n\nCritical Thinking\n\nWhat happens to the probabilities P_1 and P_2 at 100 K if \\epsilon is increased?\n\nHow many microstates should be available to the system at very high temperatures?","type":"content","url":"/lecture-10-stat-thermo#example-two-state-system","position":11},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl2":"Thermodynamic Properties from the Partition Function"},"type":"lvl2","url":"/lecture-10-stat-thermo#thermodynamic-properties-from-the-partition-function","position":12},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl2":"Thermodynamic Properties from the Partition Function"},"content":"The partition function Z contains all the information needed to calculate the thermodynamic properties of a system.","type":"content","url":"/lecture-10-stat-thermo#thermodynamic-properties-from-the-partition-function","position":13},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl3":"Average Energy and Internal Energy","lvl2":"Thermodynamic Properties from the Partition Function"},"type":"lvl3","url":"/lecture-10-stat-thermo#average-energy-and-internal-energy","position":14},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl3":"Average Energy and Internal Energy","lvl2":"Thermodynamic Properties from the Partition Function"},"content":"For example, the average energy \\langle E \\rangle of the system is given by\\langle E \\rangle = -\\frac{\\partial}{\\partial \\beta} \\ln Z\n\nwhere \\beta = 1/(k_{\\rm B}T). The microstate-averaged energy \\langle E \\rangle is a measure of the internal energy of the system in a given macrostate.\n\n# Average energy\nE_avg = -np.gradient(np.log(Z), 1 / (k_B * T))\n\n# Plotting average energy\nplt.figure(figsize=(6, 4))\nplt.plot(T, E_avg, color='green')\nplt.annotate('Ground state energy', xy=(30, 0), xytext=(100, 0.0001),\n             arrowprops=dict(facecolor='black', arrowstyle='->'))\nplt.xlabel('Temperature (K)')\nplt.ylabel('Average Energy ($\\\\langle E \\\\rangle$) (eV)')\nplt.title('Average Energy of Two-State System')\nplt.grid(True)\nplt.show()\n\nCritical Thinking\n\nWhat is the average energy of the two-state system at very high temperatures?","type":"content","url":"/lecture-10-stat-thermo#average-energy-and-internal-energy","position":15},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl3":"Heat Capacity at Constant Volume","lvl2":"Thermodynamic Properties from the Partition Function"},"type":"lvl3","url":"/lecture-10-stat-thermo#heat-capacity-at-constant-volume","position":16},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl3":"Heat Capacity at Constant Volume","lvl2":"Thermodynamic Properties from the Partition Function"},"content":"The average energy \\langle E \\rangle can be used to calculate other thermodynamic properties such as the heat capacity C_V of the system. The heat capacity is defined asC_V = \\frac{\\partial \\langle E \\rangle}{\\partial T}\n\nand provides information about how the internal energy of the system changes with temperature.\n\n# Heat capacity\nC_V = np.gradient(E_avg, T)\n\n# Plotting heat capacity\nplt.figure(figsize=(6, 4))\nplt.plot(T, C_V, color='blue')\nplt.xlabel('Temperature (K)')\nplt.ylabel('Heat Capacity ($C_V$) (eV/K)')\nplt.title('Heat Capacity of Two-State System')\nplt.grid(True)\nplt.show()\n\nThe plot above shows the heat capacity C_V of the two-state system as a function of temperature. The heat capacity is zero at low temperatures, peaks at intermediate temperatures, and approaches zero at high temperatures. The peak in the heat capacity corresponds to the temperature at which the system undergoes an order-disorder transition between the ground and excited states.\n\nCritical Thinking\n\nWhat happens to the peak in the heat capacity if the energy difference \\epsilon between the ground and excited states is increased?","type":"content","url":"/lecture-10-stat-thermo#heat-capacity-at-constant-volume","position":17},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl2":"Free Energy and Entropy"},"type":"lvl2","url":"/lecture-10-stat-thermo#free-energy-and-entropy","position":18},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl2":"Free Energy and Entropy"},"content":"The partition function Z can also be used to calculate the free energy F of the system, which is defined asA = -k_{\\rm B}T \\ln Z\n\nThe free energy F is a measure of the energy available to do work in the system. The entropy S of the system can be calculated from the free energy asS = -\\left(\\frac{\\partial A}{\\partial T}\\right)_V\n\nThe entropy S is a measure of the disorder or randomness of the system and provides information about the number of microstates available to the system.\n\n# Free energy\nA = -k_B * T * np.log(Z)\n\n# Entropy\nS = -np.gradient(A, T)\n\n# Plotting free energy and entropy\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Plotting free energy\nax1.plot(T, A, color='red')\nax1.set_xlabel('Temperature (K)')\nax1.set_ylabel('Free Energy ($A$) (eV)')\nax1.set_title('Free Energy of Two-State System')\nax1.grid(True)\n\n# Plotting entropy\nax2.plot(T, S, color='orange')\nax2.set_xlabel('Temperature (K)')\nax2.set_ylabel(r'Entropy ($S$) (eV/K)')\nax2.set_title('Entropy of Two-State System')\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nThe plots above show the free energy A and entropy S of the two-state system as a function of temperature. The free energy decreases with increasing temperature, while the entropy increases with temperature.\n\nCritical Thinking\n\nAt very high temperatures, the probability of the system being in either the ground or excited state approaches 1/2. For conditions where all microstates are equally probable, the entropy of the system can be calculated as S = k_{\\rm B} \\ln \\Omega, where \\Omega is the total number of microstates available to the system. How does the entropy calculated from the partition function compare to the entropy calculated from the total number of microstates at high temperatures?","type":"content","url":"/lecture-10-stat-thermo#free-energy-and-entropy","position":19},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl2":"Summary"},"type":"lvl2","url":"/lecture-10-stat-thermo#summary","position":20},{"hierarchy":{"lvl1":"Chapter 10: Statistical Thermodynamics","lvl2":"Summary"},"content":"In this lecture, we introduced the concept of statistical thermodynamics, which provides a microscopic view of thermodynamic systems by considering the statistical distribution of particles in different energy states. We discussed the distinction between microstates and macrostates and introduced the Boltzmann distribution, which describes the probability of a system being in a given microstate. We also introduced the partition function, which is used to calculate the thermodynamic properties of a system, such as the average energy, heat capacity, free energy, and entropy. By understanding the behavior of individual particles in a system, we can derive macroscopic thermodynamic properties and gain insights into the behavior of complex systems.","type":"content","url":"/lecture-10-stat-thermo#summary","position":21},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity"},"type":"lvl1","url":"/lecture-11-ensembles","position":0},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity"},"content":"","type":"content","url":"/lecture-11-ensembles","position":1},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-11-ensembles#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to\n\nDefine the microcanonical, canonical, isothermal-isobaric, and grand canonical ensembles.\n\nUnderstand the concept of ergodicity and its implications for the relationship between ensemble averages and time averages.","type":"content","url":"/lecture-11-ensembles#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl2":"Why Should You Care About Ensembles?"},"type":"lvl2","url":"/lecture-11-ensembles#why-should-you-care-about-ensembles","position":4},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl2":"Why Should You Care About Ensembles?"},"content":"In the previous lecture, we discussed the concept of a microstate and how it can be used to describe the state of a system. However, in many cases, we are not interested in the properties of individual microstates but rather in the average behavior of a system over many microstates. This is where the concept of an ensemble comes in.\n\nAn ensemble is a collection of systems that are identical in all respects except for the particular microstate they occupy. By studying the properties of an ensemble, we can make predictions about the behavior of a system without having to consider every possible microstate individually.","type":"content","url":"/lecture-11-ensembles#why-should-you-care-about-ensembles","position":5},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl2":"Types of Ensembles"},"type":"lvl2","url":"/lecture-11-ensembles#types-of-ensembles","position":6},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl2":"Types of Ensembles"},"content":"There are several different types of ensembles that are commonly used in statistical mechanics. The choice of ensemble depends on the conditions under which the system is being studied. Each of these ensembles has its own set of constraints and corresponding thermodynamic potentials that can be used to describe the behavior of the system.","type":"content","url":"/lecture-11-ensembles#types-of-ensembles","position":7},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl3":"Microcanonical Ensemble","lvl2":"Types of Ensembles"},"type":"lvl3","url":"/lecture-11-ensembles#microcanonical-ensemble","position":8},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl3":"Microcanonical Ensemble","lvl2":"Types of Ensembles"},"content":"In this ensemble, the system is isolated and has fixed values of energy, volume, and number of particles. The microcanonical ensemble is used to study systems that are closed and do not exchange energy or particles with their surroundings.","type":"content","url":"/lecture-11-ensembles#microcanonical-ensemble","position":9},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl4":"Microcanonical Ensemble: Example","lvl3":"Microcanonical Ensemble","lvl2":"Types of Ensembles"},"type":"lvl4","url":"/lecture-11-ensembles#microcanonical-ensemble-example","position":10},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl4":"Microcanonical Ensemble: Example","lvl3":"Microcanonical Ensemble","lvl2":"Types of Ensembles"},"content":"\n\nConsider a system of one electron in an f orbital. The energy of the electron is quantized, and the electron can occupy one of the 14 f orbitals. The microcanonical ensemble for this system would consist of all possible configurations of the electron in the f orbitals that have the same total energy. The equal a priori probability postulate states that all microstates with the same energy are equally likely to occur. Therefore, the probability of finding the electron in any particular f orbital is the same, i.e., 1/14.","type":"content","url":"/lecture-11-ensembles#microcanonical-ensemble-example","position":11},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl3":"Canonical Ensemble","lvl2":"Types of Ensembles"},"type":"lvl3","url":"/lecture-11-ensembles#canonical-ensemble","position":12},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl3":"Canonical Ensemble","lvl2":"Types of Ensembles"},"content":"In this ensemble, the system is in thermal contact with a heat bath at a fixed temperature. The canonical ensemble is used to study systems that are in thermal equilibrium with their surroundings.","type":"content","url":"/lecture-11-ensembles#canonical-ensemble","position":13},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl3":"Isothermal-Isobaric Ensemble","lvl2":"Types of Ensembles"},"type":"lvl3","url":"/lecture-11-ensembles#isothermal-isobaric-ensemble","position":14},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl3":"Isothermal-Isobaric Ensemble","lvl2":"Types of Ensembles"},"content":"In this ensemble, the system is in thermal and mechanical equilibrium with a heat bath at a fixed temperature and pressure. The isothermal-isobaric ensemble is used to study systems that are in thermal and mechanical equilibrium with their surroundings.","type":"content","url":"/lecture-11-ensembles#isothermal-isobaric-ensemble","position":15},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl4":"Isothermal-Isobaric Ensemble: Example","lvl3":"Isothermal-Isobaric Ensemble","lvl2":"Types of Ensembles"},"type":"lvl4","url":"/lecture-11-ensembles#isothermal-isobaric-ensemble-example","position":16},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl4":"Isothermal-Isobaric Ensemble: Example","lvl3":"Isothermal-Isobaric Ensemble","lvl2":"Types of Ensembles"},"content":"Consider a gas confined to a container with a movable piston. The gas is in thermal equilibrium with a heat bath at a fixed temperature and pressure. The isothermal-isobaric ensemble for this system would consist of all possible configurations of the gas molecules that have the same number of particles, pressure, and temperature.","type":"content","url":"/lecture-11-ensembles#isothermal-isobaric-ensemble-example","position":17},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl3":"Grand Canonical Ensemble","lvl2":"Types of Ensembles"},"type":"lvl3","url":"/lecture-11-ensembles#grand-canonical-ensemble","position":18},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl3":"Grand Canonical Ensemble","lvl2":"Types of Ensembles"},"content":"In this ensemble, the system is in thermal and chemical equilibrium with a heat bath at a fixed temperature and chemical potential. The grand canonical ensemble is used to study systems that are in thermal and chemical equilibrium with their surroundings.","type":"content","url":"/lecture-11-ensembles#grand-canonical-ensemble","position":19},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl4":"Grand Canonical Ensemble: Example","lvl3":"Grand Canonical Ensemble","lvl2":"Types of Ensembles"},"type":"lvl4","url":"/lecture-11-ensembles#grand-canonical-ensemble-example","position":20},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl4":"Grand Canonical Ensemble: Example","lvl3":"Grand Canonical Ensemble","lvl2":"Types of Ensembles"},"content":"Note\n\nThe chemical potential is a measure of the energy required to add a particle to a system.\n\nConsider a model system with sorbent particles that interact with adsorption sites on a solid surface but do not interact with each other.\n\nFor one adsorption site, there are two possible states: empty or occupied. Therefore, the partition function for the system is given byZ = \\sum_{N=0}^{1} e^{\\beta \\mu N}\n\nwhere \\mu is the chemical potential. Since the sorbent particles do not interact with each other, the ensemble average of the coverage of the surface by the sorbent particles \\theta is given by the ensemble average of the number of particles at each adsorption site, which is\\theta = \\langle N \\rangle = \\frac{\\partial \\ln Z}{\\partial \\left( \\beta \\mu \\right)} = \\frac{e^{\\beta \\mu}}{1 + e^{\\beta \\mu}}\n\nIf the sorbent particles are in equilibrium with their ideal gas phase, the chemical potential is given by\\mu = \\mu^{\\circ} + k_B T \\ln \\left( \\frac{P}{P^{\\circ}} \\right)\n\nwhere \\mu^{\\circ} is the standard chemical potential, P is the pressure of the ideal gas phase, and P^{\\circ} is the standard pressure. The coverage of the surface by the sorbent particles can then be calculated as a function of the pressure of the ideal gas phase.\\theta = \\frac{e^{\\beta \\mu^{\\circ}} \\left( \\frac{P}{P^{\\circ}} \\right)}{1 + e^{\\beta \\mu^{\\circ}} \\left( \\frac{P}{P^{\\circ}} \\right)} = \\frac{K P}{1 + K P}\n\nwhere K = e^{\\beta \\mu^{\\circ}} / P^{\\circ}. This equation describes the Langmuir adsorption isotherm, which is commonly used to describe the adsorption of gases on solid surfaces.\n\nThe plot above shows the coverage of the surface by the sorbent particles as a function of the pressure of the ideal gas phase. As the pressure increases, the coverage of the surface also increases, but it reaches a maximum value at high pressures.","type":"content","url":"/lecture-11-ensembles#grand-canonical-ensemble-example","position":21},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl2":"Ergodicity"},"type":"lvl2","url":"/lecture-11-ensembles#ergodicity","position":22},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl2":"Ergodicity"},"content":"The concept of ergodicity is central to the relationship between ensemble averages and time averages in statistical mechanics. A system is said to be ergodic if it explores all of its microstates over time. In an ergodic system, the time average of a property is equal to the ensemble average of that property.","type":"content","url":"/lecture-11-ensembles#ergodicity","position":23},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl3":"Implications of Ergodicity","lvl2":"Ergodicity"},"type":"lvl3","url":"/lecture-11-ensembles#implications-of-ergodicity","position":24},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl3":"Implications of Ergodicity","lvl2":"Ergodicity"},"content":"If a system is ergodic, then the ensemble average of a property can be calculated by averaging the property over time. This means that we can use time averages to make predictions about the behavior of a system without having to consider every possible microstate individually.","type":"content","url":"/lecture-11-ensembles#implications-of-ergodicity","position":25},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl3":"Non-Ergodic Systems","lvl2":"Ergodicity"},"type":"lvl3","url":"/lecture-11-ensembles#non-ergodic-systems","position":26},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl3":"Non-Ergodic Systems","lvl2":"Ergodicity"},"content":"Not all systems are ergodic. In non-ergodic systems, the time average of a property may not be equal to the ensemble average of that property. In such cases, it may be necessary to consider the properties of individual microstates to make accurate predictions about the behavior of the system. Non-ergodic systems are often more difficult to study than ergodic systems because they require a more detailed understanding of the properties of individual microstates. However, many real-world systems are non-ergodic, and understanding their behavior is an important area of research in statistical mechanics.","type":"content","url":"/lecture-11-ensembles#non-ergodic-systems","position":27},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl2":"Summary"},"type":"lvl2","url":"/lecture-11-ensembles#summary","position":28},{"hierarchy":{"lvl1":"Chapter 11: Ensembles and Ergodicity","lvl2":"Summary"},"content":"In this lecture, we discussed the concept of ensembles and how they can be used to study the average behavior of a system over many microstates. We also introduced the concept of ergodicity and its implications for the relationship between ensemble averages and time averages. By understanding these concepts, we can make predictions about the behavior of complex systems without having to consider every possible microstate individually.\n\nWhat’s Next?\n\nNow, we’re ready to take the next step: developing our own simulation codes to explore the behavior of systems using Monte Carlo and molecular dynamics techniques.","type":"content","url":"/lecture-11-ensembles#summary","position":29},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method"},"type":"lvl1","url":"/lecture-12-monte-carlo","position":0},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method"},"content":"","type":"content","url":"/lecture-12-monte-carlo","position":1},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-12-monte-carlo#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to\n\nUnderstand the basic principles of the Monte Carlo method.\n\nApply Monte Carlo sampling to estimate integrals.\n\nExplain the concept of importance sampling and its benefits.","type":"content","url":"/lecture-12-monte-carlo#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl2":"Introduction to Monte Carlo Method"},"type":"lvl2","url":"/lecture-12-monte-carlo#introduction-to-monte-carlo-method","position":4},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl2":"Introduction to Monte Carlo Method"},"content":"The Monte Carlo method is a powerful computational technique used to solve problems by relying on random sampling. To illustrate its basic idea, let’s start with a simple example: evaluating the integral of  f(x) = x^2  over the interval [0, 1]I = \\int_0^1 x^2 \\, dx","type":"content","url":"/lecture-12-monte-carlo#introduction-to-monte-carlo-method","position":5},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Relating Integrals to Averages","lvl2":"Introduction to Monte Carlo Method"},"type":"lvl3","url":"/lecture-12-monte-carlo#relating-integrals-to-averages","position":6},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Relating Integrals to Averages","lvl2":"Introduction to Monte Carlo Method"},"content":"Recall that the average value of a function  f(x)  over an interval [a, b] is given by\\langle f(x) \\rangle = \\frac{1}{b - a} \\int_a^b f(x) \\, dx\n\nIn this case, we can write the integral as the average of  x^2  over [0, 1]I = \\langle x^2 \\rangle = \\int_0^1 x^2 \\, dx","type":"content","url":"/lecture-12-monte-carlo#relating-integrals-to-averages","position":7},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Monte Carlo Estimation","lvl2":"Introduction to Monte Carlo Method"},"type":"lvl3","url":"/lecture-12-monte-carlo#monte-carlo-estimation","position":8},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Monte Carlo Estimation","lvl2":"Introduction to Monte Carlo Method"},"content":"Instead of solving this integral analytically, we can approximate it by randomly sampling points from the interval [0, 1] and evaluating  x^2  at those points. This is the essence of the Monte Carlo method.\n\nSuppose we generate  N  random numbers  x_i  uniformly distributed between 0 and 1. We can approximate the integral as the average of  x_i^2 I \\approx \\frac{1}{N} \\sum_{i=1}^N x_i^2\n\nThis gives us an estimate for the integral. The larger the number of random samples  N , the closer the estimate gets to the true value.","type":"content","url":"/lecture-12-monte-carlo#monte-carlo-estimation","position":9},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Python Implementation","lvl2":"Introduction to Monte Carlo Method"},"type":"lvl3","url":"/lecture-12-monte-carlo#python-implementation","position":10},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Python Implementation","lvl2":"Introduction to Monte Carlo Method"},"content":"The following Python code demonstrates how to estimate the integral using Monte Carlo sampling.\n\nimport numpy as np\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Number of random samples\nN = 1000\n\n# Generate N random numbers uniformly distributed between 0 and 1\nx = np.random.rand(N)\n\n# Estimate the average of x^2 over [0, 1]\naverage_x_squared = np.mean(x**2)\n\n# Estimate the variance of the estimator\nvariance = np.var(x**2)\n\n# Print the result with a descriptive message\nprint(f\"The estimated average of x^2 over [0, 1] with {N} samples is: {average_x_squared:.6f}\")\nprint(f\"The estimated variance of the estimator is: {variance:.6f}\")\nprint(f\"The exact value of the integral is: {1/3:.6f}\")\n\nTip\n\nIncreasing the number of random samples  N  reduces the variance of the estimate and improves the accuracy of the approximation.","type":"content","url":"/lecture-12-monte-carlo#python-implementation","position":11},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl2":"Importance Sampling"},"type":"lvl2","url":"/lecture-12-monte-carlo#importance-sampling","position":12},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl2":"Importance Sampling"},"content":"Note\n\nThe uniform distribution is the simplest probability distribution, characterized by a constant probability density function over a given interval.\n\nIn the previous example, we used a uniform distribution to generate random numbers for our Monte Carlo simulation. However, we can often improve the efficiency of these simulations by employing a different probability distribution—this approach is known as importance sampling.","type":"content","url":"/lecture-12-monte-carlo#importance-sampling","position":13},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Motivation for Importance Sampling","lvl2":"Importance Sampling"},"type":"lvl3","url":"/lecture-12-monte-carlo#motivation-for-importance-sampling","position":14},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Motivation for Importance Sampling","lvl2":"Importance Sampling"},"content":"Let’s reconsider the goal of estimating the integralI = \\int_0^1 x^2 \\, dx\n\nUsing a uniform distribution, the Monte Carlo estimator would directly sample from this integral. However, efficiency can be improved by selecting a distribution that matches the behavior of the integrand, i.e., a distribution that emphasizes regions where x^2 is larger. This results in a smaller variance of the estimator.","type":"content","url":"/lecture-12-monte-carlo#motivation-for-importance-sampling","position":15},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Rewriting the Integral","lvl2":"Importance Sampling"},"type":"lvl3","url":"/lecture-12-monte-carlo#rewriting-the-integral","position":16},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Rewriting the Integral","lvl2":"Importance Sampling"},"content":"We can rewrite the integral asI = \\int_0^1 \\frac{x^2}{g(x)} g(x) \\, dx\n\nwhere g(x) is any probability distribution (with support over [0, 1]) that can be used to sample points more efficiently. The idea is to choose a g(x) that is similar to the shape of the function x^2. This way, the sampling is concentrated in regions where the function x^2 contributes more to the integral.","type":"content","url":"/lecture-12-monte-carlo#rewriting-the-integral","position":17},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Choosing a Suitable g(x)","lvl2":"Importance Sampling"},"type":"lvl3","url":"/lecture-12-monte-carlo#choosing-a-suitable-g-x","position":18},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Choosing a Suitable g(x)","lvl2":"Importance Sampling"},"content":"A good candidate for g(x) is the Beta distribution with parameters \\alpha = 3 and \\beta = 1. The probability density function of the Beta distribution isg(x) = \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{B(\\alpha, \\beta)}\n\nwhere B(\\alpha, \\beta) is the Beta function, and in this case, it biases the samples toward larger values of x, aligning well with the shape of x^2.","type":"content","url":"/lecture-12-monte-carlo#choosing-a-suitable-g-x","position":19},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Monte Carlo Estimator with Importance Sampling","lvl2":"Importance Sampling"},"type":"lvl3","url":"/lecture-12-monte-carlo#monte-carlo-estimator-with-importance-sampling","position":20},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Monte Carlo Estimator with Importance Sampling","lvl2":"Importance Sampling"},"content":"Using the Beta distribution, the integral can be approximated asI \\approx \\frac{1}{N} \\sum_{i=1}^N \\frac{x_i^2}{g(x_i)}\n\nwhere x_i are samples drawn from the Beta distribution.","type":"content","url":"/lecture-12-monte-carlo#monte-carlo-estimator-with-importance-sampling","position":21},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Implementation in Python","lvl2":"Importance Sampling"},"type":"lvl3","url":"/lecture-12-monte-carlo#implementation-in-python","position":22},{"hierarchy":{"lvl1":"Chapter 12: The Monte Carlo Method","lvl3":"Implementation in Python","lvl2":"Importance Sampling"},"content":"The following code demonstrates how to estimate the integral of x^2 using importance sampling.\n\nimport numpy as np\nfrom scipy.stats import beta\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Number of random samples\nN = 100\n\n# Parameters of the Beta distribution (biasing towards larger x)\nalpha, beta_param = 3, 1\n\n# Generate N random samples from the Beta distribution\nx = np.random.beta(alpha, beta_param, size=N)\n\n# Evaluate the integrand at the random numbers (x^2)\nf_x = x**2\n\n# Evaluate the importance sampling weights\ng_x = beta.pdf(x, alpha, beta_param)  # PDF of the Beta distribution\n\n# Since the target is the uniform distribution on [0, 1], its PDF is constant 1\n# So the weights become 1/g(x)\nweights = 1 / g_x\n\n# Estimate the integral using importance sampling\nestimated_integral = np.mean(f_x * weights)\n\n# Estimate the variance of the estimator\nvariance = np.var(f_x * weights)\n\n# Print the result with a descriptive message\nprint(f\"The estimated value of the integral of x^2 over [0, 1] using importance sampling is: {estimated_integral:.6f}\")\nprint(f\"The estimated variance of the estimator is: {variance}\")\nprint(f\"The exact value of the integral is: {1/3:.6f}\")","type":"content","url":"/lecture-12-monte-carlo#implementation-in-python","position":23},{"hierarchy":{"lvl1":"Chapter 13: Monte Carlo Integration"},"type":"lvl1","url":"/lecture-13-mc-integration","position":0},{"hierarchy":{"lvl1":"Chapter 13: Monte Carlo Integration"},"content":"","type":"content","url":"/lecture-13-mc-integration","position":1},{"hierarchy":{"lvl1":"Chapter 13: Monte Carlo Integration","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-13-mc-integration#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 13: Monte Carlo Integration","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to\n\nApply random sampling to calculate the overlap integral of two H 1s orbitals.\n\nApply importance sampling to improve the efficiency of Monte Carlo integration.","type":"content","url":"/lecture-13-mc-integration#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 13: Monte Carlo Integration","lvl2":"Return to the Overlap Integral"},"type":"lvl2","url":"/lecture-13-mc-integration#return-to-the-overlap-integral","position":4},{"hierarchy":{"lvl1":"Chapter 13: Monte Carlo Integration","lvl2":"Return to the Overlap Integral"},"content":"In lecture 5, we calculated the overlap integral of two H 1s orbitals using a grid-based numerical integration method. In this lecture, we will calculate the same integral using Monte Carlo integration. The integral we want to calculate isS = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\psi_{1s}^*(x + R / 2, y, z) \\psi_{1s}(x - R / 2, y, z) dx dy dz\n\nwhere \\psi_{1s} is the 1s orbital of hydrogen and R is the distance between the two hydrogen atoms. We will set R = 1 a_0.","type":"content","url":"/lecture-13-mc-integration#return-to-the-overlap-integral","position":5},{"hierarchy":{"lvl1":"Chapter 13: Monte Carlo Integration","lvl2":"Random Sampling"},"type":"lvl2","url":"/lecture-13-mc-integration#random-sampling","position":6},{"hierarchy":{"lvl1":"Chapter 13: Monte Carlo Integration","lvl2":"Random Sampling"},"content":"Note\n\nWe multiply by eight because we are only sampling in the first octant.\n\nFirst, we will use random sampling to calculate the overlap integral. We will generate random points in the region 0 \\leq x \\leq 7 a_0, 0 \\leq y \\leq 7 a_0, and 0 \\leq z \\leq 7 a_0. We will then calculate the value of the integrand at each point, average the value, and multiply by eight to get the integral. The code to do this is shown below.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML\n\ndef psi_1s(x, y, z):\n    \"\"\"\n    Calculate the value of the 1s orbital of hydrogen at a given point.\n\n    Parameters\n    ----------\n    x : float\n        The x-coordinate of the point.\n    y : float\n        The y-coordinate of the point.\n    z : float\n        The z-coordinate of the point.\n\n    Returns\n    -------\n    float\n        The value of the 1s orbital at the given point.\n    \"\"\"\n    r = np.sqrt(x**2 + y**2 + z**2)\n    value = (1 / (np.sqrt(np.pi))) * np.exp(-r)\n    return value\n\n# Set the random seed for reproducibility\nnp.random.seed(42)\n\n# Set the number of points to sample\nn_points_list = [100, 1000, 10000, 100000, 1000000]\n\n# Set the integration limits\na = 0\nb = 7\n\n# Create lists to store the results\naverages = []\nstd_devs = []\n\n# Loop over the number of points to sample\nfor n_points in n_points_list:\n    x = np.random.uniform(a, b, n_points)\n    y = np.random.uniform(a, b, n_points)\n    z = np.random.uniform(a, b, n_points)\n    integrand = psi_1s(x + 0.5, y, z) * psi_1s(x - 0.5, y, z)\n    integral = 8 * np.mean(integrand) * (b - a)**3\n    variance = 8 * np.var(integrand) * (b - a)**3\n    averages.append(integral)\n    std_devs.append(np.sqrt(variance))\n\n# Plot the results\nplt.figure(figsize=(8, 6))\nplt.errorbar(n_points_list, averages, yerr=std_devs, fmt='o-')\nplt.axhline(0.858385, color='black', linestyle='--', label='Exact Value')\nplt.xscale('log')\nplt.xlabel('Number of Points Sampled')\nplt.ylabel('Integral Value')\nplt.title('Overlap Integral of Two H 1s Orbitals')\nplt.legend()\nplt.show()\n\nThe plot above shows the value of the integral as a function of the number of points sampled. The exact value of the integral is shown as a dashed line. As you can see, the value of the integral converges to the exact value as the number of points sampled increases.","type":"content","url":"/lecture-13-mc-integration#random-sampling","position":7},{"hierarchy":{"lvl1":"Chapter 13: Monte Carlo Integration","lvl2":"Importance Sampling"},"type":"lvl2","url":"/lecture-13-mc-integration#importance-sampling","position":8},{"hierarchy":{"lvl1":"Chapter 13: Monte Carlo Integration","lvl2":"Importance Sampling"},"content":"Next, we will use importance sampling to improve the efficiency of the Monte Carlo integration. We will sample points from a distribution that is proportional to the integrand, rather than a uniform distribution.","type":"content","url":"/lecture-13-mc-integration#importance-sampling","position":9},{"hierarchy":{"lvl1":"Chapter 13: Monte Carlo Integration","lvl3":"Choosing the Importance Sampling Distribution","lvl2":"Importance Sampling"},"type":"lvl3","url":"/lecture-13-mc-integration#choosing-the-importance-sampling-distribution","position":10},{"hierarchy":{"lvl1":"Chapter 13: Monte Carlo Integration","lvl3":"Choosing the Importance Sampling Distribution","lvl2":"Importance Sampling"},"content":"To choose the importance sampling distribution, we need to find a function g(x, y, z) that is proportional to the integrand \\psi_{1s}^*(x + R / 2, y, z) \\psi_{1s}(x - R / 2, y, z). We can do this by noting that the integrand is a product of two 1s orbitals, which are spherically symmetric. Therefore, we can choose the importance sampling distribution to be a spherically symmetric distribution. One possible choice is the exponential distributiong(x, y, z) = \\exp(-\\sqrt{x^2 + y^2 + z^2})\n\nLet’s plot the integrand and the importance sampling distribution to see if they are similar.\n\nfrom scipy.stats import expon\n\n# Plot the integrand and the importance sampling distribution\nx = np.linspace(0, 7, 100)\ny = 0\nz = 0\nintegrand = psi_1s(x + 0.5, y, z) * psi_1s(x - 0.5, y, z)\nimportance_sampling = expon.pdf(x)\nplt.figure(figsize=(8, 6))\nplt.plot(x, integrand, label='Integrand')\nplt.plot(x, importance_sampling, label='Importance Sampling Distribution')\nplt.xlabel('x')\nplt.ylabel('Value')\nplt.title('Integrand and Importance Sampling Distribution')\nplt.legend()\nplt.show()\n\nAs you can see from the plot above, the integrand and the importance sampling distribution are similar, which means that the importance sampling distribution is a good choice.\n\nTo use importance sampling, we need to modify the Monte Carlo integration code to sample points from the importance sampling distribution. The code to do this is shown below.\n\n# Set the number of points to sample\nn_points_list = [100, 1000, 10000, 100000, 1000000]\n\n# Create lists to store the results\naverages = []\nstd_devs = []\n\n# Loop over the number of points to sample\nfor n_points in n_points_list:\n    x = expon.rvs(size=n_points, scale=1)\n    y = expon.rvs(size=n_points, scale=1)\n    z = expon.rvs(size=n_points, scale=1)\n    numer = psi_1s(x + 0.5, y, z) * psi_1s(x - 0.5, y, z)\n    denom = expon.pdf(x) * expon.pdf(y) * expon.pdf(z)\n    integrand = numer / denom\n    integral = 8 * np.mean(integrand)\n    variance = 8 * np.var(integrand)\n    averages.append(integral)\n    std_devs.append(np.sqrt(variance))\n\n# Plot the results\nplt.figure(figsize=(8, 6))\nplt.errorbar(n_points_list, averages, yerr=std_devs, fmt='o-')\nplt.axhline(0.858385, color='black', linestyle='--', label='Exact Value')\nplt.xscale('log')\nplt.xlabel('Number of Points Sampled')\nplt.ylabel('Integral Value')\nplt.title('Overlap Integral of Two H 1s Orbitals (Importance Sampling)')\nplt.legend()\nplt.show()\n\nThe plot above shows the value of the integral as a function of the number of points sampled using importance sampling. The exact value of the integral is shown as a dashed line. As you can see, the value of the integral converges to the exact value much faster using importance sampling compared to random sampling.","type":"content","url":"/lecture-13-mc-integration#choosing-the-importance-sampling-distribution","position":11},{"hierarchy":{"lvl1":"Chapter 13: Monte Carlo Integration","lvl2":"Summary"},"type":"lvl2","url":"/lecture-13-mc-integration#summary","position":12},{"hierarchy":{"lvl1":"Chapter 13: Monte Carlo Integration","lvl2":"Summary"},"content":"In this lecture, we used Monte Carlo integration to calculate the overlap integral of two H 1s orbitals. We first used random sampling and then improved the efficiency of the calculation using importance sampling. We found that importance sampling converges to the exact value much faster than random sampling.","type":"content","url":"/lecture-13-mc-integration#summary","position":13},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm"},"type":"lvl1","url":"/lecture-14-metropolis","position":0},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm"},"content":"","type":"content","url":"/lecture-14-metropolis","position":1},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-14-metropolis#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to\n\nDerive the Metropolis algorithm for sampling from a probability distribution.\n\nImplement a basic Metropolis algorithm in Python.\n\nApply the Metropolis algorithm to sample thermally accessible configurations of a simple system.","type":"content","url":"/lecture-14-metropolis#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Introduction"},"type":"lvl2","url":"/lecture-14-metropolis#introduction","position":4},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Introduction"},"content":"Evaluating statistical averages of the form\\langle A \\rangle = \\frac{\\int d\\mathbf{r}^N \\, e^{-\\beta U(\\mathbf{r}^N)} A(\\mathbf{r}^N)}{\\int d\\mathbf{r}^N \\, e^{-\\beta U(\\mathbf{r}^N)}} = \\int d\\mathbf{r}^N \\, p(\\mathbf{r}^N) A(\\mathbf{r}^N),\n\nwhere \\mathbf{r}^N represents the coordinates of a system of N particles, U(\\mathbf{r}^N) is the potential energy, \\beta = 1/(k_B T) is the inverse temperature, p(\\mathbf{r}^N) = e^{-\\beta U(\\mathbf{r}^N)}/Z is the Boltzmann probability distribution, and Z is the partition function, can be challenging due to the high dimensionality of the integral. The Metropolis algorithm provides a way to sample configurations from p(\\mathbf{r}^N), allowing us to approximate the average \\langle A \\rangle by\\langle A \\rangle \\approx \\frac{1}{M} \\sum_{i=1}^M A(\\mathbf{r}^N_i),\n\nwhere M is the number of sampled configurations \\mathbf{r}^N_i.","type":"content","url":"/lecture-14-metropolis#introduction","position":5},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"The Metropolis Algorithm: “Measuring the Depth of the Mississippi”"},"type":"lvl2","url":"/lecture-14-metropolis#the-metropolis-algorithm-measuring-the-depth-of-the-mississippi","position":6},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"The Metropolis Algorithm: “Measuring the Depth of the Mississippi”"},"content":"\n\nTraditional numerical integration methods, like quadrature, evaluate the integrand at predefined grid points, which can be inefficient in high-dimensional spaces or when the integrand is significant only in a small region of the domain. This is analogous to measuring the depth of the Mississippi River at regular intervals, regardless of whether those points are over water or land. The Metropolis algorithm improves upon this by focusing computational effort on regions where the integrand (or probability density) is large—much like concentrating measurements over the river itself rather than the surrounding land.","type":"content","url":"/lecture-14-metropolis#the-metropolis-algorithm-measuring-the-depth-of-the-mississippi","position":7},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"Derivation of the Metropolis Algorithm","lvl2":"The Metropolis Algorithm: “Measuring the Depth of the Mississippi”"},"type":"lvl3","url":"/lecture-14-metropolis#derivation-of-the-metropolis-algorithm","position":8},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"Derivation of the Metropolis Algorithm","lvl2":"The Metropolis Algorithm: “Measuring the Depth of the Mississippi”"},"content":"To sample from the probability distribution p(\\mathbf{r}^N), we construct a Markov chain where the probability of transitioning from one configuration to another depends only on those two configurations.","type":"content","url":"/lecture-14-metropolis#derivation-of-the-metropolis-algorithm","position":9},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl4":"Detailed Balance Condition","lvl3":"Derivation of the Metropolis Algorithm","lvl2":"The Metropolis Algorithm: “Measuring the Depth of the Mississippi”"},"type":"lvl4","url":"/lecture-14-metropolis#detailed-balance-condition","position":10},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl4":"Detailed Balance Condition","lvl3":"Derivation of the Metropolis Algorithm","lvl2":"The Metropolis Algorithm: “Measuring the Depth of the Mississippi”"},"content":"For the Markov chain to converge to the desired distribution p(\\mathbf{r}^N), it must satisfy the detailed balance condition\\pi(\\mathbf{r}_{\\text{old}} \\rightarrow \\mathbf{r}_{\\text{new}}) \\, p(\\mathbf{r}_{\\text{old}}) = \\pi(\\mathbf{r}_{\\text{new}} \\rightarrow \\mathbf{r}_{\\text{old}}) \\, p(\\mathbf{r}_{\\text{new}}),\n\nwhere \\pi(\\mathbf{r}_{\\text{old}} \\rightarrow \\mathbf{r}_{\\text{new}}) is the total transition probability from the old configuration to the new one. We can decompose \\pi into two parts:\n\nProposal probability \\alpha(\\mathbf{r}_{\\text{old}} \\rightarrow \\mathbf{r}_{\\text{new}}): The probability of proposing a move to \\mathbf{r}_{\\text{new}}.\n\nAcceptance probability \\text{acc}(\\mathbf{r}_{\\text{old}} \\rightarrow \\mathbf{r}_{\\text{new}}): The probability of accepting the proposed move.\n\nThus,\\pi(\\mathbf{r}_{\\text{old}} \\rightarrow \\mathbf{r}_{\\text{new}}) = \\alpha(\\mathbf{r}_{\\text{old}} \\rightarrow \\mathbf{r}_{\\text{new}}) \\, \\text{acc}(\\mathbf{r}_{\\text{old}} \\rightarrow \\mathbf{r}_{\\text{new}}).\n\nIf we choose a symmetric proposal distribution such that\\alpha(\\mathbf{r}_{\\text{old}} \\rightarrow \\mathbf{r}_{\\text{new}}) = \\alpha(\\mathbf{r}_{\\text{new}} \\rightarrow \\mathbf{r}_{\\text{old}}),\n\nthe detailed balance condition simplifies to\\frac{\\text{acc}(\\mathbf{r}_{\\text{old}} \\rightarrow \\mathbf{r}_{\\text{new}})}{\\text{acc}(\\mathbf{r}_{\\text{new}} \\rightarrow \\mathbf{r}_{\\text{old}})} = \\frac{p(\\mathbf{r}_{\\text{new}})}{p(\\mathbf{r}_{\\text{old}})} = e^{-\\beta [U(\\mathbf{r}_{\\text{new}}) - U(\\mathbf{r}_{\\text{old}})]}.","type":"content","url":"/lecture-14-metropolis#detailed-balance-condition","position":11},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl4":"Acceptance Probability","lvl3":"Derivation of the Metropolis Algorithm","lvl2":"The Metropolis Algorithm: “Measuring the Depth of the Mississippi”"},"type":"lvl4","url":"/lecture-14-metropolis#acceptance-probability","position":12},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl4":"Acceptance Probability","lvl3":"Derivation of the Metropolis Algorithm","lvl2":"The Metropolis Algorithm: “Measuring the Depth of the Mississippi”"},"content":"Metropolis et al. proposed setting the acceptance probability to\\text{acc}(\\mathbf{r}_{\\text{old}} \\rightarrow \\mathbf{r}_{\\text{new}}) = \\min\\left[1, e^{-\\beta [U(\\mathbf{r}_{\\text{new}}) - U(\\mathbf{r}_{\\text{old}})]}\\right].\n\nThis choice satisfies the detailed balance condition and ensures that the Markov chain will converge to the Boltzmann distribution.","type":"content","url":"/lecture-14-metropolis#acceptance-probability","position":13},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"The Metropolis Algorithm Steps","lvl2":"The Metropolis Algorithm: “Measuring the Depth of the Mississippi”"},"type":"lvl3","url":"/lecture-14-metropolis#the-metropolis-algorithm-steps","position":14},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"The Metropolis Algorithm Steps","lvl2":"The Metropolis Algorithm: “Measuring the Depth of the Mississippi”"},"content":"Initialization: Choose an initial configuration \\mathbf{r}_{\\text{old}}.\n\nProposal: Generate a new configuration \\mathbf{r}_{\\text{new}} by making a small random change to \\mathbf{r}_{\\text{old}}.\n\nCompute Acceptance Probability:\\text{acc} = \\min\\left[1, e^{-\\beta [U(\\mathbf{r}_{\\text{new}}) - U(\\mathbf{r}_{\\text{old}})]}\\right].\n\nAccept or Reject Move:\n\nGenerate a random number r \\in [0, 1].\n\nIf r < \\text{acc}, accept the move (\\mathbf{r}_{\\text{new}} becomes the current configuration).\n\nElse, reject the move (retain \\mathbf{r}_{\\text{old}}).\n\nIteration: Repeat steps 2–4 for a large number of steps to sample the distribution.","type":"content","url":"/lecture-14-metropolis#the-metropolis-algorithm-steps","position":15},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Example: Sampling a Classical Morse Oscillator"},"type":"lvl2","url":"/lecture-14-metropolis#example-sampling-a-classical-morse-oscillator","position":16},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Example: Sampling a Classical Morse Oscillator"},"content":"The Morse potential models the potential energy of a diatomic moleculeU(x) = D_e \\left[1 - e^{-\\alpha (x - x_e)}\\right]^2,\n\nwhere D_e is the dissociation energy, x_e is the equilibrium bond length, \\alpha determines the width of the potential well, and x is the bond length.","type":"content","url":"/lecture-14-metropolis#example-sampling-a-classical-morse-oscillator","position":17},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"Visualization of the Morse Potential","lvl2":"Example: Sampling a Classical Morse Oscillator"},"type":"lvl3","url":"/lecture-14-metropolis#visualization-of-the-morse-potential","position":18},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"Visualization of the Morse Potential","lvl2":"Example: Sampling a Classical Morse Oscillator"},"content":"\n\nThe potential well is asymmetric, leading to interesting thermal properties like thermal expansion.","type":"content","url":"/lecture-14-metropolis#visualization-of-the-morse-potential","position":19},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Implementing the Metropolis Algorithm in Python"},"type":"lvl2","url":"/lecture-14-metropolis#implementing-the-metropolis-algorithm-in-python","position":20},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Implementing the Metropolis Algorithm in Python"},"content":"We’ll implement the Metropolis algorithm to sample configurations from the Boltzmann distribution of the Morse oscillator.\n\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.constants import k, eV\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Constants\nk_B = k / eV  # Boltzmann constant in eV/K\n\n# Morse potential parameters\nD_e = 1.0      # Dissociation energy in eV\nx_e = 1.0      # Equilibrium bond length\nalpha = 1.5    # Width parameter in 1/Å\nT = 300.0      # Temperature in K\nbeta = 1.0 / (k_B * T)\n\n# Define the Morse potential function\ndef morse_potential(x):\n    return D_e * (1 - np.exp(-alpha * (x - x_e)))**2\n\n# Define the Metropolis algorithm\ndef metropolis_sampling(x_init, n_steps, beta, delta):\n    x = x_init\n    samples = []\n    for _ in range(n_steps):\n        # Propose a new position\n        x_new = x + np.random.uniform(-delta, delta)\n        \n        # Compute the change in potential energy\n        delta_U = morse_potential(x_new) - morse_potential(x)\n        \n        # Acceptance probability\n        acceptance_prob = min(1.0, np.exp(-beta * delta_U))\n        \n        # Accept or reject the new position\n        if np.random.rand() < acceptance_prob:\n            x = x_new  # Accept the move\n        \n        samples.append(x)\n    return np.array(samples)\n\n","type":"content","url":"/lecture-14-metropolis#implementing-the-metropolis-algorithm-in-python","position":21},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"Explanation of the Code","lvl2":"Implementing the Metropolis Algorithm in Python"},"type":"lvl3","url":"/lecture-14-metropolis#explanation-of-the-code","position":22},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"Explanation of the Code","lvl2":"Implementing the Metropolis Algorithm in Python"},"content":"morse_potential(x): Computes the Morse potential at position x.\n\nmetropolis_sampling(x_init, n_steps, beta, delta):\n\nx_init: Initial position.\n\nn_steps: Number of sampling steps.\n\nbeta: Inverse temperature.\n\ndelta: Maximum change allowed in a single move (controls step size).\n\nThe function returns an array of sampled positions.","type":"content","url":"/lecture-14-metropolis#explanation-of-the-code","position":23},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Running the Simulation"},"type":"lvl2","url":"/lecture-14-metropolis#running-the-simulation","position":24},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Running the Simulation"},"content":"\n\n# Simulation parameters\nx_initial = x_e  # Start at equilibrium position\nn_equilibration = 10000\nn_sampling = 50000\ndelta = 0.1      # Maximum step size\n\n# Equilibration phase\nx_equilibrated = metropolis_sampling(x_initial, n_equilibration, beta, delta)\n\n# Sampling phase\nx_samples = metropolis_sampling(x_equilibrated[-1], n_sampling, beta, delta)\n\nNote\n\nThe equilibration phase allows the system to reach equilibrium before sampling. We discard these initial samples to ensure the system is in a stable configuration. The sampling phase collects samples for analysis.","type":"content","url":"/lecture-14-metropolis#running-the-simulation","position":25},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Analyzing the Results"},"type":"lvl2","url":"/lecture-14-metropolis#analyzing-the-results","position":26},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Analyzing the Results"},"content":"","type":"content","url":"/lecture-14-metropolis#analyzing-the-results","position":27},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"Average Bond Length","lvl2":"Analyzing the Results"},"type":"lvl3","url":"/lecture-14-metropolis#average-bond-length","position":28},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"Average Bond Length","lvl2":"Analyzing the Results"},"content":"\n\n# Compute the average bond length\nx_avg = np.mean(x_samples)\nprint(f\"Average bond length at {T} K: {x_avg:.4f} Å\")\n\n","type":"content","url":"/lecture-14-metropolis#average-bond-length","position":29},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"Plotting the Results","lvl2":"Analyzing the Results"},"type":"lvl3","url":"/lecture-14-metropolis#plotting-the-results","position":30},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"Plotting the Results","lvl2":"Analyzing the Results"},"content":"Note\n\n“Time” here refers to the number of sampling steps.\n\n# Plotting the sampled positions over \"time\"\nplt.figure(figsize=(12, 5))\n\n# Left: \"Time series\" of bond lengths\nplt.subplot(1, 2, 1)\nplt.plot(x_samples, color='blue', alpha=0.5)\nplt.hlines([x_e, x_avg], xmin=0, xmax=n_sampling, colors=['red', 'green'], linestyles='dashed')\nplt.xlabel('Step')\nplt.ylabel('Bond Length (Å)')\nplt.title('Metropolis Sampling of Bond Length')\nplt.legend([r'$x_{\\text{sample}}$', r'$x_e$', r'$\\langle x \\rangle$'])\n\n# Right: Histogram and potential\nplt.subplot(1, 2, 2)\nx_range = np.linspace(x_e - 0.5, x_e + 1.0, 500)\nplt.plot(x_range, morse_potential(x_range), 'k-', label='Morse Potential')\nplt.hist(x_samples, bins=50, density=True, color='lightblue', alpha=0.7, label='Sampled Distribution')\nplt.xlabel('Bond Length (Å)')\nplt.ylabel('Probability Density')\nplt.title('Bond Length Distribution')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nInterpretation\n\nThe left plot shows how the bond length fluctuates over “time” during the simulation. The right plot overlays the Morse potential with the histogram of sampled bond lengths, showing that the sampling is concentrated near the minimum of the potential but is skewed towards longer bond lengths due to the asymmetry of the Morse potential.","type":"content","url":"/lecture-14-metropolis#plotting-the-results","position":31},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Thermal Expansion of the Morse Oscillator"},"type":"lvl2","url":"/lecture-14-metropolis#thermal-expansion-of-the-morse-oscillator","position":32},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Thermal Expansion of the Morse Oscillator"},"content":"We now investigate how the average bond length changes with temperature, illustrating thermal expansion.","type":"content","url":"/lecture-14-metropolis#thermal-expansion-of-the-morse-oscillator","position":33},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"Simulation Over a Range of Temperatures","lvl2":"Thermal Expansion of the Morse Oscillator"},"type":"lvl3","url":"/lecture-14-metropolis#simulation-over-a-range-of-temperatures","position":34},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"Simulation Over a Range of Temperatures","lvl2":"Thermal Expansion of the Morse Oscillator"},"content":"\n\n# Temperature range from 100 K to 1000 K\nT_values = np.linspace(100, 1000, 10)\nx_averages = []\n\nfor T in T_values:\n    beta = 1.0 / (k_B * T)\n    # Re-equilibrate at new temperature\n    _ = metropolis_sampling(x_e, n_equilibration, beta, delta)\n    # Sample\n    x_samples = metropolis_sampling(x_e, n_sampling, beta, delta)\n    # Compute average bond length\n    x_avg = np.mean(x_samples)\n    x_averages.append(x_avg)\n\n# Convert results to numpy arrays\nT_values = np.array(T_values)\nx_averages = np.array(x_averages)\n\n","type":"content","url":"/lecture-14-metropolis#simulation-over-a-range-of-temperatures","position":35},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"Plotting Thermal Expansion","lvl2":"Thermal Expansion of the Morse Oscillator"},"type":"lvl3","url":"/lecture-14-metropolis#plotting-thermal-expansion","position":36},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl3":"Plotting Thermal Expansion","lvl2":"Thermal Expansion of the Morse Oscillator"},"content":"\n\n# Plot average bond length vs. temperature\nplt.figure(figsize=(8, 6))\nplt.plot(T_values, x_averages, 'o-', color='darkblue')\nplt.xlabel('Temperature (K)')\nplt.ylabel('Average Bond Length (Å)')\nplt.title('Thermal Expansion of the Morse Oscillator')\nplt.grid(True)\nplt.show()\n\nInterpretation\n\nThe plot shows that the average bond length increases with temperature. This is due to the anharmonicity of the Morse potential, which causes asymmetry in the potential energy well.","type":"content","url":"/lecture-14-metropolis#plotting-thermal-expansion","position":37},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Summary"},"type":"lvl2","url":"/lecture-14-metropolis#summary","position":38},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"Summary"},"content":"The Metropolis algorithm is a powerful tool for sampling from complex probability distributions, especially in high-dimensional spaces.\n\nBy satisfying the detailed balance condition, the algorithm ensures convergence to the desired distribution.\n\nApplying the algorithm to the Morse oscillator demonstrates thermal expansion due to the anharmonicity of the potential.\n\nThe Morse oscillator’s average bond length increases with temperature, illustrating how molecular vibrations contribute to macroscopic thermal expansion.\n\nAdditional Notes\n\nChoice of Step Size (delta): The step size should be tuned to achieve an acceptance ratio (accepted moves over total proposed moves) of about 30%–50% for efficient sampling.\n\nAutocorrelation: Successive samples may be correlated. To obtain independent samples, one can sample every few steps (thinning) or analyze the autocorrelation “time.”\n\nHigher Dimensions: The algorithm extends to systems with many particles by proposing moves in the multidimensional configuration space.","type":"content","url":"/lecture-14-metropolis#summary","position":39},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"References"},"type":"lvl2","url":"/lecture-14-metropolis#references","position":40},{"hierarchy":{"lvl1":"Chapter 14: A Basic Monte Carlo Algorithm","lvl2":"References"},"content":"Metropolis, N., et al. “Equation of State Calculations by Fast Computing Machines.” The Journal of Chemical Physics 21.6 (1953): 1087-1092. \n\nMetropolis et al. (1953)\n\nFrenkel, D., and B. Smit. Understanding Molecular Simulation: From Algorithms to Applications. Academic Press, 2023. \n\nElsevier (2023)","type":"content","url":"/lecture-14-metropolis#references","position":41},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing"},"type":"lvl1","url":"/lecture-15-nanoparticles","position":0},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing"},"content":"","type":"content","url":"/lecture-15-nanoparticles","position":1},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-15-nanoparticles#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to\n\nDescribe how the shape of nanoparticles influences their physical, chemical, and mechanical properties.\n\nExplain the principles of simulated annealing and how it can be used to find the optimal shape of a nanoparticle.\n\nImplement simulated annealing to optimize a system of interacting particles.","type":"content","url":"/lecture-15-nanoparticles#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl2":"Nanoparticle Shape"},"type":"lvl2","url":"/lecture-15-nanoparticles#nanoparticle-shape","position":4},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl2":"Nanoparticle Shape"},"content":"The shape of a nanoparticle plays a crucial role in determining its properties. Due to their small size and high surface-to-volume ratio, nanoparticles exhibit unique behaviors compared to bulk materials. The shape can affect\n\nOptical Properties: Nanoscale boundary conditions can lead to quantum confinement effects, altering the optical absorption and emission spectra.\n\nMechanical Properties: The truncation of long-range interactions and the presence of surface atoms can change the stiffness and strength of nanoparticles.\n\nChemical Properties: The number and arrangement of surface atoms influence reactivity and catalytic activity.\n\nUnderstanding and controlling nanoparticle shape is essential for applications in drug delivery, catalysis, photonics, and materials science.","type":"content","url":"/lecture-15-nanoparticles#nanoparticle-shape","position":5},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl2":"Local vs. Global Geometry Optimization"},"type":"lvl2","url":"/lecture-15-nanoparticles#local-vs-global-geometry-optimization","position":6},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl2":"Local vs. Global Geometry Optimization"},"content":"When optimizing the geometry of a nanoparticle or any complex system, we aim to find the configuration that minimizes the system’s potential energy. However, potential energy surfaces often contain multiple local minima due to the complex interactions between particles. Local optimization algorithms, such as gradient descent or methods implemented in \n\nscipy.optimize, can efficiently find a nearby minimum but may get trapped in a local minimum rather than finding the global minimum.\n\nIn the figure above, a local optimization algorithm starting at \\mathbf{3.5} may converge to the local minimum at \\color{red} \\mathbf{2}, missing the global minimum at \\color{blue} \\mathbf{-2}.","type":"content","url":"/lecture-15-nanoparticles#local-vs-global-geometry-optimization","position":7},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl2":"Simulated Annealing"},"type":"lvl2","url":"/lecture-15-nanoparticles#simulated-annealing","position":8},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl2":"Simulated Annealing"},"content":"To overcome the limitations of local optimization, we use global optimization algorithms that can escape local minima. Simulated annealing is a probabilistic technique inspired by the annealing process in metallurgy, where controlled cooling allows atoms to reach lower energy states.","type":"content","url":"/lecture-15-nanoparticles#simulated-annealing","position":9},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"Principles of Simulated Annealing","lvl2":"Simulated Annealing"},"type":"lvl3","url":"/lecture-15-nanoparticles#principles-of-simulated-annealing","position":10},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"Principles of Simulated Annealing","lvl2":"Simulated Annealing"},"content":"Simulated annealing adapts the Metropolis-Hastings algorithm from statistical mechanics. The key steps are:\n\nInitialization: Start with an initial configuration and a high “temperature” parameter.\n\nTemperature Schedule: Define a cooling schedule to gradually reduce the temperature.\n\nGeneration of New Configurations: At each step, generate a new configuration by making a random change to the current configuration.\n\nEnergy Calculation: Compute the change in energy, \\Delta E, between the new and current configurations.\n\nAcceptance Criterion: Accept the new configuration with probabilityP(\\text{accept}) =\n   \\begin{cases}\n   1, & \\text{if } \\Delta E \\leq 0 \\\\\n   \\exp\\left(-\\dfrac{\\Delta E}{k_B T}\\right), & \\text{if } \\Delta E > 0\n   \\end{cases}\n\nwhere k_\\text{B} is the Boltzmann constant, and T is the current temperature.\n\nBy allowing occasional uphill moves (accepting higher energy states), the algorithm can escape local minima and explore the configuration space more thoroughly.","type":"content","url":"/lecture-15-nanoparticles#principles-of-simulated-annealing","position":11},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"type":"lvl2","url":"/lecture-15-nanoparticles#example-optimizing-the-shape-of-a-nanoparticle","position":12},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"content":"","type":"content","url":"/lecture-15-nanoparticles#example-optimizing-the-shape-of-a-nanoparticle","position":13},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"Problem Statement","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"type":"lvl3","url":"/lecture-15-nanoparticles#problem-statement","position":14},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"Problem Statement","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"content":"We will optimize the configuration of a cluster of five argon atoms interacting via the Lennard-Jones potential. One atom is fixed at the origin, and the other four are free to move. Our goal is to find the arrangement that minimizes the total potential energy.","type":"content","url":"/lecture-15-nanoparticles#problem-statement","position":15},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"The Lennard-Jones Potential","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"type":"lvl3","url":"/lecture-15-nanoparticles#the-lennard-jones-potential","position":16},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"The Lennard-Jones Potential","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"content":"The Lennard-Jones (LJ) potential models the interaction between a pair of neutral atoms or moleculesV_{\\text{LJ}}(r) = 4\\varepsilon \\left[ \\left( \\dfrac{\\sigma}{r} \\right)^{12} - \\left( \\dfrac{\\sigma}{r} \\right)^{6} \\right]\n\nwhere r is the distance between two particles, \\varepsilon is the depth of the potential well (interaction strength), and \\sigma is the finite distance at which the inter-particle potential is zero.","type":"content","url":"/lecture-15-nanoparticles#the-lennard-jones-potential","position":17},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"Implementing the Potential Energy Function","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"type":"lvl3","url":"/lecture-15-nanoparticles#implementing-the-potential-energy-function","position":18},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"Implementing the Potential Energy Function","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"content":"\n\nimport numpy as np\n\ndef lennard_jones(r, epsilon=0.0103, sigma=3.4):\n    \"\"\"\n    Calculate the Lennard-Jones potential energy between two particles.\n    Parameters:\n        r (float): Distance between two particles.\n        epsilon (float): Depth of the potential well (eV).\n        sigma (float): Finite distance at which the inter-particle potential is zero (Angstrom).\n    Returns:\n        float: Potential energy (eV).\n    \"\"\"\n    return 4 * epsilon * ((sigma / r) ** 12 - (sigma / r) ** 6)\n\nWe also need a function to compute the total potential energy of the system:\n\ndef total_potential_energy(positions, epsilon=0.0103, sigma=3.4):\n    \"\"\"\n    Calculate the total potential energy of the system of particles.\n    Parameters:\n        positions (ndarray): Array of particle positions with shape (N, 3).\n        epsilon (float): Depth of the potential well (eV).\n        sigma (float): Finite distance at which the inter-particle potential is zero (Angstrom).\n    Returns:\n        float: Total potential energy (eV).\n    \"\"\"\n    energy = 0.0\n    num_particles = len(positions)\n    for i in range(num_particles):\n        for j in range(i + 1, num_particles):\n            r = np.linalg.norm(positions[i] - positions[j])\n            if r > 0:\n                energy += lennard_jones(r, epsilon, sigma)\n    return energy\n\n","type":"content","url":"/lecture-15-nanoparticles#implementing-the-potential-energy-function","position":19},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"Simulated Annealing Algorithm","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"type":"lvl3","url":"/lecture-15-nanoparticles#simulated-annealing-algorithm","position":20},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"Simulated Annealing Algorithm","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"content":"Now, we implement the simulated annealing algorithm to optimize the particle positions.\n\ndef simulated_annealing(positions, initial_temp, cooling_rate, num_steps, freeze_particle=0, epsilon=0.0103, sigma=3.4):\n    \"\"\"\n    Perform simulated annealing to optimize the nanoparticle configuration.\n    Parameters:\n        positions (ndarray): Initial positions of the particles.\n        initial_temp (float): Initial temperature (arbitrary units).\n        cooling_rate (float): Multiplicative factor for cooling (0 < cooling_rate < 1).\n        num_steps (int): Number of simulation steps.\n        freeze_particle (int): Index of the particle to keep fixed.\n        epsilon (float): Depth of the potential well (eV).\n        sigma (float): Finite distance at which the inter-particle potential is zero (Angstrom).\n    Returns:\n        best_positions (ndarray): Optimized positions of the particles.\n        best_energy (float): Total potential energy of the optimized configuration.\n        energy_history (list): List of energy values over time.\n        temp_history (list): List of temperature values over time.\n    \"\"\"\n    positions = positions.copy()\n    num_particles = len(positions)\n    temperature = initial_temp\n    best_positions = positions.copy()\n    best_energy = total_potential_energy(positions, epsilon, sigma)\n    energy_history = []\n    temp_history = []\n    kB = 8.617333262145e-5  # Boltzmann constant in eV/K\n\n    for step in range(num_steps):\n        # Select a random particle to move, excluding the frozen one\n        move_particle = np.random.choice([i for i in range(num_particles) if i != freeze_particle])\n\n        # Propose a new position by making a small random displacement\n        displacement = np.random.normal(0, 0.1, size=3)\n        new_positions = positions.copy()\n        new_positions[move_particle] += displacement\n\n        # Compute energies\n        current_energy = total_potential_energy(positions, epsilon, sigma)\n        new_energy = total_potential_energy(new_positions, epsilon, sigma)\n        delta_energy = new_energy - current_energy\n\n        # Acceptance probability\n        if delta_energy < 0 or np.random.rand() < np.exp(-delta_energy / (kB * temperature)):\n            positions = new_positions\n            current_energy = new_energy\n            if new_energy < best_energy:\n                best_energy = new_energy\n                best_positions = new_positions.copy()\n\n        # Record energy and temperature\n        energy_history.append(current_energy)\n        temp_history.append(temperature)\n\n        # Update temperature\n        temperature *= cooling_rate\n\n    return best_positions, best_energy, energy_history, temp_history\n\n","type":"content","url":"/lecture-15-nanoparticles#simulated-annealing-algorithm","position":21},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"Running the Simulation","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"type":"lvl3","url":"/lecture-15-nanoparticles#running-the-simulation","position":22},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"Running the Simulation","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"content":"Set up the initial positions and parameters, and run the simulated annealing algorithm.\n\nnp.random.seed(42)\n\n# Lennard-Jones parameters for Argon\nepsilon = 0.0103  # eV\nsigma = 3.4  # Angstrom\nr_min = 2 ** (1 / 6) * sigma  # Distance at minimum potential\n\n# Initial positions (one fixed at the origin)\npositions = np.array([\n    [0.0, 0.0, 0.0],  # Fixed particle\n    [r_min, 0.0, 0.0],\n    [0.0, r_min, 0.0],\n    [0.0, 0.0, r_min],\n    [r_min, r_min, r_min]\n])\n\n# Simulated annealing parameters\ninitial_temp = 1000.0  # Initial temperature in K\ncooling_rate = 0.999  # Closer to 1 means slower cooling\nnum_steps = 10000\n\n# Run simulated annealing\nbest_positions, best_energy, energy_history, temp_history = simulated_annealing(\n    positions,\n    initial_temp,\n    cooling_rate,\n    num_steps,\n    freeze_particle=0,\n    epsilon=epsilon,\n    sigma=sigma\n)\n\n","type":"content","url":"/lecture-15-nanoparticles#running-the-simulation","position":23},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"Visualizing the Results","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"type":"lvl3","url":"/lecture-15-nanoparticles#visualizing-the-results","position":24},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"Visualizing the Results","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"content":"Plot the total potential energy and temperature over the simulation steps.\n\nimport matplotlib.pyplot as plt\n\nfig, ax1 = plt.subplots(figsize=(10, 6))\n\ncolor_energy = 'tab:blue'\nax1.set_xlabel('Simulation Step')\nax1.set_ylabel('Total Potential Energy (eV)', color=color_energy)\nax1.plot(energy_history, color=color_energy)\nax1.tick_params(axis='y', labelcolor=color_energy)\n\nax2 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis\n\ncolor_temp = 'tab:red'\nax2.set_ylabel('Temperature (K)', color=color_temp)\nax2.plot(temp_history, color=color_temp)\nax2.tick_params(axis='y', labelcolor=color_temp)\n\nfig.tight_layout()\nplt.title('Simulated Annealing Optimization')\nplt.show()\n\nInterpretation\n\nThe plot shows how the total potential energy of the system decreases over time as the simulated annealing algorithm progresses. Initially, at high temperatures, the system explores a wide range of configurations, allowing for higher energy states. As the temperature decreases, the acceptance of higher energy states becomes less probable, and the system gradually settles into lower energy configurations. The energy curve shows fluctuations corresponding to accepted uphill moves, but overall trends downward, indicating convergence towards the global minimum.","type":"content","url":"/lecture-15-nanoparticles#visualizing-the-results","position":25},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"Analyzing the Optimized Configuration","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"type":"lvl3","url":"/lecture-15-nanoparticles#analyzing-the-optimized-configuration","position":26},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl3":"Analyzing the Optimized Configuration","lvl2":"Example: Optimizing the Shape of a Nanoparticle"},"content":"Let’s output the optimized positions and visualize the final configuration.\n\n# Print the optimized configuration in XYZ format\nprint(f\"{len(best_positions)}\")\nprint(\"Optimized configuration of the nanoparticle\")\nfor position in best_positions:\n    print(f\"Ar {position[0]:.6f} {position[1]:.6f} {position[2]:.6f}\")\n\nYou can save this output to an .xyz file and visualize it using molecular visualization software like VESTA or Avogadro.\n\nInterpretation\n\nThe optimized configuration shows the five argon atoms arranged in a trigonal bipyramidal structure. This geometry minimizes the total potential energy by optimizing the distances between particles to balance attractive and repulsive forces as defined by the Lennard-Jones potential. Each atom (except the fixed one) adjusts its position to achieve the most energetically favorable arrangement.","type":"content","url":"/lecture-15-nanoparticles#analyzing-the-optimized-configuration","position":27},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl2":"Exercise"},"type":"lvl2","url":"/lecture-15-nanoparticles#exercise","position":28},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl2":"Exercise"},"content":"Parameter Exploration: Modify the simulated annealing parameters (initial_temp, cooling_rate, num_steps) to observe their effects on the optimization process. How does changing the cooling rate affect the convergence?\n\nAlternative Optimization Methods: Implement a different global optimization algorithm, such as Differential Evolution or Basin Hopping from \n\nscipy.optimize. Compare the results and efficiency with simulated annealing.\n\nScaling Up: Increase the number of particles (e.g., to 10 or 20) and observe how the optimization process scales with system size. What challenges arise with larger systems?\n\nPotential Function Variation: Try using a different interatomic potential, such as the Morse potential. How does the choice of potential affect the optimized configuration?","type":"content","url":"/lecture-15-nanoparticles#exercise","position":29},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl2":"Summary"},"type":"lvl2","url":"/lecture-15-nanoparticles#summary","position":30},{"hierarchy":{"lvl1":"Chapter 15: Nanoparticle Shape and Simulated Annealing","lvl2":"Summary"},"content":"In this lecture, we explored the significance of nanoparticle shape in determining their properties and learned how global optimization algorithms like simulated annealing can be employed to find optimal configurations. Simulated annealing mimics the physical process of annealing, allowing the system to escape local minima and converge towards the global minimum by controlled cooling. By implementing this algorithm, we optimized a small cluster of Lennard-Jones particles, demonstrating its effectiveness in solving complex optimization problems in the chemical sciences.","type":"content","url":"/lecture-15-nanoparticles#summary","position":31},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc."},"type":"lvl1","url":"/lecture-16-tech-details","position":0},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc."},"content":"","type":"content","url":"/lecture-16-tech-details","position":1},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-16-tech-details#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to\n\nExplain the importance and implementation of periodic boundary conditions in molecular simulations.\n\nApply the minimum image convention to compute the shortest distance between particles under periodic boundary conditions.\n\nUnderstand and implement interaction truncation methods, including the use of shift functions to handle discontinuities in potential energy functions.","type":"content","url":"/lecture-16-tech-details#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl2":"Introduction"},"type":"lvl2","url":"/lecture-16-tech-details#introduction","position":4},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl2":"Introduction"},"content":"Simulating physical systems at the atomic or molecular level involves managing interactions among a vast number of particles. Directly simulating all these particles is computationally infeasible. To address this challenge, we use techniques such as periodic boundary conditions and interaction truncation to make simulations manageable while still capturing essential physical behaviors.","type":"content","url":"/lecture-16-tech-details#introduction","position":5},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl2":"Periodic Boundary Conditions"},"type":"lvl2","url":"/lecture-16-tech-details#periodic-boundary-conditions","position":6},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl2":"Periodic Boundary Conditions"},"content":"Consider simulating a macroscopic amount of a substance, like a glass of water, at atomic-level precision. A typical glass contains about 250 mL of water. The number of atoms in such a volume can be estimated\\begin{align*}\n\\text{Number of atoms} &= 250 \\, \\text{mL} \\times \\frac{1 \\, \\text{g}}{1 \\, \\text{mL}} \\times \\frac{1 \\, \\text{mol}}{18 \\, \\text{g}} \\times \\frac{6.022 \\times 10^{23} \\, \\text{molecules}}{1 \\, \\text{mol}} \\times \\frac{3 \\, \\text{atoms}}{1 \\, \\text{molecule}} \\\\\n&\\approx 2.5 \\times 10^{25} \\, \\text{atoms}.\n\\end{align*}\n\nSimulating this many atoms directly is beyond current computational capabilities. However, we can exploit the fact that many interactions are short-ranged, decaying rapidly with distance. This allows us to simulate a smaller, representative system (typically between \n\n103 and \n\n109 atoms) by applying periodic boundary conditions (PBCs).","type":"content","url":"/lecture-16-tech-details#periodic-boundary-conditions","position":7},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl3":"Concept of Periodic Boundary Conditions","lvl2":"Periodic Boundary Conditions"},"type":"lvl3","url":"/lecture-16-tech-details#concept-of-periodic-boundary-conditions","position":8},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl3":"Concept of Periodic Boundary Conditions","lvl2":"Periodic Boundary Conditions"},"content":"PBCs mimic an infinite system by repeating a finite simulation box in all spatial directions. Each particle interacts not only with other particles within the box but also with the periodic images of particles in neighboring boxes. This setup eliminates surface effects that would otherwise arise from the finite size of the simulation box.\n\nFor example, in two dimensions:\n\nThe central box is the simulation box and the surrounding boxes are periodic images. When a particle moves out of the simulation box, it re-enters from the opposite side, maintaining a constant number of particles.","type":"content","url":"/lecture-16-tech-details#concept-of-periodic-boundary-conditions","position":9},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl2":"Minimum Image Convention"},"type":"lvl2","url":"/lecture-16-tech-details#minimum-image-convention","position":10},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl2":"Minimum Image Convention"},"content":"When using PBCs, calculating distances between particles requires careful consideration to ensure interactions are computed correctly. Simply using positions within the simulation box might not yield the shortest distance due to periodic images.","type":"content","url":"/lecture-16-tech-details#minimum-image-convention","position":11},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl3":"Calculating the Minimum Image Distance","lvl2":"Minimum Image Convention"},"type":"lvl3","url":"/lecture-16-tech-details#calculating-the-minimum-image-distance","position":12},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl3":"Calculating the Minimum Image Distance","lvl2":"Minimum Image Convention"},"content":"The minimum image convention states that the distance between two particles should be the shortest distance considering all periodic images. For a box with edge length L, the minimum image distance \\Delta \\mathbf{r} between particles at positions \\mathbf{r}_1 and \\mathbf{r}_2 is\\Delta \\mathbf{r} = \\mathbf{r}_1 - \\mathbf{r}_2 - L \\cdot \\text{round}\\left(\\frac{\\mathbf{r}_1 - \\mathbf{r}_2}{L}\\right),\n\nwhere \\text{round} operates element-wise on vector components, rounding to the nearest integer. This formula maps the displacement vector into the range \\left(-\\frac{L}{2}, \\frac{L}{2}\\right], ensuring it corresponds to the minimum image.","type":"content","url":"/lecture-16-tech-details#calculating-the-minimum-image-distance","position":13},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl4":"Implementation of the Minimum Image Convention in Python","lvl3":"Calculating the Minimum Image Distance","lvl2":"Minimum Image Convention"},"type":"lvl4","url":"/lecture-16-tech-details#implementation-of-the-minimum-image-convention-in-python","position":14},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl4":"Implementation of the Minimum Image Convention in Python","lvl3":"Calculating the Minimum Image Distance","lvl2":"Minimum Image Convention"},"content":"Let’s implement the minimum image convention in Python for a two-dimensional system.\n\nimport numpy as np\n\ndef minimum_image_distance(p1, p2, box_size):\n    \"\"\"\n    Compute the minimum image distance between two particles p1 and p2\n    under periodic boundary conditions in a 2D box.\n\n    Parameters:\n    p1, p2 : np.ndarray\n        Coordinates of the two particles (2D vectors).\n    box_size : float\n        Length of the simulation box edge.\n\n    Returns:\n    float\n        Minimum image distance between p1 and p2.\n    \"\"\"\n    delta = p1 - p2\n    # Apply minimum image convention\n    delta -= box_size * np.round(delta / box_size)\n    distance = np.linalg.norm(delta)\n    return distance\n\n# Define the box size\nbox_size = 10.0\n\n# Define positions of the particles\nblue_particle = np.array([2.0, 8.0])   # Coordinates of the blue particle\ncyan_particle = np.array([9.0, 9.0])   # Coordinates of the cyan particle\n\n# Calculate the minimum image distance\ndistance = minimum_image_distance(blue_particle, cyan_particle, box_size)\nprint(f\"The minimum image distance between the blue and cyan particles is {distance:.2f} units.\")\n\nExplanation\n\ndelta calculates the displacement vector between the two particles. The minimum image convention adjusts this displacement to ensure it corresponds to the shortest distance under periodic boundary conditions. The Euclidean norm of the adjusted displacement gives the minimum image distance.\n\nNote\n\nThis implementation assumes a cubic (or square in 2D) simulation box with equal edge lengths and right angles.","type":"content","url":"/lecture-16-tech-details#implementation-of-the-minimum-image-convention-in-python","position":15},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl2":"Truncation of Interactions"},"type":"lvl2","url":"/lecture-16-tech-details#truncation-of-interactions","position":16},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl2":"Truncation of Interactions"},"content":"In many-body simulations, the computational cost of calculating interactions scales as \\mathcal{O}(N^2). To make simulations feasible, especially for large N, we often truncate interactions by introducing a cutoff distance r_c, beyond which interactions are neglected.\n\nMinimum Image Convention, Truncated Interactions, and Box Size\n\nWhen using the minimum image convention with truncated interactions, the box size should be chosen such that the cutoff distance r_c is less than half the box size. This ensures that the minimum image distance is always within the cutoff range.","type":"content","url":"/lecture-16-tech-details#truncation-of-interactions","position":17},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl3":"Potential Energy Truncation","lvl2":"Truncation of Interactions"},"type":"lvl3","url":"/lecture-16-tech-details#potential-energy-truncation","position":18},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl3":"Potential Energy Truncation","lvl2":"Truncation of Interactions"},"content":"Consider the Lennard-Jones potential, describing the interaction between a pair of neutral atoms or moleculesU(r) = 4\\varepsilon \\left[ \\left( \\frac{\\sigma}{r} \\right)^{12} - \\left( \\frac{\\sigma}{r} \\right)^6 \\right],\n\nwhere U(r) is the potential energy as a function of distance r, \\varepsilon is the depth of the potential well (energy scale), and \\sigma is the finite distance at which the interparticle potential is zero (length scale). Truncating this potential at r_c sets U(r) = 0 for r \\geq r_c, but introduces a discontinuity at r = r_c, leading to non-physical forces due to the abrupt change.","type":"content","url":"/lecture-16-tech-details#potential-energy-truncation","position":19},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl3":"Shift Function to Smooth the Potential","lvl2":"Truncation of Interactions"},"type":"lvl3","url":"/lecture-16-tech-details#shift-function-to-smooth-the-potential","position":20},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl3":"Shift Function to Smooth the Potential","lvl2":"Truncation of Interactions"},"content":"To mitigate the discontinuity, we can use a shifted potential ensuring the potential energy smoothly approaches zero at r_cU_{\\text{shifted}}(r) = \\begin{cases}\nU(r) - U(r_c) & \\text{if } r < r_c, \\\\\n0 & \\text{if } r \\geq r_c.\n\\end{cases}\n\nBy subtracting U(r_c), we ensure U_{\\text{shifted}}(r_c) = 0, eliminating the discontinuity in potential energy.\n\nWarning\n\nWhile the shift function removes the discontinuity in potential energy at r = r_c, it does not eliminate the discontinuity in the force (the derivative of the potential). This can still lead to artifacts in simulations, especially affecting dynamical properties.","type":"content","url":"/lecture-16-tech-details#shift-function-to-smooth-the-potential","position":21},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl4":"Implementation of a Shifted Lennard-Jones Potential in Python","lvl3":"Shift Function to Smooth the Potential","lvl2":"Truncation of Interactions"},"type":"lvl4","url":"/lecture-16-tech-details#implementation-of-a-shifted-lennard-jones-potential-in-python","position":22},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl4":"Implementation of a Shifted Lennard-Jones Potential in Python","lvl3":"Shift Function to Smooth the Potential","lvl2":"Truncation of Interactions"},"content":"Implementing the shifted Lennard-Jones potential:def lennard_jones(r, epsilon, sigma):\n    \"\"\"\n    Compute the Lennard-Jones potential energy between two particles\n    at distance r.\n\n    Parameters:\n    r : float or np.ndarray\n        Distance between the two particles.\n    epsilon : float\n        Depth of the potential well.\n    sigma : float\n        Finite distance at which the interparticle potential is zero.\n\n    Returns:\n    float or np.ndarray\n        Potential energy between the two particles.\n    \"\"\"\n    sr6 = (sigma / r) ** 6\n    return 4 * epsilon * (sr6 ** 2 - sr6)\n\ndef lennard_jones_shifted(r, epsilon, sigma, r_c):\n    \"\"\"\n    Compute the shifted Lennard-Jones potential energy between two particles\n    at distance r.\n\n    Parameters:\n    r : float or np.ndarray\n        Distance between the two particles.\n    epsilon : float\n        Depth of the potential well.\n    sigma : float\n        Finite distance at which the interparticle potential is zero.\n    r_c : float\n        Cutoff distance.\n\n    Returns:\n    float or np.ndarray\n        Shifted potential energy between the two particles.\n    \"\"\"\n    if r < r_c:\n        U = lennard_jones(r, epsilon, sigma)\n        U_c = lennard_jones(r_c, epsilon, sigma)\n        return U - U_c\n    else:\n        return 0.0\n\n# Parameters for the Lennard-Jones potential\nepsilon = 1.0    # Energy units\nsigma = 1.0      # Distance units\nr_c = 2.5 * sigma  # Cutoff distance\n\n# Example calculation\nr = 2.0  # Distance between particles\nenergy_shifted = lennard_jones_shifted(r, epsilon, sigma, r_c)\nenergy_original = lennard_jones(r, epsilon, sigma)\n\nprint(f\"At distance r = {r}, the shifted Lennard-Jones potential energy is {energy_shifted:.4f}.\")\nprint(f\"Without shifting, the Lennard-Jones potential energy is {energy_original:.4f}.\")\n\nExplanation\n\nThe lennard_jones function calculates the standard Lennard-Jones potential energy U(r), while lennard_jones_shifted computes the shifted potential energy U_{\\text{shifted}}(r) by subtracting U(r_c) when r < r_c. The cutoff handling ensures U_{\\text{shifted}}(r) = 0 when r \\geq r_c.\n\nNote\n\nTo further smooth the potential and its derivatives, methods like force shifting or spline smoothing can be employed.","type":"content","url":"/lecture-16-tech-details#implementation-of-a-shifted-lennard-jones-potential-in-python","position":23},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl2":"Summary"},"type":"lvl2","url":"/lecture-16-tech-details#summary","position":24},{"hierarchy":{"lvl1":"Chapter 16: Technical Details: Boundary Conditions, Truncation of Interactions, Etc.","lvl2":"Summary"},"content":"In this lecture, we explored key technical details essential for efficient and accurate molecular simulations:\n\nPeriodic Boundary Conditions (PBCs): Simulate infinite systems by repeating a finite simulation box, reducing finite-size effects.\n\nMinimum Image Convention: Calculate the shortest distance between particles under PBCs by considering the closest periodic image.\n\nTruncation of Interactions: Limit the range of interactions to a cutoff distance to reduce computational cost, and address discontinuities introduced by truncation using shift functions.\n\nUnderstanding and correctly implementing these concepts are crucial for simulating large systems while maintaining computational efficiency and physical accuracy.","type":"content","url":"/lecture-16-tech-details#summary","position":25},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations"},"type":"lvl1","url":"/lecture-17-lipids","position":0},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations"},"content":"","type":"content","url":"/lecture-17-lipids","position":1},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-17-lipids#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to\n\nDescribe a coarse-grained model for lipids and its advantages in simulating lipid bilayers.\n\nImplement a Monte Carlo simulation to study lipid interactions in a membrane.","type":"content","url":"/lecture-17-lipids#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl2":"Introduction"},"type":"lvl2","url":"/lecture-17-lipids#introduction","position":4},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl2":"Introduction"},"content":"Lipids are essential components of cell membranes, forming a bilayer structure that separates the cell from its environment. Understanding lipid interactions is crucial for studying membrane properties and functions. In this lecture, we will explore how to model lipid interactions using a coarse-grained approach and simulate a lipid membrane using Monte Carlo methods.","type":"content","url":"/lecture-17-lipids#introduction","position":5},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl2":"Coarse-Grained Models for Lipids"},"type":"lvl2","url":"/lecture-17-lipids#coarse-grained-models-for-lipids","position":6},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl2":"Coarse-Grained Models for Lipids"},"content":"","type":"content","url":"/lecture-17-lipids#coarse-grained-models-for-lipids","position":7},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl3":"Advantages of Coarse-Grained Models","lvl2":"Coarse-Grained Models for Lipids"},"type":"lvl3","url":"/lecture-17-lipids#advantages-of-coarse-grained-models","position":8},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl3":"Advantages of Coarse-Grained Models","lvl2":"Coarse-Grained Models for Lipids"},"content":"Coarse-grained models represent multiple atoms as a single interaction site, reducing the number of particles in the simulation. This simplification allows us to study larger systems and longer timescales than all-atom models. For lipids, coarse-grained models capture essential interactions while maintaining computational efficiency.\n\nTip\n\nCoarse-grained models are also useful for studying lipid phase transitions, membrane fusion, and other membrane properties because, by capturing just the essential interactions, they allow us to efficiently and systematically test hypotheses about lipid behavior.","type":"content","url":"/lecture-17-lipids#advantages-of-coarse-grained-models","position":9},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl3":"Cooke-WCA Model","lvl2":"Coarse-Grained Models for Lipids"},"type":"lvl3","url":"/lecture-17-lipids#cooke-wca-model","position":10},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl3":"Cooke-WCA Model","lvl2":"Coarse-Grained Models for Lipids"},"content":"Note\n\nA pair potential is a function that describes the interaction energy between two particles as a function of their separation distance.\n\nThe \n\nCooke-WCA Model is a coarse-grained model for lipids that captures the essential interactions between lipid head and tail groups. The model represents lipids as three particles: one head and two tails. The head-head, head-tail, and tail-tail interactions are modeled using a pair potential that includes repulsive, bond, bend, and attractive terms.","type":"content","url":"/lecture-17-lipids#cooke-wca-model","position":11},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl4":"Repulsive Potential","lvl3":"Cooke-WCA Model","lvl2":"Coarse-Grained Models for Lipids"},"type":"lvl4","url":"/lecture-17-lipids#repulsive-potential","position":12},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl4":"Repulsive Potential","lvl3":"Cooke-WCA Model","lvl2":"Coarse-Grained Models for Lipids"},"content":"The repulsive potential V_{\\mathrm{rep}}(r_{ij};b_{ij}) prevents particles from overlapping. The potential is given byV_{\\mathrm{rep}}(r_{ij};b_{ij}) = \\begin{cases}\n4\\epsilon\\left[\\left(\\frac{b_{ij}}{r_{ij}}\\right)^{12} - \\left(\\frac{b_{ij}}{r_{ij}}\\right)^6 + \\frac{1}{4}\\right] & r_{ij} \\leq r_c \\\\\n0 & r_{ij} > r_c\n\\end{cases}\n\nwhere r_{ij} is the distance between particles i and j, b_{ij} is the sum of the radii of particles i and j, \\epsilon is the interaction strength, and r_c = 2^{1/6}b_{ij} is the cutoff distance. The repulsive potential is zero beyond the cutoff distance, representing the hard-core diameter of the particles.\n\nLet’s visualize the repulsive potential for different values of b.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef repulsive_potential(r, b):\n    \"\"\"\n    Compute the repulsive potential between two particles.\n\n    Parameters\n    ----------\n    r : float\n        Distance between particles.\n    b : float\n        Sum of the hard-core radii of the particles.\n\n    Returns\n    -------\n    potential : float\n        Repulsive potential between the particles.\n    \"\"\"\n    potential = np.where(\n        r <= 2**(1/6)*b,\n        4*((b/r)**12 - (b/r)**6 + 1/4),\n        0\n    )\n    return potential\n\n# Define the distance range\nr = np.linspace(0.5, 3, 100)\nb_values = [1, 1.5, 2]\n\n# Plot the repulsive potential for different values of b\nplt.figure(figsize=(10, 6))\ncolors = [\"C0\", \"C1\", \"C2\"]\nfor i, b in enumerate(b_values):\n    plt.plot(r, repulsive_potential(r, b), label=f\"b={b}\", color=colors[i])\n    plt.axvline(2**(1/6)*b, color=\"k\", linestyle=\"--\")\nplt.xlim(0.5, 3)\nplt.ylim(-1, 5)\nplt.xlabel(r\"Distance ($r / \\sigma$)\")\nplt.ylabel(r\"Repulsive Potential ($V_{\\mathrm{rep}} / \\epsilon$)\")\nplt.title(\"Cooke-WCA Repulsive Potential\")\nplt.legend()\nplt.show()\n\nThe plot shows that the repulsive potential increases rapidly as particles approach each other, preventing overlap due to the hard-core diameter b_{ij}. The potential is zero beyond the cutoff distance, ensuring that particles do not interact beyond this range.\n\nNote\n\nTo ensure an effective cylindrical lipid shape, we choose b_{\\text{head,head}} = b_{\\text{head,tail}} = 0.95\\sigma and b_{\\text{tail,tail}} = \\sigma, where \\sigma is the unit of length.","type":"content","url":"/lecture-17-lipids#repulsive-potential","position":13},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl4":"Bond Potential","lvl3":"Cooke-WCA Model","lvl2":"Coarse-Grained Models for Lipids"},"type":"lvl4","url":"/lecture-17-lipids#bond-potential","position":14},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl4":"Bond Potential","lvl3":"Cooke-WCA Model","lvl2":"Coarse-Grained Models for Lipids"},"content":"The bond potential V_{\\mathrm{bond}}(r_{ij}) prevents particles from drifting too far apart. The potential is given byV_{\\mathrm{bond}}(r_{ij}) = -\\frac{1}{2}k_{\\mathrm{bond}}r_{\\infty}^2\\ln\\left[1 - \\left(\\frac{r_{ij}}{r_{\\infty}}\\right)^2\\right]\n\nwhere k_{\\mathrm{bond}} = 30\\epsilon/\\sigma^2 is the bond stiffness, r_{\\infty} = 1.5\\sigma is the “divergence distance,” and \\sigma is the particle separation. The bond potential prevents particles from drifting too far apart. Let’s visualize the bond potential for different values of r_{\\infty}.\n\ndef bond_potential(r, k_bond, r_inf):\n    \"\"\"\n    Compute the bond potential between two particles.\n\n    Parameters\n    ----------\n    r : float\n        Distance between particles.\n    k_bond : float\n        Bond stiffness.\n    r_inf : float\n        Divergence distance.\n\n    Returns\n    -------\n    potential : float\n        Bond potential between the particles.\n    \"\"\"\n    potential = -0.5*k_bond*r_inf**2*np.log(1 - (r/r_inf)**2)\n    return potential\n\n# Define the distance range\nr_min = 0.5\nk_bond = 30\nr_inf_values = [1, 1.25, 1.5]\n\n# Plot the bond potential for different values of r_inf\nplt.figure(figsize=(10, 6))\ncolors = [\"C0\", \"C1\", \"C2\"]\nfor i, r_inf in enumerate(r_inf_values):\n    r = np.linspace(r_min, r_inf - 0.01, 100)\n    plt.plot(r, bond_potential(r, k_bond, r_inf), label=f\"$r_{{\\\\infty}}$={r_inf}\", color=colors[i])\n    plt.axvline(r_inf, color=\"k\", linestyle=\"--\")\nplt.xlabel(r\"Distance ($r / \\sigma$)\")\nplt.ylabel(r\"Bond Potential ($V_{\\mathrm{bond}} / \\epsilon$)\")\nplt.title(\"FENE Bond Potential\")\nplt.legend()\nplt.show()\n\nThe plot shows that the bond potential increases rapidly as particles drift apart, preventing them from moving beyond the divergence distance r_{\\infty}. The potential is infinite at r_{\\infty}, ensuring that particles remain within a certain range.\n\nFENE Potential\n\nThe bond potential is based on the FENE (Finite Extensible Nonlinear Elastic) model, which is commonly used to model bond stretching in polymer chains. The FENE potential prevents bond lengths from exceeding a critical value, ensuring the stability of the polymer chain. For more details, see:\n\nKremer, K.; Grest, G. S. Dynamics of Entangled Linear Polymer Melts: A Molecular-Dynamics Simulation. The Journal of Chemical Physics 1990, 92 (8), 5057–5086. \n\nKremer & Grest (1990).","type":"content","url":"/lecture-17-lipids#bond-potential","position":15},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl4":"Bend Potential","lvl3":"Cooke-WCA Model","lvl2":"Coarse-Grained Models for Lipids"},"type":"lvl4","url":"/lecture-17-lipids#bend-potential","position":16},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl4":"Bend Potential","lvl3":"Cooke-WCA Model","lvl2":"Coarse-Grained Models for Lipids"},"content":"The bend potential V_{\\mathrm{bend}}(r_{ij}) promotes a preferred angle between particles. The potential is given byV_{\\mathrm{bend}}(r_{ij}) = \\frac{1}{2}k_{\\mathrm{bend}}\\left(r_{ij} - 4\\sigma\\right)^2\n\nwhere k_{\\mathrm{bend}} = 10\\epsilon/\\sigma^2 is the bending stiffness. The bend potential favors a preferred angle between particles, promoting a straight configuration. Let’s calculate the bend potential for different angles.\n\ndef bend_potential(r, sigma, k_bend):\n    \"\"\"\n    Compute the bend potential between two particles.\n\n    Parameters\n    ----------\n    r : float\n        Distance between particles.\n    sigma : float\n        Unit of length.\n    k_bend : float\n        Bending stiffness.\n\n    Returns\n    -------\n    potential : float\n        Bend potential between the particles.\n    \"\"\"\n    potential = 0.5*k_bend*(r - 4*sigma)**2\n    return potential\n\n# Define the distance range\nr = np.linspace(3, 5, 100)\nsigma = 1\nk_bend = 10\n\n# Plot the bend potential\nplt.figure(figsize=(10, 6))\nplt.plot(r, bend_potential(r, sigma, k_bend), color=\"C0\")\nplt.axvline(4*sigma, color=\"k\", linestyle=\"--\")\nplt.xlabel(\"Distance ($r$)\")\nplt.ylabel(r\"Bend Potential ($V_{\\mathrm{bend}} / \\epsilon$)\")\nplt.title(\"Bend Potential\")\nplt.show()\n\nThe plot shows that the bend potential increases as the angle deviates from the preferred value, favoring a straight configuration. The potential is zero at the preferred angle, ensuring that particles maintain the desired orientation.","type":"content","url":"/lecture-17-lipids#bend-potential","position":17},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl4":"Attractive Potential","lvl3":"Cooke-WCA Model","lvl2":"Coarse-Grained Models for Lipids"},"type":"lvl4","url":"/lecture-17-lipids#attractive-potential","position":18},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl4":"Attractive Potential","lvl3":"Cooke-WCA Model","lvl2":"Coarse-Grained Models for Lipids"},"content":"The attractive potential V_{\\mathrm{attr}}(r_{ij}) promotes interactions between lipid tails, leading to lipid aggregation. The attractive potential is given byV_{\\mathrm{attr}}(r_{ij}) =\n\\begin{cases}\n-\\epsilon & r_{ij} < r_c \\\\\n-\\epsilon \\cos^2\\left[\\frac{\\pi\\left(r_{ij} - r_c\\right)}{2 w_c}\\right] & r_c \\leq r_{ij} \\leq r_c + w_c \\\\\n0 & r_{ij} > r_c + w_c\n\\end{cases}\n\nwhere w_c is the attractive range. The attractive potential favors interactions within a specific distance range, promoting lipid aggregation. Let’s visualize the attractive potential for different values of w_c.\n\ndef attractive_potential(r, r_c, w_c):\n    \"\"\"\n    Compute the attractive potential between two particles.\n\n    Parameters\n    ----------\n    r : float\n        Distance between particles.\n    r_c : float\n        Cutoff distance.\n    w_c : float\n        Attractive range.\n\n    Returns\n    -------\n    potential : float\n        Attractive potential between the particles.\n    \"\"\"\n    potential = np.where(\n        r < r_c,\n        -1,\n        np.where(\n            r <= r_c + w_c,\n            -1*np.cos(np.pi*(r - r_c)/(2*w_c))**2,\n            0\n        )\n    )\n    return potential\n\n# Define the distance range\nr = np.linspace(0.5, 3, 100)\nr_c = 1\nw_c_values = [0.5, 1, 1.5, 2]\n\n# Plot the attractive potential for different values of w_c\nplt.figure(figsize=(10, 6))\ncolors = [\"C0\", \"C1\", \"C2\", \"C3\"]\nfor i, w_c in enumerate(w_c_values):\n    plt.plot(r, attractive_potential(r, r_c, w_c), label=f\"$w_c$={w_c}\", color=colors[i])\n    plt.axvline(r_c, color=\"k\", linestyle=\"--\")\n    plt.axvline(r_c + w_c, color=\"k\", linestyle=\"--\")\nplt.xlim(0.5, 3)\nplt.ylim(-1, 0)\nplt.xlabel(r\"Distance ($r / \\sigma$)\")\nplt.ylabel(r\"Attractive Potential ($V_{\\mathrm{attr}} / \\epsilon$)\")\nplt.title(\"Attractive Potential\")\nplt.legend()\nplt.show()\n\nThe plot shows that the attractive potential promotes interactions within a specific distance range, favoring lipid aggregation. The potential is zero beyond the attractive range, ensuring that interactions do not occur beyond this distance.","type":"content","url":"/lecture-17-lipids#attractive-potential","position":19},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl3":"Total Potential","lvl2":"Coarse-Grained Models for Lipids"},"type":"lvl3","url":"/lecture-17-lipids#total-potential","position":20},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl3":"Total Potential","lvl2":"Coarse-Grained Models for Lipids"},"content":"The total potential V_{\\mathrm{total}}(r_{ij}) is the sum of the repulsive, bond, bend, and attractive potentials for all particle pairs.\\begin{align}\nV_{\\mathrm{total}}(\\mathbf{r}) &= \\sum_{i < j} V_{\\mathrm{rep}}(r_{ij};b_{ij}) + \\sum_{i < j} V_{\\mathrm{bond}}(r_{ij}) + \\sum_{i < j} V_{\\mathrm{bend}}(r_{ij}) + \\sum_{i < j} V_{\\mathrm{attr}}(r_{ij}) \\\\\n&= \\sum_{i < j} V_{\\mathrm{rep}}(r_{ij};b_{ij}) + \\sum_{\\mathrm{lipid}} \\sum_{\\mathrm{bond}} V_{\\mathrm{bond}}(r_{\\mathrm{lipid,bond}}) + \\sum_{\\mathrm{lipid}} \\sum_{\\mathrm{bend}} V_{\\mathrm{bend}}(r_{\\mathrm{lipid,bend}}) + \\sum_{i < j \\in \\mathrm{tails}} V_{\\mathrm{attr}}(r_{ij})\n\\end{align}\n\nwhere \\mathbf{r} is the configuration of all particles, r_{\\mathrm{lipid,bond}} is the distance between a particle and its bonded particle in the same lipid, and r_{\\mathrm{lipid,bend}} is the distance between a head particle and the second tail particle in the same lipid.","type":"content","url":"/lecture-17-lipids#total-potential","position":21},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl4":"Testing the Total Potential","lvl3":"Total Potential","lvl2":"Coarse-Grained Models for Lipids"},"type":"lvl4","url":"/lecture-17-lipids#testing-the-total-potential","position":22},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl4":"Testing the Total Potential","lvl3":"Total Potential","lvl2":"Coarse-Grained Models for Lipids"},"content":"Let’s test the total potential for a simple configuration of two tail particles with a separations of \\sigma.\n\n# Calculate the repulsive potential\nr = 1  # Separation between particles in sigma\nb = 1  # Sum of the hard-core radii of the particles in sigma\nprint(f\"Repulsive Potential: {repulsive_potential(1, 1):.2f} (expected: 1.00)\")\n\n# Calculate the bond potential\nk_bond = 30  # Bond stiffness in epsilon/sigma^2\nr_inf = 1.5  # Divergence distance in sigma\nprint(f\"Bond Potential: {bond_potential(r, k_bond, r_inf):.2f} (expected: 19.84)\")\n\n# Calculate the attractive potential\nr_c = 2**(1/6)  # Cutoff distance in sigma\nw_c = 1  # Attractive range in sigma\nprint(f\"Attractive Potential: {attractive_potential(r, r_c, w_c):.2f} (expected: -1.00)\")\n\n","type":"content","url":"/lecture-17-lipids#testing-the-total-potential","position":23},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl2":"Building a Lipid Bilayer"},"type":"lvl2","url":"/lecture-17-lipids#building-a-lipid-bilayer","position":24},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl2":"Building a Lipid Bilayer"},"content":"To build a lipid bilayer, we need to create a system of lipids with head and tail particles. We will use the Cooke-WCA model to represent the interactions between lipids. Let’s define the parameters for the Cooke-WCA model.\n\n# Define the Cooke-WCA parameters\nb_head_head = 0.95  # Sum of the hard-core radii for head-head interactions\nb_head_tail = 0.95  # Sum of the hard-core radii for head-tail interactions\nb_tail_tail = 1.0  # Sum of the hard-core radii for tail-tail interactions\nk_bond = 30.0  # Bond stiffness\nr_inf = 1.5  # Divergence distance\nk_bend = 10.0  # Bending stiffness\nr_c = 2**(1/6)  # Cutoff distance for attractive potential\nw_c = 1.0  # Attractive range\n\n","type":"content","url":"/lecture-17-lipids#building-a-lipid-bilayer","position":25},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl3":"Building One Lipid","lvl2":"Building a Lipid Bilayer"},"type":"lvl3","url":"/lecture-17-lipids#building-one-lipid","position":26},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl3":"Building One Lipid","lvl2":"Building a Lipid Bilayer"},"content":"Let’s build a single lipid with one head and two tail particles. We will define the particle positions and visualize the lipid configuration.\n\nimport matplotlib.patches as patches\n\n# Define the particle positions for one lipid\nhead = np.array([0, 0])\ntail1 = head + np.array([b_head_tail, 0])\ntail2 = tail1 + np.array([b_tail_tail * np.cos(np.pi/3), b_tail_tail * np.sin(np.pi/3)])\n\n# Plot the lipid configuration\nplt.figure(figsize=(6, 3))\nplt.plot(*head, \"o\", color=\"C0\", label=\"Head\")\nplt.plot(*tail1, \"o\", color=\"C1\", label=\"Tail 1\")\nplt.plot(*tail2, \"o\", color=\"C2\", label=\"Tail 2\")\nplt.plot([head[0], tail1[0]], [head[1], tail1[1]], \"k-\")\nplt.plot([tail1[0], tail2[0]], [tail1[1], tail2[1]], \"k-\")\nplt.gca().add_patch(patches.Circle(head, 19 / 40, edgecolor=\"k\", facecolor=\"none\"))\nplt.gca().add_patch(patches.Circle(tail1, 19 / 40, edgecolor=\"k\", facecolor=\"none\"))\nplt.gca().add_patch(patches.Circle(tail2, 21 / 40, edgecolor=\"k\", facecolor=\"none\"))\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.gca().set_aspect(\"equal\", adjustable=\"box\")\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.title(\"Lipid Configuration\")\nplt.legend()\nplt.show()\n\n","type":"content","url":"/lecture-17-lipids#building-one-lipid","position":27},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl3":"Optimizing the Lipid’s Geometry","lvl2":"Building a Lipid Bilayer"},"type":"lvl3","url":"/lecture-17-lipids#optimizing-the-lipids-geometry","position":28},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl3":"Optimizing the Lipid’s Geometry","lvl2":"Building a Lipid Bilayer"},"content":"To optimize the lipid’s geometry, we need to minimize the total potential V_{\\mathrm{total}}(r_{ij}) by adjusting the particle positions. We will use the L-BFGS-B algorithm to minimize the potential and visualize the optimized lipid configuration.\n\nfrom scipy.optimize import minimize\n\ndef one_lipid_potential(positions, b_head_head, b_head_tail, b_tail_tail, k_bond, r_inf, k_bend, r_c, w_c):\n    \"\"\"\n    Compute the total potential for one lipid.\n\n    Parameters\n    ----------\n    positions : array_like\n        Particle positions for one lipid.\n    b_head_head : float\n        Sum of the hard-core radii for head-head interactions.\n    b_head_tail : float\n        Sum of the hard-core radii for head-tail interactions.\n    b_tail_tail : float\n        Sum of the hard-core radii for tail-tail interactions.\n    k_bond : float\n        Bond stiffness.\n    r_inf : float\n        Divergence distance.\n    k_bend : float\n        Bending stiffness.\n    r_c : float\n        Cutoff distance for attractive potential.\n    w_c : float\n        Attractive range.\n\n    Returns\n    -------\n    potential : float\n        Total potential for one lipid.\n    \"\"\"\n    head, tail1, tail2 = positions.reshape((3, 2))\n    r_head_tail1 = np.linalg.norm(head - tail1)\n    r_head_tail2 = np.linalg.norm(head - tail2)\n    r_tail1_tail2 = np.linalg.norm(tail1 - tail2)\n    potential = (\n        repulsive_potential(r_head_tail1, b_head_tail) +\n        repulsive_potential(r_head_tail2, b_head_tail) +\n        repulsive_potential(r_tail1_tail2, b_tail_tail) +\n        bond_potential(r_head_tail1, k_bond, r_inf) +\n        bond_potential(r_tail1_tail2, k_bond, r_inf) +\n        bend_potential(r_head_tail2, 1, k_bend) +\n        attractive_potential(r_tail1_tail2, r_c, w_c)\n    )\n    return potential\n\n# Define the initial particle positions for one lipid\ninitial_positions = np.array([head, tail1, tail2]).flatten()\n\n# Minimize the total potential for one lipid\nresult = minimize(one_lipid_potential, initial_positions, args=(b_head_head, b_head_tail, b_tail_tail, k_bond, r_inf, k_bend, r_c, w_c), method=\"L-BFGS-B\")\noptimized_positions = result.x.reshape((3, 2))\n\n# Plot the optimized lipid configuration\nplt.figure(figsize=(6, 3))\nplt.plot(*optimized_positions[0], \"o\", color=\"C0\", label=\"Head\")\nplt.plot(*optimized_positions[1], \"o\", color=\"C1\", label=\"Tail 1\")\nplt.plot(*optimized_positions[2], \"o\", color=\"C2\", label=\"Tail 2\")\nplt.plot([optimized_positions[0][0], optimized_positions[1][0]], [optimized_positions[0][1], optimized_positions[1][1]], \"k-\")\nplt.plot([optimized_positions[1][0], optimized_positions[2][0]], [optimized_positions[1][1], optimized_positions[2][1]], \"k-\")\nplt.gca().add_patch(patches.Circle(optimized_positions[0], 19 / 40, edgecolor=\"k\", facecolor=\"none\"))\nplt.gca().add_patch(patches.Circle(optimized_positions[1], 19 / 40, edgecolor=\"k\", facecolor=\"none\"))\nplt.gca().add_patch(patches.Circle(optimized_positions[2], 21 / 40, edgecolor=\"k\", facecolor=\"none\"))\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.gca().set_aspect(\"equal\", adjustable=\"box\")\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.title(\"Optimized Lipid Configuration\")\nplt.legend()\nplt.show()\n\n","type":"content","url":"/lecture-17-lipids#optimizing-the-lipids-geometry","position":29},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl3":"Building a Lipid Monolayer","lvl2":"Building a Lipid Bilayer"},"type":"lvl3","url":"/lecture-17-lipids#building-a-lipid-monolayer","position":30},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl3":"Building a Lipid Monolayer","lvl2":"Building a Lipid Bilayer"},"content":"To build a lipid monolayer, we need to create a system of lipids with head and tail particles. We will optimize the lipid geometry and visualize the monolayer configuration.\n\ndef build_lipid_monolayer(n_lipids, b_head_head, b_head_tail, b_tail_tail, k_bond, r_inf, k_bend, r_c, w_c):\n    \"\"\"\n    Build a lipid monolayer with the specified number of lipids.\n\n    Parameters\n    ----------\n    n_lipids : int\n        Number of lipids in the monolayer.\n    b_head_head : float\n        Sum of the hard-core radii for head-head interactions.\n    b_head_tail : float\n        Sum of the hard-core radii for head-tail interactions.\n    b_tail_tail : float\n        Sum of the hard-core radii for tail-tail interactions.\n    k_bond : float\n        Bond stiffness.\n    r_inf : float\n        Divergence distance.\n    k_bend : float\n        Bending stiffness.\n    r_c : float\n        Cutoff distance for attractive potential.\n    w_c : float\n        Attractive range.\n\n    Returns\n    -------\n    lipid_positions : array_like\n        Particle positions for the lipid monolayer.\n    \"\"\"\n    np.random.seed(42)\n    lipid_positions = []\n    for i in range(n_lipids):\n        head = np.array([i * b_head_tail, 0]) + np.random.uniform(-0.05, 0.05, 2)\n        tail1 = head - np.array([0, b_head_tail]) + np.random.uniform(-0.05, 0.05, 2)\n        theta = np.random.uniform(7*np.pi/16, 9*np.pi/16)\n        tail2 = tail1 - np.array([b_tail_tail * np.cos(theta), b_tail_tail * np.sin(theta)]) + np.random.uniform(-0.05, 0.05, 2)\n        lipid_positions.extend([head, tail1, tail2])\n    return np.array(lipid_positions)\n\n# Build a lipid monolayer with 10 lipids\nn_lipids = 10\nlipid_positions = build_lipid_monolayer(n_lipids, b_head_head, b_head_tail, b_tail_tail, k_bond, r_inf, k_bend, r_c, w_c)\n\n# Plot the lipid monolayer configuration\nplt.figure(figsize=(6, 3))\nfor i in range(n_lipids):\n    head, tail1, tail2 = lipid_positions[i*3:(i+1)*3]\n    plt.plot(*head, \"o\", color=\"C0\")\n    plt.plot(*tail1, \"o\", color=\"C1\")\n    plt.plot(*tail2, \"o\", color=\"C2\")\n    plt.plot([head[0], tail1[0]], [head[1], tail1[1]], \"k-\")\n    plt.plot([tail1[0], tail2[0]], [tail1[1], tail2[1]], \"k-\")\n    plt.gca().add_patch(patches.Circle(head, 19 / 40, edgecolor=\"k\", facecolor=\"none\"))\n    plt.gca().add_patch(patches.Circle(tail1, 19 / 40, edgecolor=\"k\", facecolor=\"none\"))\n    plt.gca().add_patch(patches.Circle(tail2, 21 / 40, edgecolor=\"k\", facecolor=\"none\"))\nplt.gca().set_aspect(\"equal\", adjustable=\"box\")\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.title(\"Lipid Monolayer Configuration\")\nplt.show()\n\n","type":"content","url":"/lecture-17-lipids#building-a-lipid-monolayer","position":31},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl3":"Equilibrating the Lipid Monolayer","lvl2":"Building a Lipid Bilayer"},"type":"lvl3","url":"/lecture-17-lipids#equilibrating-the-lipid-monolayer","position":32},{"hierarchy":{"lvl1":"Chapter 17: Lipid Interactions in Membranes and Monte Carlo Simulations","lvl3":"Equilibrating the Lipid Monolayer","lvl2":"Building a Lipid Bilayer"},"content":"To equilibrate the lipid monolayer, we will perform a Metropolis Monte Carlo simulation to sample lipid configurations. We will implement the Metropolis algorithm to accept or reject moves based on the total potential V_{\\mathrm{total}}(r_{ij}).\n\ndef lipid_monolayer_potential(positions, n_lipids, b_head_head, b_head_tail, b_tail_tail, k_bond, r_inf, k_bend, r_c, w_c):\n    \"\"\"\n    Compute the total potential for a lipid monolayer.\n\n    Parameters\n    ----------\n    positions : array_like\n        Particle positions for the lipid monolayer.\n    n_lipids : int\n        Number of lipids in the monolayer.\n    b_head_head : float\n        Sum of the hard-core radii for head-head interactions.\n    b_head_tail : float\n        Sum of the hard-core radii for head-tail interactions.\n    b_tail_tail : float\n        Sum of the hard-core radii for tail-tail interactions.\n    k_bond : float\n        Bond stiffness.\n    r_inf : float\n        Divergence distance.\n    k_bend : float\n        Bending stiffness.\n    r_c : float\n        Cutoff distance for attractive potential.\n    w_c : float\n        Attractive range.\n\n    Returns\n    -------\n    potential : float\n        Total potential for the lipid monolayer.\n    \"\"\"\n    n_particles = n_lipids * 3\n\n    # Compute V_rep for all particle pairs\n    V_rep = 0\n    for i in range(n_particles):\n        for j in range(i + 1, n_particles):\n            r = np.linalg.norm(positions[i] - positions[j])\n\n            # Determine the particle types\n            if i % 3 == 0 and j % 3 == 0:\n                b = b_head_head\n            elif i % 3 == 0 or j % 3 == 0:\n                b = b_head_tail\n            else:\n                b = b_tail_tail\n\n            V_rep += repulsive_potential(r, b)\n\n    # Compute V_bond for bonded particles\n    V_bond = 0\n    for i in range(0, n_particles, 3):\n        r_head_tail1 = np.linalg.norm(positions[i] - positions[i + 1])\n        r_tail1_tail2 = np.linalg.norm(positions[i + 1] - positions[i + 2])\n        V_bond += bond_potential(r_head_tail1, k_bond, r_inf)\n        V_bond += bond_potential(r_tail1_tail2, k_bond, r_inf)\n    \n    # Compute V_bend for head-tail-tail angles\n    V_bend = 0\n    for i in range(0, n_particles, 3):\n        r_head_tail2 = np.linalg.norm(positions[i] - positions[i + 2])\n        V_bend += bend_potential(r_head_tail2, 1, k_bend)\n\n    # Compute V_attr for tail-tail interactions\n    V_attr = 0\n    for i in range(2, n_particles, 3):\n        for j in range(i + 3, n_particles, 3):\n            r = np.linalg.norm(positions[i] - positions[j])\n            V_attr += attractive_potential(r, r_c, w_c)\n\n    potential = V_rep + V_bond + V_bend + V_attr\n    return potential\n\ndef metropolis_mc(positions, n_lipids, b_head_head, b_head_tail, b_tail_tail, k_bond, r_inf, k_bend, r_c, w_c, n_steps, temperature):\n    \"\"\"\n    Perform a Metropolis Monte Carlo simulation for a lipid monolayer.\n\n    Parameters\n    ----------\n    positions : array_like\n        Initial particle positions for the lipid monolayer.\n    n_lipids : int\n        Number of lipids in the monolayer.\n    b_head_head : float\n        Sum of the hard-core radii for head-head interactions.\n    b_head_tail : float\n        Sum of the hard-core radii for head-tail interactions.\n    b_tail_tail : float\n        Sum of the hard-core radii for tail-tail interactions.\n    k_bond : float\n        Bond stiffness.\n    r_inf : float\n        Divergence distance.\n    k_bend : float\n        Bending stiffness.\n    r_c : float\n        Cutoff distance for attractive potential.\n    w_c : float\n        Attractive range.\n    n_steps : int\n        Number of Monte Carlo steps.\n    temperature : float\n        Temperature of the system.\n\n    Returns\n    -------\n    positions : array_like\n        Final particle positions for the lipid monolayer.\n    \"\"\"\n    beta = 1 / temperature\n    n_particles = n_lipids * 3\n\n    trajectory = np.zeros((n_steps, n_particles, 2))\n    energies = np.zeros(n_steps)\n    for step in range(n_steps):\n        # Randomly select a particle to move\n        i = np.random.randint(n_particles)\n        move = np.random.uniform(-0.1, 0.1, 2)\n        new_positions = positions.copy()\n        new_positions[i] += move\n\n        # Compute the change in potential\n        delta_potential = lipid_monolayer_potential(new_positions, n_lipids, b_head_head, b_head_tail, b_tail_tail, k_bond, r_inf, k_bend, r_c, w_c) - lipid_monolayer_potential(positions, n_lipids, b_head_head, b_head_tail, b_tail_tail, k_bond, r_inf, k_bend, r_c, w_c)\n\n        # Accept or reject the move\n        if delta_potential < 0 or np.random.rand() < np.exp(-beta * delta_potential):\n            positions = new_positions\n\n        # Store the particle positions\n        trajectory[step] = positions\n\n        # Compute the total energy\n        energies[step] = lipid_monolayer_potential(positions, n_lipids, b_head_head, b_head_tail, b_tail_tail, k_bond, r_inf, k_bend, r_c, w_c)\n\n    return positions, trajectory, energies\n\n# Equilibrate the lipid monolayer\nn_steps = 1000\ntemperature = 1.0\nequilibrated_positions, trajectory, energies = metropolis_mc(lipid_positions, n_lipids, b_head_head, b_head_tail, b_tail_tail, k_bond, r_inf, k_bend, r_c, w_c, n_steps, temperature)\n\n# Plot the equilibrated lipid monolayer configuration and energy\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\nfor i in range(n_lipids):\n    head, tail1, tail2 = equilibrated_positions[i*3:(i+1)*3]\n    axs[0].plot(*head, \"o\", color=\"C0\")\n    axs[0].plot(*tail1, \"o\", color=\"C1\")\n    axs[0].plot(*tail2, \"o\", color=\"C2\")\n    axs[0].plot([head[0], tail1[0]], [head[1], tail1[1]], \"k-\")\n    axs[0].plot([tail1[0], tail2[0]], [tail1[1], tail2[1]], \"k-\")\n    axs[0].add_patch(patches.Circle(head, 19 / 40, edgecolor=\"k\", facecolor=\"none\"))\n    axs[0].add_patch(patches.Circle(tail1, 19 / 40, edgecolor=\"k\", facecolor=\"none\"))\n    axs[0].add_patch(patches.Circle(tail2, 21 / 40, edgecolor=\"k\", facecolor=\"none\"))\naxs[0].set_aspect(\"equal\", adjustable=\"box\")\naxs[0].set_xlabel(\"$x$\")\naxs[0].set_ylabel(\"$y$\")\naxs[0].set_title(\"Equilibrated Lipid Monolayer Configuration\")\n\naxs[1].plot(energies, color=\"C0\")\naxs[1].set_xlabel(\"Step\")\naxs[1].set_ylabel(\"Energy\")\naxs[1].set_title(\"Energy vs. Step\")\nplt.tight_layout()\nplt.show()","type":"content","url":"/lecture-17-lipids#equilibrating-the-lipid-monolayer","position":33},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces"},"type":"lvl1","url":"/lecture-18-adsorption","position":0},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces"},"content":"","type":"content","url":"/lecture-18-adsorption","position":1},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-18-adsorption#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to\n\nDescribe the concept of adsorption and its importance in surface science.\n\nImplement a Monte Carlo simulation to study adsorption on surfaces.\n\nAnalyze the results of a Monte Carlo simulation to determine the adsorption isotherm.","type":"content","url":"/lecture-18-adsorption#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Introduction"},"type":"lvl2","url":"/lecture-18-adsorption#introduction","position":4},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Introduction"},"content":"Adsorption is the process by which molecules or atoms adhere to the surface of a solid or liquid. It plays a crucial role in various applications, such as catalysis, gas separation, and drug delivery. In this project, we will explore how to simulate adsorption on solid surfaces using Monte Carlo methods.","type":"content","url":"/lecture-18-adsorption#introduction","position":5},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Adsorption on Surfaces"},"type":"lvl2","url":"/lecture-18-adsorption#adsorption-on-surfaces","position":6},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Adsorption on Surfaces"},"content":"","type":"content","url":"/lecture-18-adsorption#adsorption-on-surfaces","position":7},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Types of Adsorption","lvl2":"Adsorption on Surfaces"},"type":"lvl3","url":"/lecture-18-adsorption#types-of-adsorption","position":8},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Types of Adsorption","lvl2":"Adsorption on Surfaces"},"content":"Adsorption on surfaces can be classified into three main types:\n\nPhysisorption: In physisorption, molecules are held to the surface by weak van der Waals forces. This type of adsorption is reversible and does not involve the formation of chemical bonds between the adsorbate and the surface.\n\nChemisorption: Chemisorption involves the formation of chemical bonds between the adsorbate and the surface. This type of adsorption is stronger than physisorption and can lead to irreversible changes in the adsorbate.\n\nDissociative Adsorption: In dissociative adsorption, the adsorbate molecule dissociates into atoms or radicals upon adsorption. This process is common for molecules like hydrogen and oxygen on metal surfaces.","type":"content","url":"/lecture-18-adsorption#types-of-adsorption","position":9},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Adsorption Isotherms","lvl2":"Adsorption on Surfaces"},"type":"lvl3","url":"/lecture-18-adsorption#adsorption-isotherms","position":10},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Adsorption Isotherms","lvl2":"Adsorption on Surfaces"},"content":"The adsorption isotherm is a plot of the amount of adsorbate adsorbed on the surface as a function of pressure or concentration at constant temperature. The shape of the isotherm provides information about the adsorption mechanism and the surface properties.","type":"content","url":"/lecture-18-adsorption#adsorption-isotherms","position":11},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Lattice Gas Model"},"type":"lvl2","url":"/lecture-18-adsorption#lattice-gas-model","position":12},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Lattice Gas Model"},"content":"In the lattice gas model, we represent the surface as a 2D lattice where each lattice site can be occupied by an adsorbate molecule. The adsorbate molecules interact with each other and with the surface through pairwise interactions. The energy of the system is given by the sum of these pairwise interactions.","type":"content","url":"/lecture-18-adsorption#lattice-gas-model","position":13},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Ideal Sorbent","lvl2":"Lattice Gas Model"},"type":"lvl3","url":"/lecture-18-adsorption#ideal-sorbent","position":14},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Ideal Sorbent","lvl2":"Lattice Gas Model"},"content":"In this model system, N_s sorbent particles interact with N_a adsorption sites on a solid surface but do not interact with each other.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define grid size\ngridsize = 4\n\n# Create figure and axis\nfig, ax = plt.subplots()\n\n# Create gridlines\nax.set_xticks(np.arange(0, gridsize + 1, 1))\nax.set_yticks(np.arange(0, gridsize + 1, 1))\nax.grid(True)\n\n# Set the limits of the plot\nax.set_xlim(0, gridsize)\nax.set_ylim(0, gridsize)\n\n# Label adsorption sites\nfor x in range(gridsize):\n    for y in range(gridsize):\n        ax.text(x + 0.5, y + 0.5, 'A', ha='center', va='center')\n\n# Define sorbent particles' positions\nsorbent_positions = [(0.5, 0.5), (2.5, 0.5), (1.5, 1.5), (3.5, 1.5), (0.5, 2.5)]\n\n# Plot sorbent particles\nfor (x, y) in sorbent_positions:\n    ax.plot(x, y, 'o', color='blue', markersize=15, zorder=10)\n\n# Add labels for N_s and N_a\nax.text(2, -0.5, r'$N_a = 4 \\times 4 = 16$ adsorption sites', ha='center')\nax.text(2, 4.5, r'$N_s = 5$ sorbent particles', color='blue', ha='center')\n\n# Hide ticks\nax.set_xticklabels([])\nax.set_yticklabels([])\n\n# Show plot\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\nNote\n\nA square lattice can be used to represent the (100) surface of face-centered cubic solids like the alkali metals Ca and Sr, group nine transition metals Rh and Ir, group ten transition metals (Ni, Pd, and Pt), group eleven transition metals (Cu, Ag, and Au), Al, and Pb.\n\nThe figure above shows an example of an ideal sorbent system with N_s = 5 sorbent particles and N_a = 16 adsorption sites on a 4 \\times 4 square lattice. The sorbent particles are represented by blue circles, and the adsorption sites are labeled with the letter “A.”\n\nIn Lecture 11, we showed that the coverage of the surface by the sorbent particles can then be calculated as\\theta = \\frac{e^{-\\beta (\\epsilon - \\mu)}}{1 + e^{-\\beta (\\epsilon - \\mu)}}\n\nwhere \\epsilon is the interaction energy between the sorbent particles and the surface, \\mu is the chemical potential of the sorbent particles, and \\beta = \\frac{1}{k_{\\rm B} T}.\n\nInterpretation of Chemical Potential\n\nThe chemical potential \\mu can be interpreted as the energy change when a sorbent particle is added to the system from a reservoir at constant temperature. A positive chemical potential indicates that the sorbent particles are more likely to adsorb on the surface, while a negative chemical potential indicates that the sorbent particles are more likely to desorb from the surface.","type":"content","url":"/lecture-18-adsorption#ideal-sorbent","position":15},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Sorbent with Mean-Field Interactions","lvl2":"Lattice Gas Model"},"type":"lvl3","url":"/lecture-18-adsorption#sorbent-with-mean-field-interactions","position":16},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Sorbent with Mean-Field Interactions","lvl2":"Lattice Gas Model"},"content":"In a more realistic model, we can introduce mean-field interactions between the sorbent particles. The mean-field interaction energy between one sorbent particle and all other sorbent particles can be set to a constant value. This interaction energy can be used to model the attractive or repulsive interactions between the sorbent particles.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define grid size\ngridsize = 4\n\n# Create figure and axis\nfig, ax = plt.subplots()\n\n# Create gridlines\nax.set_xticks(np.arange(0, gridsize + 1, 1))\nax.set_yticks(np.arange(0, gridsize + 1, 1))\nax.grid(True)\n\n# Set the limits of the plot\nax.set_xlim(0, gridsize)\nax.set_ylim(0, gridsize)\n\n# Label adsorption sites\nfor x in range(gridsize):\n    for y in range(gridsize):\n        ax.text(x + 0.5, y + 0.5, 'A', ha='center', va='center')\n\n# Define sorbent particles' positions\nsorbent_positions = [(0.5, 0.5), (2.5, 0.5), (1.5, 1.5), (3.5, 1.5), (0.5, 2.5)]\n\n# Plot sorbent particles\nfor (x, y) in sorbent_positions:\n    ax.plot(x, y, 'o', color='blue', markersize=15, zorder=10)\n\n# Mean-field interaction arrows (red lines)\nfor (x1, y1) in sorbent_positions:\n    for (x2, y2) in sorbent_positions:\n        ax.arrow(x1, y1, x2 - x1, y2 - y1, head_width=0.1, head_length=0.1, fc='red', ec='red', length_includes_head=True)\n\n# Add labels for N_s and N_a\nax.text(2, -0.5, r'$N_a = 4 \\times 4 = 16$ adsorption sites', ha='center')\nax.text(2, 4.5, r'$N_s = 5$ sorbent particles', color='blue', ha='center')\n\n# Hide ticks\nax.set_xticklabels([])\nax.set_yticklabels([])\n\n# Show plot\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\nThe figure above shows the same sorbent system as before, but now with interactions between the sorbent particles represented by red arrows. In a mean-field model, these interactions are replaced by a constant mean-field interaction energy.","type":"content","url":"/lecture-18-adsorption#sorbent-with-mean-field-interactions","position":17},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Grand Canonical Monte Carlo Simulations"},"type":"lvl2","url":"/lecture-18-adsorption#grand-canonical-monte-carlo-simulations","position":18},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Grand Canonical Monte Carlo Simulations"},"content":"In Lecture 14, we implemented a basic Metropolis algorithm for the canonical ensemble. In this lecture and the associated project, we will extend the Metropolis algorithm to the grand canonical ensemble to study adsorption on surfaces.","type":"content","url":"/lecture-18-adsorption#grand-canonical-monte-carlo-simulations","position":19},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Grand Canonical Ensemble","lvl2":"Grand Canonical Monte Carlo Simulations"},"type":"lvl3","url":"/lecture-18-adsorption#grand-canonical-ensemble","position":20},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Grand Canonical Ensemble","lvl2":"Grand Canonical Monte Carlo Simulations"},"content":"In the grand canonical ensemble, the system is in contact with a reservoir of particles at constant chemical potential \\mu, volume V, and inverse temperature \\beta. The number of particles in the system is allowed to fluctuate, and the probability of finding the system in a state with N particles is given byP(N) = \\frac{e^{-\\beta [E(N) - \\mu N]}}{Z}\n\nwhere E(N) is the energy of the system with N particles, and Z is the grand canonical partition function.","type":"content","url":"/lecture-18-adsorption#grand-canonical-ensemble","position":21},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Metropolis Algorithm for Grand Canonical Ensemble","lvl2":"Grand Canonical Monte Carlo Simulations"},"type":"lvl3","url":"/lecture-18-adsorption#metropolis-algorithm-for-grand-canonical-ensemble","position":22},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Metropolis Algorithm for Grand Canonical Ensemble","lvl2":"Grand Canonical Monte Carlo Simulations"},"content":"To simulate the grand canonical ensemble using the Metropolis algorithm, we need to modify the acceptance criterion to account for changes in the number of particles. The acceptance probability for adding a particle to the system is given by{\\rm acc}(N \\rightarrow N+1) = \\min\\left[1, \\frac{N_a - N}{N + 1} \\exp\\left[-\\beta (E(N+1) - E(N) - \\mu)\\right]\\right]\n\nwhere N_a is the total number of adsorption sites on the surface. The acceptance probability for removing a particle from the system is given by{\\rm acc}(N \\rightarrow N-1) = \\min\\left[1, \\frac{N}{N_a - N + 1} \\exp\\left[-\\beta (E(N-1) - E(N) + \\mu)\\right]\\right]","type":"content","url":"/lecture-18-adsorption#metropolis-algorithm-for-grand-canonical-ensemble","position":23},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Steps for Implementing a Grand Canonical Monte Carlo Simulation"},"type":"lvl2","url":"/lecture-18-adsorption#steps-for-implementing-a-grand-canonical-monte-carlo-simulation","position":24},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Steps for Implementing a Grand Canonical Monte Carlo Simulation"},"content":"To implement a grand canonical Monte Carlo simulation for adsorption on surfaces, we can follow these steps:\n\nInitialize the system with a random distribution of sorbent particles on the surface.\n\nChoose a random move (addition or removal of a sorbent particle) and calculate the change in energy.\n\nAccept or reject the move based on the acceptance probability.\n\nBy repeating these steps for a large number of iterations, we can sample the grand canonical ensemble and calculate the adsorption isotherm.","type":"content","url":"/lecture-18-adsorption#steps-for-implementing-a-grand-canonical-monte-carlo-simulation","position":25},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Example 1: Adsorption of Ideal Sorbent Particles on a Square Lattice"},"type":"lvl2","url":"/lecture-18-adsorption#example-1-adsorption-of-ideal-sorbent-particles-on-a-square-lattice","position":26},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Example 1: Adsorption of Ideal Sorbent Particles on a Square Lattice"},"content":"Let’s consider the adsorption of ideal sorbent particles on a 4 \\times 4 square lattice. We will implement a grand canonical Monte Carlo simulation to study the adsorption isotherm of the system.","type":"content","url":"/lecture-18-adsorption#example-1-adsorption-of-ideal-sorbent-particles-on-a-square-lattice","position":27},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Energy Calculation","lvl2":"Example 1: Adsorption of Ideal Sorbent Particles on a Square Lattice"},"type":"lvl3","url":"/lecture-18-adsorption#energy-calculation","position":28},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Energy Calculation","lvl2":"Example 1: Adsorption of Ideal Sorbent Particles on a Square Lattice"},"content":"The energy of the system is given byE(N) = N \\epsilon\n\nwhere \\epsilon is the interaction energy between the sorbent particles and the surface. Let’s define the energy function for the system.\n\ndef energy_ideal_sorbent(N, epsilon=-1.0):\n    \"\"\"Calculate the energy of the ideal sorbent system.\"\"\"\n    return N * epsilon\n\n","type":"content","url":"/lecture-18-adsorption#energy-calculation","position":29},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Grand Canonical Monte Carlo Simulation","lvl2":"Example 1: Adsorption of Ideal Sorbent Particles on a Square Lattice"},"type":"lvl3","url":"/lecture-18-adsorption#grand-canonical-monte-carlo-simulation","position":30},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Grand Canonical Monte Carlo Simulation","lvl2":"Example 1: Adsorption of Ideal Sorbent Particles on a Square Lattice"},"content":"Now, let’s implement the grand canonical Monte Carlo simulation for the ideal sorbent system.\n\nimport numpy as np\n\nnp.random.seed(42)\n\ndef grand_canonical_mc_ideal_sorbent(N_a, mu, beta, epsilon, n_steps=10000):\n    \"\"\"Perform grand canonical Monte Carlo simulation for ideal sorbent particles.\"\"\"\n    # Initialize the system\n    N = 0\n    coverage = []\n    \n    for _ in range(n_steps):\n        # Choose a random move\n        if np.random.rand() < 0.5:\n            # Attempt to add a particle\n            energy_diff = energy_ideal_sorbent(N + 1, epsilon) - energy_ideal_sorbent(N, epsilon)\n            # acc_prob = min(1, (N_a - N) / (N + 1) * np.exp(-beta * (epsilon - mu)))\n            acc_prob = min(1, (N_a - N) / (N + 1) * np.exp(-beta * (energy_diff - mu)))\n            if np.random.rand() < acc_prob:\n                N += 1\n        else:\n            # Attempt to remove a particle\n            energy_diff = energy_ideal_sorbent(N - 1, epsilon) - energy_ideal_sorbent(N, epsilon)\n            # acc_prob = min(1, N / (N_a - N + 1) * np.exp(-beta * (epsilon + mu)))\n            acc_prob = min(1, N / (N_a - N + 1) * np.exp(-beta * (energy_diff + mu)))\n            if np.random.rand() < acc_prob:\n                N -= 1\n        \n        # Calculate coverage\n        theta = N / N_a\n        coverage.append(theta)\n    \n    return coverage\n\n","type":"content","url":"/lecture-18-adsorption#grand-canonical-monte-carlo-simulation","position":31},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Simulation Results","lvl2":"Example 1: Adsorption of Ideal Sorbent Particles on a Square Lattice"},"type":"lvl3","url":"/lecture-18-adsorption#simulation-results","position":32},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Simulation Results","lvl2":"Example 1: Adsorption of Ideal Sorbent Particles on a Square Lattice"},"content":"Let’s run the grand canonical Monte Carlo simulation for the ideal sorbent system and plot the adsorption isotherm.\n\nimport matplotlib.pyplot as plt\n\n# Define system parameters\nN_a = 16\nmus = np.linspace(-2, 0, 21)\nbeta = 10.0\nepsilon = -1.0\n\n# Perform grand canonical Monte Carlo simulation for different chemical potentials\ncoverages = [np.mean(grand_canonical_mc_ideal_sorbent(N_a, mu, beta, epsilon)) for mu in mus]\n\n# Calculate the analytical solution\nmus_analytical = np.linspace(-2, 0, 100)\ncoverages_analytical = [np.exp(beta * (mu - epsilon)) / (1 + np.exp(beta * (mu - epsilon))) for mu in mus_analytical]\n\n# Plot the adsorption isotherm\nplt.plot(mus, coverages, 'o-', label='Simulation')\nplt.plot(mus_analytical, coverages_analytical, '--', label='Analytical')\nplt.xlabel(r'Chemical potential $\\mu$')\nplt.ylabel(r'Coverage $\\theta$')\nplt.legend()\nplt.show()\n\nThe plot above shows the adsorption isotherm for the ideal sorbent system obtained from the grand canonical Monte Carlo simulation (solid line) and the analytical solution (dashed line). The coverage of the surface increases with the chemical potential, as expected because the sorbent particles are more likely to adsorb on the surface at higher chemical potentials.","type":"content","url":"/lecture-18-adsorption#simulation-results","position":33},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Example 2: Adsorption of Sorbent Particles with Mean-Field Interactions"},"type":"lvl2","url":"/lecture-18-adsorption#example-2-adsorption-of-sorbent-particles-with-mean-field-interactions","position":34},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Example 2: Adsorption of Sorbent Particles with Mean-Field Interactions"},"content":"Next, let’s consider the adsorption of sorbent particles with mean-field interactions on a 4 \\times 4 square lattice. We will implement a grand canonical Monte Carlo simulation to study the adsorption isotherm of the system.","type":"content","url":"/lecture-18-adsorption#example-2-adsorption-of-sorbent-particles-with-mean-field-interactions","position":35},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Energy Calculation","lvl2":"Example 2: Adsorption of Sorbent Particles with Mean-Field Interactions"},"type":"lvl3","url":"/lecture-18-adsorption#energy-calculation-1","position":36},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Energy Calculation","lvl2":"Example 2: Adsorption of Sorbent Particles with Mean-Field Interactions"},"content":"The energy of the system with mean-field interactions is given byE(N) = N \\epsilon + N_a \\theta z \\epsilon_{\\rm int}\n\nwhere z is the coordination number of the lattice, and \\epsilon_{\\rm int} is the mean-field interaction energy between the sorbent particles. Let’s define the energy function for the system with mean-field interactions.\n\ndef energy_mean_field_sorbent(N, N_a, coverage, z=4, epsilon=-1.0, epsilon_int=0.1):\n    \"\"\"Calculate the energy of the system with mean-field interactions.\"\"\"\n    return N * epsilon + N_a * coverage * z * epsilon_int\n\nDerivation of N_a \\theta z\n\nThe term N_a \\theta z in the energy expression accounts for the mean-field interactions between the sorbent particles. The factor N_a represents the total number of adsorption sites on the surface, \\theta is the coverage of the surface by the sorbent particles, and z is the coordination number of the lattice. The product N_a \\theta z gives the number of interactions between each sorbent particle and its z nearest neighbors on the lattice.","type":"content","url":"/lecture-18-adsorption#energy-calculation-1","position":37},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Grand Canonical Monte Carlo Simulation","lvl2":"Example 2: Adsorption of Sorbent Particles with Mean-Field Interactions"},"type":"lvl3","url":"/lecture-18-adsorption#grand-canonical-monte-carlo-simulation-1","position":38},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Grand Canonical Monte Carlo Simulation","lvl2":"Example 2: Adsorption of Sorbent Particles with Mean-Field Interactions"},"content":"Now, let’s implement the grand canonical Monte Carlo simulation for the system with mean-field interactions.\n\nnp.random.seed(42)\n\ndef grand_canonical_mc_mean_field_sorbent(N_a, mu, beta, epsilon, epsilon_int, n_steps=10000):\n    \"\"\"Perform grand canonical Monte Carlo simulation for sorbent particles with mean-field interactions.\"\"\"\n    # Initialize the system\n    N = 0\n    coverage = []\n    \n    for _ in range(n_steps):\n        # Current coverage\n        theta = N / N_a\n        \n        # Choose a random move\n        if np.random.rand() < 0.5:\n            # Attempt to add a particle\n            theta_after = (N + 1) / N_a\n            energy_diff = (energy_mean_field_sorbent(N + 1, N_a, theta_after, epsilon=epsilon, epsilon_int=epsilon_int) -\n                           energy_mean_field_sorbent(N, N_a, theta, epsilon=epsilon, epsilon_int=epsilon_int))\n            acc_prob = min(1, (N_a - N) / (N + 1) * np.exp(-beta * (energy_diff - mu)))\n            if np.random.rand() < acc_prob:\n                N += 1\n        else:\n            # Attempt to remove a particle\n            theta_after = (N - 1) / N_a\n            energy_diff = (energy_mean_field_sorbent(N - 1, N_a, theta_after, epsilon=epsilon, epsilon_int=epsilon_int) -\n                           energy_mean_field_sorbent(N, N_a, theta, epsilon=epsilon, epsilon_int=epsilon_int))\n            acc_prob = min(1, N / (N_a - N + 1) * np.exp(-beta * (energy_diff + mu)))\n            if N > 0 and np.random.rand() < acc_prob:\n                N -= 1\n        \n        # Update coverage\n        theta = N / N_a\n        coverage.append(theta)\n    \n    return coverage\n\n","type":"content","url":"/lecture-18-adsorption#grand-canonical-monte-carlo-simulation-1","position":39},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Simulation Results","lvl2":"Example 2: Adsorption of Sorbent Particles with Mean-Field Interactions"},"type":"lvl3","url":"/lecture-18-adsorption#simulation-results-1","position":40},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl3":"Simulation Results","lvl2":"Example 2: Adsorption of Sorbent Particles with Mean-Field Interactions"},"content":"Let’s run the grand canonical Monte Carlo simulation for the system with mean-field interactions and plot the adsorption isotherm.\n\n# Define system parameters\nN_a = 16\nmus = np.linspace(-2, 1, 31)\nbeta = 10.0\nepsilon = -1.0\nepsilon_int = 0.1\n\n# Perform grand canonical Monte Carlo simulation for different chemical potentials\ncoverages_ideal_sorbent = [np.mean(grand_canonical_mc_ideal_sorbent(N_a, mu, beta, epsilon)) for mu in mus]\ncoverages_mean_field_sorbent = [np.mean(grand_canonical_mc_mean_field_sorbent(N_a, mu, beta, epsilon, epsilon_int)) for mu in mus]\n\n# Plot the adsorption isotherm\nplt.plot(mus, coverages_ideal_sorbent, 'o-', label='Ideal Sorbent')\nplt.plot(mus, coverages_mean_field_sorbent, 'o-', label='Mean-Field Sorbent')\nplt.axhline(y=0.5, color='gray', linestyle='--', label=r'$\\theta = 0.5$')\nplt.axvline(x=epsilon, color='gray', linestyle='--', label=r'$\\mu = \\epsilon$')\nplt.axvline(x=epsilon + 4 * epsilon_int, color='gray', linestyle='--', label=r'$\\mu = \\epsilon + 4 \\epsilon_{\\rm int}$')\nplt.xlabel(r'Chemical potential $\\mu$')\nplt.ylabel(r'Coverage $\\theta$')\nplt.legend()\nplt.show()\n\nThe plot above shows the adsorption isotherm for the ideal sorbent system and the system with mean-field interactions obtained from the grand canonical Monte Carlo simulation. The dashed lines indicate the critical chemical potentials where the coverage of the surface is equal to 0.5.","type":"content","url":"/lecture-18-adsorption#simulation-results-1","position":41},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Conclusion"},"type":"lvl2","url":"/lecture-18-adsorption#conclusion","position":42},{"hierarchy":{"lvl1":"Chapter 18: Monte Carlo Simulations of Adsorption on Surfaces","lvl2":"Conclusion"},"content":"In this lecture, we introduced the concept of adsorption on surfaces and discussed the importance of Monte Carlo simulations in studying adsorption phenomena. We implemented grand canonical Monte Carlo simulations for ideal sorbent particles and sorbent particles with mean-field interactions on a square lattice. The simulations allowed us to calculate the adsorption isotherms of the systems and analyze the effects of chemical potential and interaction energies on the adsorption behavior.\n\nNote\n\nIn the associated project, you will have the opportunity to explore more complex adsorption models and study the effects of different parameters on the adsorption isotherm. You will also learn how to analyze the simulation results and extract valuable information about the adsorption process.","type":"content","url":"/lecture-18-adsorption#conclusion","position":43},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption"},"type":"lvl1","url":"/lecture-19-project-1","position":0},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption"},"content":"","type":"content","url":"/lecture-19-project-1","position":1},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl2":"Introduction"},"type":"lvl2","url":"/lecture-19-project-1#introduction","position":2},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl2":"Introduction"},"content":"In surface science, competitive adsorption refers to the scenario where multiple species (adsorbates) compete for adsorption sites on a surface. This phenomenon is crucial in heterogeneous catalysis, separation processes, and sensor technology, where the presence of one adsorbate can significantly influence the adsorption behavior of another.\n\nIn this project, you will write a Python program that uses the grand canonical Monte Carlo (GCMC) method and the Metropolis algorithm to simulate competitive adsorption of two different adsorbate species on a two-dimensional (2D) square lattice. By varying parameters such as chemical potential, temperature, and interaction energies, you will compute the phase diagram of the system.","type":"content","url":"/lecture-19-project-1#introduction","position":3},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl2":"Competitive Adsorption on Surfaces"},"type":"lvl2","url":"/lecture-19-project-1#competitive-adsorption-on-surfaces","position":4},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl2":"Competitive Adsorption on Surfaces"},"content":"","type":"content","url":"/lecture-19-project-1#competitive-adsorption-on-surfaces","position":5},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Two-Component Adsorption Model","lvl2":"Competitive Adsorption on Surfaces"},"type":"lvl3","url":"/lecture-19-project-1#two-component-adsorption-model","position":6},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Two-Component Adsorption Model","lvl2":"Competitive Adsorption on Surfaces"},"content":"\n\n# Import necessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create figure and subplots\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n# Left panel: Types of Sites\nax = axs[0]\nax.set_title('Types of Sites')\ngrid_size = 5\n\n# Draw grid\nfor x in range(grid_size + 1):\n    ax.plot([x, x], [0, grid_size], color='black', linewidth=0.5)\nfor y in range(grid_size + 1):\n    ax.plot([0, grid_size], [y, y], color='black', linewidth=0.5)\n\n# Define site types\nempty_sites = [(1, 1), (3, 3)]\nadsorbate_A_sites = [(2, 2), (4, 1)]\nadsorbate_B_sites = [(1, 3), (3, 1)]\n\n# Plot empty sites\nfor (x, y) in empty_sites:\n    ax.plot(x + 0.5, y + 0.5, 's', color='white', markersize=40, markeredgecolor='black')\n\n# Plot adsorbate A sites\nfor (x, y) in adsorbate_A_sites:\n    ax.plot(x + 0.5, y + 0.5, 'o', color='blue', markersize=20)\n    ax.text(x + 0.5, y + 0.5, 'A', ha='center', va='center', color='white', fontsize=12)\n\n# Plot adsorbate B sites\nfor (x, y) in adsorbate_B_sites:\n    ax.plot(x + 0.5, y + 0.5, 'o', color='red', markersize=20)\n    ax.text(x + 0.5, y + 0.5, 'B', ha='center', va='center', color='white', fontsize=12)\n\nax.set_xlim(0, grid_size)\nax.set_ylim(0, grid_size)\nax.set_aspect('equal')\nax.set_xticks([])\nax.set_yticks([])\n\n# Middle panel: Adsorption Energies\nax = axs[1]\nax.set_title('Adsorption Energies')\n\n# Draw surface line\nax.plot([0, 5], [0, 0], color='black', linewidth=2)\nax.text(2.5, -0.5, 'Surface', ha='center', fontsize=12)\n\n# Adsorbate A approaching surface\nax.plot(1, 3, 'o', color='blue', markersize=20)\nax.arrow(1, 2.5, 0, -2.0, head_width=0.2, head_length=0.3, fc='blue', ec='blue')\nax.text(1, 3.5, r'$\\epsilon_A$', ha='center', color='blue', fontsize=12)\n\n# Adsorbate B approaching surface\nax.plot(4, 3, 'o', color='red', markersize=20)\nax.arrow(4, 2.5, 0, -2.0, head_width=0.2, head_length=0.3, fc='red', ec='red')\nax.text(4, 3.5, r'$\\epsilon_B$', ha='center', color='red', fontsize=12)\n\nax.set_xlim(0, 5)\nax.set_ylim(-1, 4)\nax.axis('off')\n\n# Right panel: Interaction Energies\nax = axs[2]\nax.set_title('Interaction Energies')\n\n# Adsorbates on surface\nax.plot(2, 2, 'o', color='blue', markersize=20)\nax.text(2, 2, 'A', ha='center', va='center', color='white', fontsize=12)\nax.plot(3, 2, 'o', color='blue', markersize=20)\nax.text(3, 2, 'A', ha='center', va='center', color='white', fontsize=12)\nax.plot(2.5, 1, 'o', color='red', markersize=20)\nax.text(2.5, 1, 'B', ha='center', va='center', color='white', fontsize=12)\n\n# Interaction between A-A\nax.plot([2, 3], [2, 2], color='black', linestyle='--')\nax.text(2.5, 2.1, r'$\\epsilon_{AA}$', ha='center', fontsize=12)\n\n# Interaction between A-B\nax.plot([2, 2.5], [2, 1], color='black', linestyle='--')\nax.text(2.4, 1.5, r'$\\epsilon_{AB}$', ha='center', fontsize=12)\n\nax.set_xlim(1, 4)\nax.set_ylim(0.5, 3)\nax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nConsider a 2D square lattice representing the surface, where each lattice site can be empty (white squares in the left panel), occupied by adsorbate A (blue circles), or occupied by adsorbate B (red circles). The adsorption behavior is governed by adsorption energies \\epsilon_A and \\epsilon_B (middle panel) and interaction energies \\epsilon_{AA}, \\epsilon_{BB}, and \\epsilon_{AB} (right panel).","type":"content","url":"/lecture-19-project-1#two-component-adsorption-model","position":7},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Lattice Model for Two Species","lvl2":"Competitive Adsorption on Surfaces"},"type":"lvl3","url":"/lecture-19-project-1#lattice-model-for-two-species","position":8},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Lattice Model for Two Species","lvl2":"Competitive Adsorption on Surfaces"},"content":"In this lattice model, the total energy of the system is given byE = \\sum_{i} \\left( n_i^A \\epsilon_A + n_i^B \\epsilon_B \\right) + \\frac{1}{2} \\sum_{\\langle i,j \\rangle} \\left( n_i^A n_j^A \\epsilon_{AA} + n_i^B n_j^B \\epsilon_{BB} + n_i^A n_j^B \\epsilon_{AB} + n_i^B n_j^A \\epsilon_{AB} \\right)\n\nwhere n_i^A and n_i^B are the number of adsorbates A and B at site i, respectively. If site i is empty, n_i^A = n_i^B = 0. If site i is occupied by adsorbate A, n_i^A = 1 and n_i^B = 0. If site i is occupied by adsorbate B, n_i^A = 0 and n_i^B = 1. The first sum runs over all lattice sites, and the second sum runs over all pairs of neighboring sites. The factor \\frac{1}{2} avoids double-counting interactions.","type":"content","url":"/lecture-19-project-1#lattice-model-for-two-species","position":9},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Grand Canonical Ensemble","lvl2":"Competitive Adsorption on Surfaces"},"type":"lvl3","url":"/lecture-19-project-1#grand-canonical-ensemble","position":10},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Grand Canonical Ensemble","lvl2":"Competitive Adsorption on Surfaces"},"content":"In the grand canonical ensemble, this system is open to exchange particles and energy with a reservoir at fixed chemical potentials \\mu_A and \\mu_B for adsorbates A and B, respectively, and temperature T. The probability distribution function for the system is proportional toP(\\{n_i^A, n_i^B\\}) \\propto \\exp\\left(-\\beta \\Omega\\right)\n\nwhere \\Omega is the grand potential given by\\Omega = E - \\mu_A N_A - \\mu_B N_B\n\nand \\beta = \\frac{1}{kT} is the inverse temperature. The numbers of adsorbates A and B are denoted by N_A and N_B, respectively.","type":"content","url":"/lecture-19-project-1#grand-canonical-ensemble","position":11},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"type":"lvl2","url":"/lecture-19-project-1#grand-canonical-monte-carlo-gcmc-simulations","position":12},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"content":"","type":"content","url":"/lecture-19-project-1#grand-canonical-monte-carlo-gcmc-simulations","position":13},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Metropolis Algorithm Adapted for GCMC","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"type":"lvl3","url":"/lecture-19-project-1#metropolis-algorithm-adapted-for-gcmc","position":14},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Metropolis Algorithm Adapted for GCMC","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"content":"In the grand canonical ensemble, the Metropolis algorithm is adapted to allow for particle addition and removal moves.\n\nParticle Addition\n\nChoose an empty site at random.\n\nDecide to add A or B based on predefined probabilities or randomly.\n\nCalculate the change in energy \\Delta E.\n\nInclude the ratio of proposal probabilities in the acceptance probability.\n\nAccept the move with probability\\text{acc} = \\min\\left[1, \\frac{N_a - N_s}{N_s + 1} \\exp\\left(-\\beta [\\Delta E - \\mu_s]\\right)\\right]\n\nwhere N_a is the number of empty sites, N_s is the number of sites occupied by species s, and \\mu_s is the chemical potential of species s.\n\nUpdate the lattice configuration and particle counts.\n\nParticle Removal\n\nChoose an occupied site at random.\n\nCalculate the change in energy \\Delta E for removing the particle.\n\nInclude the ratio of proposal probabilities in the acceptance probability.\n\nAccept the move with probability\\text{acc} = \\min\\left[1, \\frac{N_s}{N_a - N_s + 1} \\exp\\left(-\\beta [\\Delta E + \\mu_s]\\right)\\right]\n\nwhere N_a is the number of empty sites, N_s is the number of sites occupied by species s, and \\mu_s is the chemical potential of species $s.\n\nUpdate the lattice configuration and particle counts.","type":"content","url":"/lecture-19-project-1#metropolis-algorithm-adapted-for-gcmc","position":15},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Psuedocode for GCMC Simulation","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"type":"lvl3","url":"/lecture-19-project-1#psuedocode-for-gcmc-simulation","position":16},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Psuedocode for GCMC Simulation","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"content":"","type":"content","url":"/lecture-19-project-1#psuedocode-for-gcmc-simulation","position":17},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl4":"Initialize Lattice","lvl3":"Psuedocode for GCMC Simulation","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"type":"lvl4","url":"/lecture-19-project-1#initialize-lattice","position":18},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl4":"Initialize Lattice","lvl3":"Psuedocode for GCMC Simulation","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"content":"FUNCTION initialize_lattice(size):\n    CREATE a 2D array 'lattice' of dimensions size x size\n    INITIALIZE all elements of 'lattice' to 0  # 0 represents empty sites\n    RETURN 'lattice'","type":"content","url":"/lecture-19-project-1#initialize-lattice","position":19},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl4":"Compute Neighbor Indices with Periodic Boundary Conditions","lvl3":"Psuedocode for GCMC Simulation","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"type":"lvl4","url":"/lecture-19-project-1#compute-neighbor-indices-with-periodic-boundary-conditions","position":20},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl4":"Compute Neighbor Indices with Periodic Boundary Conditions","lvl3":"Psuedocode for GCMC Simulation","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"content":"FUNCTION compute_neighbor_indices(size):\n    CREATE an empty dictionary 'neighbor_indices'\n    FOR x FROM 0 TO size - 1:\n        FOR y FROM 0 TO size - 1:\n            neighbors = [\n                ((x - 1) MOD size, y),\n                ((x + 1) MOD size, y),\n                (x, (y - 1) MOD size),\n                (x, (y + 1) MOD size)\n            ]\n            neighbor_indices[(x, y)] = neighbors\n    RETURN 'neighbor_indices'","type":"content","url":"/lecture-19-project-1#compute-neighbor-indices-with-periodic-boundary-conditions","position":21},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl4":"Calculate Interaction Energy","lvl3":"Psuedocode for GCMC Simulation","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"type":"lvl4","url":"/lecture-19-project-1#calculate-interaction-energy","position":22},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl4":"Calculate Interaction Energy","lvl3":"Psuedocode for GCMC Simulation","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"content":"FUNCTION calculate_interaction_energy(lattice, site, particle, neighbor_indices, epsilon_AA, epsilon_BB, epsilon_AB):\n    SET x, y TO coordinates of 'site'\n    SET 'interaction_energy' TO 0\n    FOR each 'neighbor' in neighbor_indices[(x, y)]:\n        SET 'neighbor_particle' TO lattice[neighbor]\n        IF 'neighbor_particle' IS NOT 0:\n            IF 'particle' IS 1:  # Particle A\n                IF 'neighbor_particle' IS 1:\n                    ADD 'epsilon_AA' TO 'interaction_energy'\n                ELSE:  # Neighbor is Particle B\n                    ADD 'epsilon_AB' TO 'interaction_energy'\n            ELSE:  # Particle B\n                IF 'neighbor_particle' IS 2:\n                    ADD 'epsilon_BB' TO 'interaction_energy'\n                ELSE:  # Neighbor is Particle A\n                    ADD 'epsilon_AB' TO 'interaction_energy'\n    RETURN 'interaction_energy'","type":"content","url":"/lecture-19-project-1#calculate-interaction-energy","position":23},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl4":"Attempt to Add or Remove a Particle","lvl3":"Psuedocode for GCMC Simulation","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"type":"lvl4","url":"/lecture-19-project-1#attempt-to-add-or-remove-a-particle","position":24},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl4":"Attempt to Add or Remove a Particle","lvl3":"Psuedocode for GCMC Simulation","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"content":"FUNCTION attempt_move(lattice, N_A, N_B, N_empty, neighbor_indices, params):\n    SET 'size' TO dimension of 'lattice'\n    SET 'N_sites' TO size * size\n    SET 'beta' TO 1 / params['T']\n    EXTRACT parameters:\n        'epsilon_A', 'epsilon_B', 'epsilon_AA', 'epsilon_BB', 'epsilon_AB', 'mu_A', 'mu_B' FROM 'params'\n\n    DECIDE whether to add or remove a particle (50% chance each)\n    IF adding a particle:\n        IF 'N_empty' IS 0:\n            RETURN 'N_A', 'N_B', 'N_empty'  # No empty sites available\n        SELECT a random 'site' from empty sites in 'lattice'\n        DECIDE which particle to add (A or B) with equal probability\n        IF adding Particle A:\n            SET 'particle' TO 1\n            SET 'mu' TO 'mu_A'\n            SET 'epsilon' TO 'epsilon_A'\n            SET 'N_s' TO 'N_A'\n        ELSE:  # Adding Particle B\n            SET 'particle' TO 2\n            SET 'mu' TO 'mu_B'\n            SET 'epsilon' TO 'epsilon_B'\n            SET 'N_s' TO 'N_B'\n        CALCULATE 'delta_E' = 'epsilon' + calculate_interaction_energy(...)\n        CALCULATE acceptance probability 'acc_prob' = MIN[1, (N_empty) / (N_s + 1) * exp(-beta * (delta_E - mu))]\n        GENERATE a random number 'r' between 0 and 1\n        IF 'r' < 'acc_prob':\n            SET lattice[site] = 'particle'\n            UPDATE counts:\n                IF 'particle' IS 1:\n                    INCREMENT 'N_A' by 1\n                ELSE:\n                    INCREMENT 'N_B' by 1\n            DECREMENT 'N_empty' by 1\n    ELSE:  # Removing a particle\n        IF 'N_sites' - 'N_empty' IS 0:\n            RETURN 'N_A', 'N_B', 'N_empty'  # No particles to remove\n        SELECT a random 'site' from occupied sites in 'lattice'\n        GET 'particle' at 'site'\n        IF 'particle' IS 1:\n            SET 'mu' TO 'mu_A'\n            SET 'epsilon' TO 'epsilon_A'\n            SET 'N_s' TO 'N_A'\n        ELSE:  # Particle B\n            SET 'mu' TO 'mu_B'\n            SET 'epsilon' TO 'epsilon_B'\n            SET 'N_s' TO 'N_B'\n        CALCULATE 'delta_E' = -'epsilon' - calculate_interaction_energy(...)\n        CALCULATE acceptance probability 'acc_prob' = MIN[1, N_s / (N_empty + 1) * exp(-beta * (delta_E + mu))]\n        GENERATE a random number 'r' between 0 and 1\n        IF 'r' < 'acc_prob':\n            SET lattice[site] = 0  # Remove particle\n            UPDATE counts:\n                IF 'particle' IS 1:\n                    DECREMENT 'N_A' by 1\n                ELSE:\n                    DECREMENT 'N_B' by 1\n            INCREMENT 'N_empty' by 1\n    RETURN 'N_A', 'N_B', 'N_empty'","type":"content","url":"/lecture-19-project-1#attempt-to-add-or-remove-a-particle","position":25},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl4":"Run the GCMC Simulation","lvl3":"Psuedocode for GCMC Simulation","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"type":"lvl4","url":"/lecture-19-project-1#run-the-gcmc-simulation","position":26},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl4":"Run the GCMC Simulation","lvl3":"Psuedocode for GCMC Simulation","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"content":"FUNCTION run_simulation(size, n_steps, params):\n    INITIALIZE 'lattice' using initialize_lattice(size)\n    COMPUTE 'neighbor_indices' using compute_neighbor_indices(size)\n    SET 'N_sites' TO size * size\n    INITIALIZE counts:\n        'N_A' = 0\n        'N_B' = 0\n        'N_empty' = 'N_sites'\n    CREATE arrays 'coverage_A' and 'coverage_B' of length 'n_steps'\n\n    FOR 'step' FROM 0 TO 'n_steps' - 1:\n        UPDATE 'N_A', 'N_B', 'N_empty' by calling attempt_move(...)\n        SET coverage_A[step] = 'N_A' / 'N_sites'\n        SET coverage_B[step] = 'N_B' / 'N_sites'\n\n    RETURN 'lattice', 'coverage_A', 'coverage_B'","type":"content","url":"/lecture-19-project-1#run-the-gcmc-simulation","position":27},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl4":"Plot Lattice Configuration","lvl3":"Psuedocode for GCMC Simulation","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"type":"lvl4","url":"/lecture-19-project-1#plot-lattice-configuration","position":28},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl4":"Plot Lattice Configuration","lvl3":"Psuedocode for GCMC Simulation","lvl2":"Grand Canonical Monte Carlo (GCMC) Simulations"},"content":"FUNCTION plot_lattice(lattice, ax, title):\n    SET 'size' TO dimension of 'lattice'\n    FOR x FROM 0 TO 'size' - 1:\n        FOR y FROM 0 TO 'size' - 1:\n            IF lattice[x, y] IS 1:\n                PLOT a red circle at position (x + 0.5, y + 0.5) on axis 'ax'\n            ELSE IF lattice[x, y] IS 2:\n                PLOT a blue circle at position (x + 0.5, y + 0.5) on axis 'ax'\n    SET axis limits and labels:\n        x-axis from 0 to 'size', y-axis from 0 to 'size'\n        REMOVE x and y tick labels\n        ENABLE minor grid lines\n        SET title of 'ax' to 'title'\n    RETURN 'ax'","type":"content","url":"/lecture-19-project-1#plot-lattice-configuration","position":29},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl2":"Example Simulation and Phase Diagram"},"type":"lvl2","url":"/lecture-19-project-1#example-simulation-and-phase-diagram","position":30},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl2":"Example Simulation and Phase Diagram"},"content":"Here is an example Python code snippet that runs a grand canonical Monte Carlo simulation for competitive adsorption on a 2D lattice and generates a phase diagram showing the coverage of adsorbates A and B as a function of chemical potential \\mu_A and temperature T. The code also plots the final lattice configurations at specific points in the phase diagram.import numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nsize = 4\nn_steps = 10000\nmus_A = np.linspace(-0.2, 0, 7)\nTs = np.linspace(0.001, 0.019, 7)\nparams = []\nfor mu_A in mus_A:\n    for T in Ts:\n        params.append({\n            'epsilon_A': -0.1,\n            'epsilon_B': -0.1,\n            'epsilon_AA': 0,\n            'epsilon_BB': 0,\n            'epsilon_AB': 0,\n            'mu_A': mu_A,\n            'mu_B': -0.1,\n            'T': T  # Temperature (in units of k)\n        })\n\n# Run the simulation\nnp.random.seed(42)\nfinal_lattice = np.zeros((len(mus_A), len(Ts), size, size))\nmean_coverage_A = np.zeros((len(mus_A), len(Ts)))\nmean_coverage_B = np.zeros((len(mus_A), len(Ts)))\nfor i, param in enumerate(params):\n    lattice, coverage_A, coverage_B = run_simulation(size, n_steps, param)\n    final_lattice[i // len(Ts), i % len(Ts)] = lattice\n    mean_coverage_A[i // len(Ts), i % len(Ts)] = np.mean(coverage_A[-1000:])\n    mean_coverage_B[i // len(Ts), i % len(Ts)] = np.mean(coverage_B[-1000:])\n\n# Plot the T-mu_A phase diagram\nfig, axs = plt.subplots_mosaic([[0, 1, 2], [3, 4, 5]], figsize=(6.5, 4.5))\n\n# Mean coverage of A\naxs[0].pcolormesh(mus_A, Ts, mean_coverage_A.T, cmap='viridis', vmin=0, vmax=1)\naxs[0].set_title(r'$\\langle \\theta_A \\rangle$')\naxs[0].set_xlabel(r'$\\mu_A$')\naxs[0].set_ylabel(r'$T$')\n\n# Mean coverage of B\naxs[1].pcolormesh(mus_A, Ts, mean_coverage_B.T, cmap='viridis', vmin=0, vmax=1)\naxs[1].set_title(r'$\\langle \\theta_B \\rangle$')\naxs[1].set_xlabel(r'$\\mu_A$')\naxs[1].set_yticks([])\n\n# Mean total coverage\ncax = axs[2].pcolormesh(mus_A, Ts, mean_coverage_A.T + mean_coverage_B.T, cmap='viridis', vmin=0, vmax=1)\naxs[2].set_title(r'$\\langle \\theta_A + \\theta_B \\rangle$')\naxs[2].set_xlabel(r'$\\mu_A$')\naxs[2].set_yticks([])\nfig.colorbar(cax, ax=axs[2], location='right', fraction=0.1)\n\n# Plot the final lattice configuration\n\n# mu_A = -0.2 eV and T = 0.01 / k\naxs[3] = plot_lattice(final_lattice[0, 3], axs[3], r'$\\mu_A = -0.2$ eV, $T = 0.01 / k$')\n\n# mu_A = -0.1 eV and T = 0.01 / k\naxs[4] = plot_lattice(final_lattice[3, 3], axs[4], r'$\\mu_A = -0.1$ eV, $T = 0.01 / k$')\n\n# mu_A = 0 eV and T = 0.01 / k\naxs[5] = plot_lattice(final_lattice[6, 3], axs[5], r'$\\mu_A = 0$ eV, $T = 0.01 / k$')\n\nplt.tight_layout()\nplt.show()\n\nThe phase diagram shows the mean coverage of adsorbates A and B on the surface as a function of chemical potential \\mu_A and temperature T. The lattice configurations at specific points in the phase diagram illustrate the adsorption behavior of the two species under different conditions.","type":"content","url":"/lecture-19-project-1#example-simulation-and-phase-diagram","position":31},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl2":"Project Description"},"type":"lvl2","url":"/lecture-19-project-1#project-description","position":32},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl2":"Project Description"},"content":"","type":"content","url":"/lecture-19-project-1#project-description","position":33},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Scenario","lvl2":"Project Description"},"type":"lvl3","url":"/lecture-19-project-1#scenario","position":34},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Scenario","lvl2":"Project Description"},"content":"You work for a chemical company that produces ammonia using the Haber-Bosch process. The company is interested in understanding the competitive adsorption of nitrogen and hydrogen on the catalyst surface to optimize the reaction conditions. Your task is to simulate the adsorption behavior of nitrogen and hydrogen on a 2D lattice and analyze the effects of varying chemical potentials and interaction energies. The company wants you to provide a detailed report on the phase diagram of the system and the implications for the ammonia synthesis process.\n\nYou decide to use a grand canonical Monte Carlo simulation to model the competitive adsorption of nitrogen and hydrogen on a 2D square lattice. You will vary the chemical potential of hydrogen (\\mu_{\\text{H}}) and hold the chemical potential of nitrogen (\\mu_{\\text{N}}) constant because nitrogen is in excess in the reaction. You do not know the exact values of the adsorption and interaction energies, so you will explore a range of parameters to understand the system’s behavior. You will generate phase diagrams showing the coverage of nitrogen and hydrogen on the surface and analyze how different parameters affect the adsorption behavior. Finally, you will write a report summarizing your findings and discussing the implications for the ammonia synthesis process.\n\nYou decide to explore the following sets of parameters:\n\nIdeal Mixture of Nitrogen and Hydrogen:\n\n\\epsilon_{\\text{N}} = -0.1 eV, \\epsilon_{\\text{H}} = -0.1 eV\n\n\\epsilon_{\\text{NN}} = 0 eV, \\epsilon_{\\text{HH}} = 0 eV, \\epsilon_{\\text{NH}} = 0 eV\n\nRepulsive Interactions between Nitrogen and Hydrogen:\n\n\\epsilon_{\\text{N}} = -0.1 eV, \\epsilon_{\\text{H}} = -0.1 eV\n\n\\epsilon_{\\text{NN}} = 0.05 eV, \\epsilon_{\\text{HH}} = 0.05 eV, \\epsilon_{\\text{NH}} = 0.05 eV\n\nAttractive Interactions between Nitrogen and Hydrogen:\n\n\\epsilon_{\\text{N}} = -0.1 eV, \\epsilon_{\\text{H}} = -0.1 eV\n\n\\epsilon_{\\text{NN}} = -0.05 eV, \\epsilon_{\\text{HH}} = -0.05 eV, \\epsilon_{\\text{NH}} = -0.05 eV\n\nImmiscible Nitrogen and Hydrogen:\n\n\\epsilon_{\\text{N}} = -0.1 eV, \\epsilon_{\\text{H}} = -0.1 eV\n\n\\epsilon_{\\text{NN}} = -0.05 eV, \\epsilon_{\\text{HH}} = -0.05 eV, \\epsilon_{\\text{NH}} = 0.05 eV\n\n“Like Dissolves Unlike” Scenario:\n\n\\epsilon_{\\text{N}} = -0.1 eV, \\epsilon_{\\text{H}} = -0.1 eV\n\n\\epsilon_{\\text{NN}} = 0.05 eV, \\epsilon_{\\text{HH}} = 0.05 eV, \\epsilon_{\\text{NH}} = -0.05 eV\n\nYou will run the simulation for each set of parameters and generate phase diagrams showing the coverage of nitrogen and hydrogen on the surface. You will analyze the phase diagrams and discuss the implications of the results for the ammonia synthesis process. Your report should include figures of the phase diagrams and lattice configurations for each parameter set and a detailed discussion of the physical interpretation of the results. You should also discuss how the competitive adsorption behavior of nitrogen and hydrogen influences the ammonia synthesis process and suggest potential strategies for optimizing the reaction conditions.","type":"content","url":"/lecture-19-project-1#scenario","position":35},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Requirements","lvl2":"Project Description"},"type":"lvl3","url":"/lecture-19-project-1#requirements","position":36},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Requirements","lvl2":"Project Description"},"content":"Implementation\n\nUse the provided pseudocode as a guide to implement the grand canonical Monte Carlo simulation for competitive adsorption.\n\nUse the provided example code snippet to test your implementation.\n\nPhase Diagrams\n\nUse the provided mus_A and Ts arrays to explore the parameter space and generate phase diagrams showing the coverage of nitrogen and hydrogen on the surface.\n\nPlot the mean coverage of nitrogen, hydrogen, and the total coverage as a function of chemical potential \\mu_{\\text{H}} and temperature T.\n\nInclude color bars to indicate the coverage values in the phase diagrams.\n\nAnalysis\n\nAnalyze the phase diagrams and discuss the adsorption behavior of nitrogen and hydrogen under different conditions.\n\nCompare the phase diagrams for the different parameter sets and discuss the effects of varying adsorption and interaction energies.\n\nDiscuss the implications of the results for the ammonia synthesis process and suggest strategies for optimizing the reaction conditions.\n\nReport\n\nWrite a report summarizing your findings.\n\nInclude figures of the phase diagrams and lattice configurations for each parameter set.\n\nDiscuss the physical interpretation of the results and the implications for the ammonia synthesis process.\n\nSubmission\n\nPush your code and report to the project repository on GitHub.\n\nSubmit the project repository link to the course portal.","type":"content","url":"/lecture-19-project-1#requirements","position":37},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Optional Enhancements (Five Bonus Points Each)","lvl2":"Project Description"},"type":"lvl3","url":"/lecture-19-project-1#optional-enhancements-five-bonus-points-each","position":38},{"hierarchy":{"lvl1":"Project 1: Grand Canonical Monte Carlo Simulations of Competitive Adsorption","lvl3":"Optional Enhancements (Five Bonus Points Each)","lvl2":"Project Description"},"content":"Implement additional moves such as particle swaps to enhance the simulation.\n\nExplore more complex lattice geometries (e.g., triangular or hexagonal lattices) and interaction schemes.\n\nInvestigate the effects of lattice size on the adsorption behavior and phase diagrams.\n\nExtend the simulation to include more than two adsorbate species and study the competitive adsorption behavior.\n\nImplement a visualization tool to animate the adsorption process and observe the stochastic behavior of the system.","type":"content","url":"/lecture-19-project-1#optional-enhancements-five-bonus-points-each","position":39},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics"},"type":"lvl1","url":"/lecture-20-molecular-dynamics","position":0},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics"},"content":"","type":"content","url":"/lecture-20-molecular-dynamics","position":1},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-20-molecular-dynamics#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to\n\nDescribe why molecular dynamics simulations are very similar to real experiments.\n\nExplain how an obserable quantity can be “measured” in a molecular dynamics simulation.\n\nDescribe the basic idea of equipartition of energy.","type":"content","url":"/lecture-20-molecular-dynamics#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl2":"Introduction"},"type":"lvl2","url":"/lecture-20-molecular-dynamics#introduction","position":4},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl2":"Introduction"},"content":"Newton’s Equations of Motion\n\nNewton’s equations of motion are a set of three equations that describe the motion of an object in terms of its position, velocity, and acceleration. The equations are given by\\begin{aligned}\nF_x &= m a_x = m \\frac{dv_x}{dt} = m \\frac{d^2 x}{dt^2} \\\\\nF_y &= m a_y = m \\frac{dv_y}{dt} = m \\frac{d^2 y}{dt^2} \\\\\nF_z &= m a_z = m \\frac{dv_z}{dt} = m \\frac{d^2 z}{dt^2}\n\\end{aligned}\n\nwhere F_x, F_y, and F_z are the forces acting on the object in the x, y, and z directions, respectively, m is the mass of the object, a_x, a_y, and a_z are the accelerations of the object in the x, y, and z directions, respectively, v_x, v_y, and v_z are the velocities of the object in the x, y, and z directions, respectively, and x, y, and z are the positions of the object in the x, y, and z directions, respectively.\n\nMolecular dynamics is a simulation technique that is used to study the behavior of atoms and molecules. The basic idea is to simulate the motion of atoms and molecules in a system by solving Newton’s equations of motion. This is done by calculating the forces acting on each atom and molecule in the system and then using these forces to update the positions and velocities of the atoms and molecules. Molecular dynamics simulations are very similar to real experiments in that they provide a way to observe the behavior of atoms and molecules in a system over time.\n\nReal Experiment\n\nMolecular Dynamics Simulation\n\nPrepare a sample of atoms or molecules\n\nCreate a model of the system with atoms and molecules\n\nMeasure the behavior of the atoms or molecules\n\nSimulate the motion of the atoms and molecules\n\nOne of the key advantages of molecular dynamics simulations is that they can provide detailed information about the behavior of atoms and molecules that is difficult or impossible to obtain experimentally.","type":"content","url":"/lecture-20-molecular-dynamics#introduction","position":5},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl2":"Measuring Observable Quantities"},"type":"lvl2","url":"/lecture-20-molecular-dynamics#measuring-observable-quantities","position":6},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl2":"Measuring Observable Quantities"},"content":"Degrees of Freedom\n\nThe number of degrees of freedom of a system is equal to the number of independent ways in which the system can store energy. For example, a particle in three-dimensional space has three degrees of freedom, corresponding to the three spatial dimensions.\n\nIn a molecular dynamics simulation, we can “measure” observable quantities by calculating the average value of the quantity over time. For example, to measure the temperature of a system, we can make use of the equipartition of energy principle, which states that the average kinetic energy of a particle in a classical system is equal to \\frac{1}{2} k_\\text{B} T per degree of freedom, where k_\\text{B} is the Boltzmann constant and T is the temperature of the system. By calculating the average kinetic energy of the particles in the system, we can determine the temperature of the system. Similarly, we can calculate other observable quantities such as the pressure, density, and diffusion coefficient of the system by measuring the appropriate quantities and averaging them over time.","type":"content","url":"/lecture-20-molecular-dynamics#measuring-observable-quantities","position":7},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl3":"Calculating Observable Quantities in Molecular Dynamics","lvl2":"Measuring Observable Quantities"},"type":"lvl3","url":"/lecture-20-molecular-dynamics#calculating-observable-quantities-in-molecular-dynamics","position":8},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl3":"Calculating Observable Quantities in Molecular Dynamics","lvl2":"Measuring Observable Quantities"},"content":"","type":"content","url":"/lecture-20-molecular-dynamics#calculating-observable-quantities-in-molecular-dynamics","position":9},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl4":"Temperature","lvl3":"Calculating Observable Quantities in Molecular Dynamics","lvl2":"Measuring Observable Quantities"},"type":"lvl4","url":"/lecture-20-molecular-dynamics#temperature","position":10},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl4":"Temperature","lvl3":"Calculating Observable Quantities in Molecular Dynamics","lvl2":"Measuring Observable Quantities"},"content":"The temperature is related to the average kinetic energyT = \\frac{2 \\langle K \\rangle}{\\left( 3N - N_\\text{constraints} \\right) k_\\text{B}}\n\nwhere \\langle K \\rangle is the average kinetic energy, N is the number of particles, and N_\\text{constraints} accounts for any constraints (e.g., fixed bonds).","type":"content","url":"/lecture-20-molecular-dynamics#temperature","position":11},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl4":"Pressure","lvl3":"Calculating Observable Quantities in Molecular Dynamics","lvl2":"Measuring Observable Quantities"},"type":"lvl4","url":"/lecture-20-molecular-dynamics#pressure","position":12},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl4":"Pressure","lvl3":"Calculating Observable Quantities in Molecular Dynamics","lvl2":"Measuring Observable Quantities"},"content":"Virial Theorem\n\nThe virial theorem relates the average kinetic energy of a system to the average potential energy of the system. It is given by\\left\\langle \\sum_{i<j} \\mathbf{r}_{ij} \\cdot \\mathbf{F}_{ij} \\right\\rangle = 3 N k_\\text{B} T\n\nwhere \\mathbf{r}_{ij} is the distance vector between particles i and j, and \\mathbf{F}_{ij} is the force between particles i and j.\n\nThe pressure can be calculated using the virial theoremP = \\frac{N k_\\text{B} T}{V} + \\frac{1}{3V} \\left\\langle \\sum_{i<j} \\mathbf{r}_{ij} \\cdot \\mathbf{F}_{ij} \\right\\rangle\n\nwhere V is the volume of the simulation box, \\mathbf{r}_{ij} is the distance vector between particles i and j, and \\mathbf{F}_{ij} is the force between particles i and j.","type":"content","url":"/lecture-20-molecular-dynamics#pressure","position":13},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl4":"Diffusion Coefficient","lvl3":"Calculating Observable Quantities in Molecular Dynamics","lvl2":"Measuring Observable Quantities"},"type":"lvl4","url":"/lecture-20-molecular-dynamics#diffusion-coefficient","position":14},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl4":"Diffusion Coefficient","lvl3":"Calculating Observable Quantities in Molecular Dynamics","lvl2":"Measuring Observable Quantities"},"content":"The diffusion coefficient D can be obtained from the mean squared displacementD = \\lim_{t \\to \\infty} \\frac{\\langle \\left[ \\mathbf{r}(t) - \\mathbf{r}(0) \\right]^2 \\rangle}{6 t}\n\nwhere \\mathbf{r}(t) is the position of the particle at time t.","type":"content","url":"/lecture-20-molecular-dynamics#diffusion-coefficient","position":15},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl3":"Understanding the Equipartition Theorem","lvl2":"Measuring Observable Quantities"},"type":"lvl3","url":"/lecture-20-molecular-dynamics#understanding-the-equipartition-theorem","position":16},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl3":"Understanding the Equipartition Theorem","lvl2":"Measuring Observable Quantities"},"content":"The equipartition theorem states that in thermal equilibrium, energy is equally partitioned among all available degrees of freedom. For each degree of freedom, the average energy is\\langle E \\rangle = \\frac{1}{2} k_\\text{B} T\n\nThe implications in molecular dynamics are that each particle in three dimensions has three translational degrees of freedom and the total kinetic energy for N particles is\\langle K \\rangle = \\frac{3}{2} N k_\\text{B} T\n\nThe theorem arises from statistical mechanics and assumes that particles follow the Maxwell-Boltzmann distribution. It applies to classical systems where quantum effects are negligible. The Maxwell-Boltzmann distribution describes the distribution of speeds of particles in a gas at a given temperature. It is given byf(v) = 4 \\pi \\left( \\frac{m}{2 \\pi k_\\text{B} T} \\right)^{3/2} v^2 e^{-\\frac{m v^2}{2 k_\\text{B} T}}\n\nwhere f(v) is the probability density function of the speed v, m is the mass of the particle, and T is the temperature of the gas. The following code cell plots the Maxwell-Boltzmann distribution for helium, neon, argon, and xenon at a temperature of 298.15 K.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.constants import physical_constants\n\n# Constants\n# https://physics.nist.gov/cgi-bin/Compositions/stand_alone.pl\nm_He = 4.002_603_254_13  # Relative atomic mass of He\nm_Ne = 19.992_440_1762   # Relative atomic mass of Ne\nm_Ar = 39.962_383_1237   # Relative atomic mass of Ar\nm_Xe = 131.904_155_0856  # Relative atomic mass of Xe\nms = np.array([m_He, m_Ne, m_Ar, m_Xe]) * physical_constants['atomic mass constant'][0]  # Masses in kg\nT = 298.15  # Temperature in K\n\n# Speed range\nv = np.linspace(0, 2500, 1000)  # Speed in m/s\n\n# Maxwell-Boltzmann distribution\ndef maxwell_boltzmann(v, m, T):\n    return 4 * np.pi * (m / (2 * np.pi * physical_constants['Boltzmann constant'][0] * T))**(3/2) * v**2 * np.exp(-m * v**2 / (2 * physical_constants['Boltzmann constant'][0] * T))\n\n# Plotting the distribution\nplt.figure(figsize=(10, 6))\nlabels = ['$^4$He', '$^{20}$Ne', '$^{40}$Ar', '$^{132}$Xe']\nfor i, m in enumerate(ms):\n    plt.plot(v, maxwell_boltzmann(v, m, T), label=labels[i])\nplt.xlabel('Speed (m/s)')\nplt.ylabel('Probability Density')\nplt.title('Maxwell-Boltzmann Distribution')\nplt.legend()\n\nplt.grid(True)\nplt.show()\n\nThe plot shows that the distribution of speeds of particles in a gas depends on the mass of the particles. Lighter particles have higher speeds on average than heavier particles at the same temperature.","type":"content","url":"/lecture-20-molecular-dynamics#understanding-the-equipartition-theorem","position":17},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl2":"Example: Macroscopic Dynamics of a Golf Ball"},"type":"lvl2","url":"/lecture-20-molecular-dynamics#example-macroscopic-dynamics-of-a-golf-ball","position":18},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl2":"Example: Macroscopic Dynamics of a Golf Ball"},"content":"Why Study the Trajectory of a Golf Ball?\n\nIn the ensuing lectures, we will study different algorithms for integrating the equations of motion. Other algorithms are implemented in scipy.integrate, such as the \n\nRunge-Kutta method. The trajectory of a golf ball is a simple example that can be used to explore these algorithms.\n\nIn this example, we will simulate the trajectory of a golf ball of \n\nmass 0.04593 kg and \n\nradius 0.02134 m using the equations of motion. The golf ball is launched with an \n\ninitial speed of 77.65 m/s at an \n\nangle of 10.22 degrees with the horizontal. We will take into account the effects of drag and lift forces on the golf ball. The drag force is given byF_d = \\frac{1}{2} \\rho v^2 C_d A\n\nwhere \\rho is the air density (\n\n1.205 kg/m^3 at 20 °C and 1 atm), v is the speed of the golf ball, C_d is the drag coefficient (\n\n~0.25 for a hexagonally dimpled ball with a \n\nspin rate of ~2,526.3 rpm and an initial speed of ~77.65 m/s), and A is the cross-sectional area of the golf ball (\\pi r^2). The lift force is given byF_l = \\frac{1}{2} \\rho v^2 C_l A\n\nwhere C_l is the lift coefficient (\n\n~0.15 for a hexagonally dimpled ball with a spin rate of ~2,526.3 rpm and an initial speed of ~77.65 m/s). The equations of motion for the golf ball are given by\\begin{aligned}\n\\frac{dv_x}{dt} &= -\\frac{F_d}{m} \\frac{v_x}{v} - \\frac{F_l}{m} \\frac{v_y}{v} \\\\\n\\frac{dv_y}{dt} &= -\\frac{F_d}{m} \\frac{v_y}{v} + \\frac{F_l}{m} \\frac{v_x}{v} - g\n\\end{aligned}\n\nwhere v = \\sqrt{v_x^2 + v_y^2} is the speed of the golf ball, g is the acceleration due to gravity, and m is the mass of the golf ball. We will solve these equations numerically to simulate the trajectory of the golf ball.\n\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nimport matplotlib.pyplot as plt\n\n# Physical constants\nm = 0.04593          # Mass of golf ball in kg\nr = 0.02134          # Radius in meters (diameter is 42.67 mm)\nA = np.pi * r**2     # Cross-sectional area in m^2\nC_d = 0.25           # Drag coefficient for a sphere\nC_l = 0.15           # Lift coefficient\nrho = 1.225          # Air density in kg/m^3\ng = 9.81             # Acceleration due to gravity in m/s^2\n\n# Initial conditions\nv0 = 77.65                   # Initial speed in m/s\ntheta = np.deg2rad(10.22)    # Launch angle in radians\nx0 = 0.0                     # Initial horizontal position\ny0 = 0.0                     # Initial vertical position\nv0x = v0 * np.cos(theta)     # Initial horizontal velocity\nv0y = v0 * np.sin(theta)     # Initial vertical velocity\n\n# Define the ODE system\ndef golf_ball_ode(t, y):\n    x, y_pos, vx, vy = y\n    v = np.hypot(vx, vy)\n    # Drag force\n    F_d = 0.5 * rho * v**2 * C_d * A\n    F_dx = -F_d * (vx / v)\n    F_dy = -F_d * (vy / v)\n    # Lift force\n    F_l = 0.5 * rho * v**2 * C_l * A\n    F_lx = F_l * (-vy / v)\n    F_ly = F_l * (vx / v)\n    # Net forces\n    Fx = F_dx + F_lx\n    Fy = F_dy + F_ly - m * g\n    # Accelerations\n    ax = Fx / m\n    ay = Fy / m\n    return [vx, vy, ax, ay]\n\n# Event function to stop integration when the ball hits the ground\ndef hit_ground(t, y):\n    return y[1]  # Vertical position\n\nhit_ground.terminal = True    # Stop the integration\nhit_ground.direction = -1     # Only detect zeros when the function is decreasing\n\n# Time span for the simulation\nt_span = (0, 10)                   # Simulate for 10 seconds\nt_eval = np.linspace(0, 10, 1000)  # Time points where solution is computed\n\n# Initial state vector\ny_initial = [x0, y0, v0x, v0y]\n\n# Solve the ODE\nsolution = solve_ivp(\n    golf_ball_ode,\n    t_span,\n    y_initial,\n    t_eval=t_eval,\n    events=hit_ground,\n    rtol=1e-8,\n    atol=1e-10\n)\n\n# Extract the solution\nx = solution.y[0]\ny_pos = solution.y[1]\n\n# Plotting the trajectory\nplt.figure(figsize=(10, 6))\nplt.plot(x, y_pos, label='Trajectory with Drag and Lift')\nplt.xlabel('Horizontal Distance (m)')\nplt.ylabel('Vertical Distance (m)')\nplt.title('Trajectory of a Golf Ball')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n","type":"content","url":"/lecture-20-molecular-dynamics#example-macroscopic-dynamics-of-a-golf-ball","position":19},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl2":"Summary"},"type":"lvl2","url":"/lecture-20-molecular-dynamics#summary","position":20},{"hierarchy":{"lvl1":"Chapter 19: Molecular Dynamics","lvl2":"Summary"},"content":"In this lecture, we introduced the basic idea of molecular dynamics simulations and how they can be used to study the behavior of atoms and molecules in a system. We also discussed how observable quantities can be “measured” in a molecular dynamics simulation by calculating the average value of the quantity over time. Finally, we presented an example of simulating the trajectory of a golf ball using the equations of motion.","type":"content","url":"/lecture-20-molecular-dynamics#summary","position":21},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration"},"type":"lvl1","url":"/lecture-21-verlet","position":0},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration"},"content":"","type":"content","url":"/lecture-21-verlet","position":1},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-21-verlet#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to:\n\nDifferentiate between conservative and non-conservative forces and compute forces on a system of particles using potential energy functions.\n\nDerive the Verlet integration algorithm from the Taylor series expansion of particle positions.\n\nImplement the Verlet algorithm to simulate the motion of particles interacting via the Lennard-Jones potential.\n\nCalculate velocities and energies within the Verlet framework to analyze kinetic, potential, and total energy conservation in the system.","type":"content","url":"/lecture-21-verlet#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl2":"Forces"},"type":"lvl2","url":"/lecture-21-verlet#forces","position":4},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl2":"Forces"},"content":"To solve Newton’s equations of motion for a system of particles, we need to calculate the forces acting on each particle. These forces are then used to update the positions and velocities over time. Forces can be classified into two main types: conservative forces and non-conservative (dissipative) forces.","type":"content","url":"/lecture-21-verlet#forces","position":5},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Conservative Forces","lvl2":"Forces"},"type":"lvl3","url":"/lecture-21-verlet#conservative-forces","position":6},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Conservative Forces","lvl2":"Forces"},"content":"Conservative forces are those for which the work done moving an object between two points is independent of the path taken—it depends only on the initial and final positions. Examples include gravity, electrostatic forces, and elastic spring forces. Conservative forces depend solely on the positions of the particles. For such forces, we can define a potential energy function U(\\mathbf{r}) such that the force is the negative gradient of the potential energy, i.e.,\\mathbf{F} = -\\nabla U(\\mathbf{r}) = -\\left( \\frac{\\partial U}{\\partial x}, \\frac{\\partial U}{\\partial y}, \\frac{\\partial U}{\\partial z} \\right)\n\nSince the potential energy is defined up to an arbitrary constant, adding a constant to U(\\mathbf{r}) doesn’t affect the force. In systems with only conservative forces, the total mechanical energy—sum of kinetic energy T and potential energy U—is conserved, i.e.,E = T + U\n\nConservation of Mechanical Energy\n\nIn an isolated system with only conservative forces, the total mechanical energy remains constant over time. However, if non-conservative forces are present or the system interacts with its surroundings, mechanical energy can change.","type":"content","url":"/lecture-21-verlet#conservative-forces","position":7},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Non-Conservative (Dissipative) Forces","lvl2":"Forces"},"type":"lvl3","url":"/lecture-21-verlet#non-conservative-dissipative-forces","position":8},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Non-Conservative (Dissipative) Forces","lvl2":"Forces"},"content":"Non-conservative or dissipative forces are forces for which the work done depends on the path taken and typically lead to a loss of mechanical energy from the system. This lost mechanical energy is usually transformed into other forms, such as heat. Examples include friction, air resistance, and viscous drag forces. In the presence of non-conservative forces, mechanical energy is not conserved, though the total energy (including all forms) remains constant.","type":"content","url":"/lecture-21-verlet#non-conservative-dissipative-forces","position":9},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Example: Force Between Two Lennard-Jones Particles","lvl2":"Forces"},"type":"lvl3","url":"/lecture-21-verlet#example-force-between-two-lennard-jones-particles","position":10},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Example: Force Between Two Lennard-Jones Particles","lvl2":"Forces"},"content":"Reminder: Lennard-Jones Potential\n\nThe Lennard-Jones potential energy function isU(r) = 4\\epsilon \\left[ \\left( \\frac{\\sigma}{r} \\right)^{12} - \\left( \\frac{\\sigma}{r} \\right)^6 \\right]\n\nwhere r = \\|\\mathbf{r}_1 - \\mathbf{r}_2\\| is the distance between the two particles, \\epsilon is the depth of the potential well, and \\sigma is the finite distance at which the inter-particle potential is zero. The potential reaches its minimum at r = r_{\\text{min}} = 2^{1/6} \\sigma, representing the equilibrium separation distance.\n\nTo calculate the force between two particles interacting via the Lennard-Jones potential, we use\\mathbf{F} = -\\nabla U(r) = -\\frac{dU}{dr} \\cdot \\frac{\\mathbf{r}}{r}\n\nFirst, compute the derivative of U(r) with respect to r, i.e.,\\frac{dU}{dr} = \\frac{24\\epsilon}{r} \\left[ \\left( \\frac{\\sigma}{r} \\right)^6 - 2 \\left( \\frac{\\sigma}{r} \\right)^{12} \\right]\n\nThe force vector is then\\mathbf{F} = -\\frac{dU}{dr} \\cdot \\frac{\\mathbf{r}}{r} = -\\left( \\frac{dU}{dr} \\cdot \\frac{1}{r} \\right) \\mathbf{r}\n\nSo, the x-component of the force isF_x = -\\frac{dU}{dr} \\cdot \\frac{x}{r}\n\nSimilarly for the y- and z-components. This force is needed to compute the acceleration and update the positions and velocities of the particles in simulations. Let’s implement the potential energy function and the force function for two Lennard-Jones particles and plot them as functions of the distance between the particles.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Lennard-Jones potential energy function\ndef U(r, epsilon=1.0, sigma=1.0):\n    return 4 * epsilon * ((sigma / r)**12 - (sigma / r)**6)\n\n# Lennard-Jones force magnitude function\ndef F(r, epsilon=1.0, sigma=1.0):\n    return 24 * epsilon * ((2 * (sigma / r)**12) - ((sigma / r)**6)) / r\n\n# Distance between the particles\nr = np.linspace(0.8, 2.5, 100)\n\n# Avoid division by zero\nr = r[r != 0]\n\n# Plot the potential energy and force as functions of distance\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\n\n# Potential Energy Plot\naxs[0].plot(r, U(r))\naxs[0].set_xlabel('Distance ($r/\\\\sigma$)')\naxs[0].set_ylabel('Potential Energy $U(r)/\\\\epsilon$')\naxs[0].set_title('Lennard-Jones Potential')\naxs[0].axvline(2**(1/6), color='gray', linestyle='--')\naxs[0].annotate('$r_{\\\\rm min}/\\\\sigma$', xy=(2**(1/6) + 0.05, 1), fontsize=12)\naxs[0].set_ylim(-2, 2)\n\n# Force Plot\naxs[1].plot(r, F(r))\naxs[1].set_xlabel('Distance ($r/\\\\sigma$)')\naxs[1].set_ylabel('Force Magnitude $F(r)\\\\sigma/\\\\epsilon$')\naxs[1].set_title('Force Between Particles')\naxs[1].axvline(2**(1/6), color='gray', linestyle='--')\naxs[1].annotate('$r_{\\\\rm min}/\\\\sigma$', xy=(2**(1/6) + 0.05, 1.5), fontsize=12)\naxs[1].axhline(0, color='gray', linestyle='--')\naxs[1].annotate('$F > 0$, repulsive', xy=(2.5, 1), fontsize=12, va='bottom', ha='right')\naxs[1].annotate('$F < 0$, attractive', xy=(2.5, -1), fontsize=12, va='top', ha='right')\naxs[1].set_ylim(-3, 3)\n\nplt.tight_layout()\nplt.show()\n\nThe potential energy reaches its minimum at r = 2^{1/6} \\sigma, representing the equilibrium separation. For r < r_{\\rm min}, the force is positive (repulsive), pushing the particles apart. For r > r_{\\rm min}, the force is negative (attractive), pulling the particles together.","type":"content","url":"/lecture-21-verlet#example-force-between-two-lennard-jones-particles","position":11},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl2":"Verlet Integration"},"type":"lvl2","url":"/lecture-21-verlet#verlet-integration","position":12},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl2":"Verlet Integration"},"content":"The Verlet integration algorithm is a widely used numerical method for integrating Newton’s equations of motion in molecular dynamics simulations. It is particularly favored for its simplicity, time-reversibility, and good energy conservation properties over long simulations.","type":"content","url":"/lecture-21-verlet#verlet-integration","position":13},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Derivation of the Verlet Algorithm","lvl2":"Verlet Integration"},"type":"lvl3","url":"/lecture-21-verlet#derivation-of-the-verlet-algorithm","position":14},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Derivation of the Verlet Algorithm","lvl2":"Verlet Integration"},"content":"Taylor Series Expansion\n\nThe Taylor series expansion of a function f(x) around a point x = a is given byf(x) = f(a) + f'(a)(x - a) + \\frac{1}{2}f''(a)(x - a)^2 + \\frac{1}{6}f'''(a)(x - a)^3 + \\ldots\n\nwhere f'(a), f''(a), f'''(a) are the first, second, and third derivatives of f(x) evaluated at x = a, respectively.\n\nThe Verlet algorithm is derived from the Taylor series expansions of the position of a particle at times t + \\Delta t and t - \\Delta t, i.e.,\\begin{align*}\n\\mathbf{r}(t + \\Delta t) &= \\mathbf{r}(t) + \\left.\\frac{d\\mathbf{r}}{dt}\\right|_{t} \\Delta t + \\frac{1}{2}\\left.\\frac{d^2\\mathbf{r}}{dt^2}\\right|_{t} \\Delta t^2 + \\frac{1}{6}\\left.\\frac{d^3\\mathbf{r}}{dt^3}\\right|_{t} \\Delta t^3 + \\mathcal{O}(\\Delta t^4) \\\\\n&= \\mathbf{r}(t) + \\mathbf{v}(t) \\Delta t + \\frac{1}{2} \\mathbf{a}(t) \\Delta t^2 + \\frac{1}{6} \\mathbf{r}^{(3)}(t) \\Delta t^3 + \\mathcal{O}(\\Delta t^4)\n\\end{align*}\\begin{align*}\n\\mathbf{r}(t - \\Delta t) &= \\mathbf{r}(t) - \\mathbf{v}(t) \\Delta t + \\frac{1}{2} \\mathbf{a}(t) \\Delta t^2 - \\frac{1}{6} \\mathbf{r}^{(3)}(t) \\Delta t^3 + \\mathcal{O}(\\Delta t^4)\n\\end{align*}\n\nHere, \\mathbf{v}(t) is the velocity, \\mathbf{a}(t) is the acceleration, and \\mathbf{r}^{(3)}(t) is the third derivative of position with respect to time. Adding the two equations,\\begin{align*}\n\\mathbf{r}(t + \\Delta t) + \\mathbf{r}(t - \\Delta t) &= 2 \\mathbf{r}(t) + \\mathbf{a}(t) \\Delta t^2 + \\mathcal{O}(\\Delta t^4)\n\\end{align*}\n\nRewriting the equation,\\mathbf{r}(t + \\Delta t) = 2 \\mathbf{r}(t) - \\mathbf{r}(t - \\Delta t) + \\mathbf{a}(t) \\Delta t^2 + \\mathcal{O}(\\Delta t^4)\n\nSince acceleration \\mathbf{a}(t) is related to the force \\mathbf{F}(t) by Newton’s second law \\mathbf{F}(t) = m \\mathbf{a}(t), we can write\\mathbf{a}(t) = \\frac{\\mathbf{F}(t)}{m}\n\nThe final Verlet update formula is\\mathbf{r}(t + \\Delta t) = 2 \\mathbf{r}(t) - \\mathbf{r}(t - \\Delta t) + \\frac{\\mathbf{F}(t)}{m} \\Delta t^2\n\nThis formula allows us to compute the new position \\mathbf{r}(t + \\Delta t) using the current position \\mathbf{r}(t), the previous position \\mathbf{r}(t - \\Delta t), and the force \\mathbf{F}(t) at time t.","type":"content","url":"/lecture-21-verlet#derivation-of-the-verlet-algorithm","position":15},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Calculating Velocities for Kinetic Energy","lvl2":"Verlet Integration"},"type":"lvl3","url":"/lecture-21-verlet#calculating-velocities-for-kinetic-energy","position":16},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Calculating Velocities for Kinetic Energy","lvl2":"Verlet Integration"},"content":"Central Difference Approximation\n\nThe central difference approximation for the derivative of a function f(x) at a point x = a is given byf'(a) \\approx \\frac{f(a + \\Delta x) - f(a - \\Delta x)}{2 \\Delta x}\n\nwhere \\Delta x is a small step size.\n\nWhile the Verlet algorithm updates positions efficiently, velocities are also needed to compute kinetic energies and other properties. An estimate of the velocity at time t can be obtained using a central difference approximation, i.e.,\\mathbf{v}(t) = \\frac{\\mathbf{r}(t + \\Delta t) - \\mathbf{r}(t - \\Delta t)}{2 \\Delta t} + \\mathcal{O}(\\Delta t^2)\n\nThis estimation provides velocities consistent with the positions calculated by the Verlet algorithm.","type":"content","url":"/lecture-21-verlet#calculating-velocities-for-kinetic-energy","position":17},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Verlet Integration Algorithm Steps","lvl2":"Verlet Integration"},"type":"lvl3","url":"/lecture-21-verlet#verlet-integration-algorithm-steps","position":18},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Verlet Integration Algorithm Steps","lvl2":"Verlet Integration"},"content":"Set initial positions \\mathbf{r}(-\\Delta t) and \\mathbf{r}(0).\n\nCalculate the forces \\mathbf{F}(t) based on the current positions \\mathbf{r}(t).\n\nUpdate positions using the final Verlet update formula.\n\nEstimate velocities using the central difference approximation for analysis or for calculating kinetic energy.\n\nCompute kinetic and potential energies based on velocities and positions.\n\nAdvance to the next time step by updating t \\leftarrow t + \\Delta t and repeat steps 2–6.\n\nPractical Considerations\n\nInitial Conditions: Physically realistic initial positions are crucial for the stability and accuracy of the simulation.\n\nTime Step Selection: The choice of \\Delta t affects the accuracy and stability; it should be small enough to capture the dynamics of the system.\n\nEnergy Conservation: Monitoring total energy helps ensure the simulation is proceeding correctly; significant drifts may indicate numerical issues.\n\nLimitations\n\nVelocity Accuracy: Velocities are not as accurately determined as positions; if precise velocities are required, alternatives like the \n\nVelocity Verlet algorithm may be preferable.\n\nHigher-Order Forces: For systems where forces depend on higher-order derivatives, additional considerations are needed.","type":"content","url":"/lecture-21-verlet#verlet-integration-algorithm-steps","position":19},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl2":"Implementation"},"type":"lvl2","url":"/lecture-21-verlet#implementation","position":20},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl2":"Implementation"},"content":"Let’s implement the Verlet algorithm to simulate the motion of two particles interacting via the Lennard-Jones potential in one dimension. We’ll calculate the forces acting on the particles using the Lennard-Jones potential energy and force functions. The particles will start from rest near the equilibrium separation. We’ll update their positions using the Verlet algorithm, calculate their velocities, and compute the kinetic and potential energies over time.","type":"content","url":"/lecture-21-verlet#implementation","position":21},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Code Implementation","lvl2":"Implementation"},"type":"lvl3","url":"/lecture-21-verlet#code-implementation","position":22},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Code Implementation","lvl2":"Implementation"},"content":"\n\n# Simulation parameters\nepsilon = 1.0    # Depth of the potential well\nsigma = 1.0      # Distance at which the potential is zero\nm = 1.0          # Mass of each particle\ndt = 0.005       # Time step size\nnsteps = 1000    # Number of time steps\n\n# Initialize position arrays for two particles\npositions = np.zeros((nsteps, 2))\n\n# Set initial positions at time t = 0\npositions[0] = [0.0, 1.2]    # Slightly perturbed from equilibrium\npositions[1] = positions[0]  # Starting from rest\n\n# Initialize arrays for velocities and forces\nvelocities = np.zeros((nsteps, 2))\nforces = np.zeros((nsteps, 2))\n\n# Initialize arrays for kinetic and potential energies\nkinetic_energy = np.zeros(nsteps)\npotential_energy = np.zeros(nsteps)\ntotal_energy = np.zeros(nsteps)\n\n# Main Verlet integration loop\nfor i in range(1, nsteps - 1):\n    # Compute the distance between the particles\n    r12 = positions[i, 0] - positions[i, 1]\n    r12_abs = np.abs(r12)\n\n    # Compute the force magnitude and direction\n    force_mag = F(r12_abs, epsilon, sigma)\n    force_dir = np.sign(r12)\n    force = force_mag * force_dir\n\n    # Assign forces to each particle (Newton's third law)\n    forces[i, 0] = force       # Force on particle 0\n    forces[i, 1] = -force      # Equal and opposite force on particle 1\n\n    # Update positions using the Verlet algorithm\n    positions[i + 1] = (2 * positions[i] - positions[i - 1] +\n                        (forces[i] / m) * dt**2)\n\n    # Estimate velocities using central difference\n    velocities[i] = (positions[i + 1] - positions[i - 1]) / (2 * dt)\n\n    # Compute kinetic energy\n    kinetic_energy[i] = 0.5 * m * np.sum(velocities[i]**2)\n\n    # Compute potential energy\n    potential_energy[i] = U(r12_abs, epsilon, sigma)\n\n    # Compute total energy\n    total_energy[i] = kinetic_energy[i] + potential_energy[i]\n\n# Time array for plotting\ntime = np.arange(nsteps) * dt\n\n# Plot the distance between the particles and (kinetic, potential, total) energies over time\nfig, axs = plt.subplots(1, 2, figsize=(16, 5))\n\n# Distance plot\naxs[0].plot(time[1:-1], np.abs(positions[1:-1, 1] - positions[1:-1, 0]))\naxs[0].axhline(2**(1/6) * sigma, color='gray', linestyle='--', label='Equilibrium Distance')\naxs[0].set_xlabel('Time')\naxs[0].set_ylabel('Distance Between Particles')\naxs[0].set_title('Inter-Particle Distance vs. Time')\n\n# Energy plot\naxs[1].plot(time[1:-1], kinetic_energy[1:-1], label='Kinetic Energy')\naxs[1].plot(time[1:-1], potential_energy[1:-1], label='Potential Energy')\naxs[1].plot(time[1:-1], total_energy[1:-1], label='Total Energy', linestyle='--')\naxs[1].set_xlabel('Time')\naxs[1].set_ylabel('Energy')\naxs[1].set_title('Energy vs. Time')\naxs[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/lecture-21-verlet#code-implementation","position":23},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Observations","lvl2":"Implementation"},"type":"lvl3","url":"/lecture-21-verlet#observations","position":24},{"hierarchy":{"lvl1":"Chapter 20: Verlet Integration","lvl3":"Observations","lvl2":"Implementation"},"content":"The left plot shows the distance between the particles as a function of time. The particles exhibit oscillatory motion around the equilibrium separation due to the interplay of attractive and repulsive forces. The right plot shows the kinetic, potential, and total energies of the system as functions of time. The total energy remains approximately constant, showcasing the energy-conserving property of the Verlet algorithm. The kinetic and potential energies oscillate out of phase, indicating the exchange of energy between motion and interaction potential.\n\nNotes and Considerations\n\nTime Step Selection (dt):\n\nA smaller time step improves accuracy but increases computational cost.\n\nIf dt is too large, the simulation may become unstable or inaccurate.\n\nSimulation Dimension:\n\nThis example is one-dimensional for simplicity.\n\nExtending to three dimensions involves vectorizing positions, velocities, and forces.\n\nPhysical Units:\n\nThe simulation uses reduced units where \\epsilon = 1, \\sigma = 1, and m = 1.\n\nIn practical simulations, you can use units consistent with the physical system.\n\nInitial Conditions:\n\nThe initial positions significantly influence the system’s behavior.","type":"content","url":"/lecture-21-verlet#observations","position":25},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting"},"type":"lvl1","url":"/lecture-22-thermostatting","position":0},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting"},"content":"","type":"content","url":"/lecture-22-thermostatting","position":1},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-22-thermostatting#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to:\n\nExplain the concept of thermostatting in molecular dynamics simulations and its importance in maintaining constant temperature.\n\nDescribe how the Andersen thermostat works and its impact on the system’s dynamics.\n\nImplement the velocity Verlet algorithm combined with the Andersen thermostat in a 2D molecular dynamics simulation.\n\nAnalyze the effects of thermostatting on energy conservation and temperature control in simulations.","type":"content","url":"/lecture-22-thermostatting#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl2":"Setting the Computational Thermostat"},"type":"lvl2","url":"/lecture-22-thermostatting#setting-the-computational-thermostat","position":4},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl2":"Setting the Computational Thermostat"},"content":"In molecular dynamics simulations, we often want to simulate a system at a constant temperature, corresponding to the canonical ensemble (NVT ensemble). However, when we numerically integrate Newton’s equations of motion, the total energy of the system is conserved, leading to a microcanonical ensemble (NVE ensemble). To model real-world systems where temperature is controlled, we need to introduce thermostats that adjust the kinetic energy of the particles to maintain the desired temperature.\n\nThermostatting involves coupling the system to an external heat bath, which can exchange energy with the system. This process allows the simulation to sample configurations according to the canonical distribution, enabling the study of temperature-dependent properties and phase behavior.","type":"content","url":"/lecture-22-thermostatting#setting-the-computational-thermostat","position":5},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl2":"Andersen Thermostat"},"type":"lvl2","url":"/lecture-22-thermostatting#andersen-thermostat","position":6},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl2":"Andersen Thermostat"},"content":"The Andersen thermostat is a stochastic method introduced by \n\nHans C. Andersen in 1980 to maintain a constant temperature in MD simulations. It simulates the effect of collisions with an imaginary heat bath by randomly reassigning particle velocities, thus mimicking interactions with a thermal reservoir.\n\nThe key idea is to occasionally interrupt the deterministic trajectory of particles by “collisions” with the heat bath:\n\nAt each time step, each particle has a probability \\nu \\Delta t of colliding with the heat bath, where \\nu is the collision frequency.\n\nWhen a collision occurs, the particle’s velocity is reassigned from the Maxwell-Boltzmann distribution at the desired temperature T.\n\nParticles not colliding continue their motion according to Newton’s laws.\n\nThis approach maintains the correct canonical distribution of particle velocities and allows the system to equilibrate at the desired temperature.\n\nNote\n\nThe Andersen thermostat disrupts the conservation of total momentum because velocities are randomly reassigned without regard to the system’s overall momentum. This can affect properties dependent on momentum conservation, such as diffusion coefficients and flow behavior. If preserving momentum is critical for your simulation, consider using thermostats like the Nosé-Hoover thermostat, which conserves momentum while controlling temperature.","type":"content","url":"/lecture-22-thermostatting#andersen-thermostat","position":7},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl2":"Velocity Verlet Algorithm"},"type":"lvl2","url":"/lecture-22-thermostatting#velocity-verlet-algorithm","position":8},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl2":"Velocity Verlet Algorithm"},"content":"The velocity Verlet algorithm is an integration scheme that updates both positions and velocities in a time-symmetric way, offering improved numerical stability and energy conservation compared to basic Verlet integration.\n\nAlgorithm Steps\n\nPosition Update:\\mathbf{r}(t + \\Delta t) = \\mathbf{r}(t) + \\mathbf{v}(t)\\Delta t + \\frac{\\mathbf{F}(t)}{2m}\\Delta t^2\n\nCompute Forces:\nCalculate \\mathbf{F}(t + \\Delta t) based on the updated positions \\mathbf{r}(t + \\Delta t).\n\nVelocity Update:\\mathbf{v}(t + \\Delta t) = \\mathbf{v}(t) + \\frac{\\mathbf{F}(t) + \\mathbf{F}(t + \\Delta t)}{2m}\\Delta t\n\nThis method ensures that the positions and velocities are updated consistently, providing accurate trajectories for the particles.","type":"content","url":"/lecture-22-thermostatting#velocity-verlet-algorithm","position":9},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl2":"Velocity Verlet with Andersen Thermostat"},"type":"lvl2","url":"/lecture-22-thermostatting#velocity-verlet-with-andersen-thermostat","position":10},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl2":"Velocity Verlet with Andersen Thermostat"},"content":"When combining the velocity Verlet algorithm with the Andersen thermostat, the velocity update step is adjusted to include stochastic collisions.\n\nModified Velocity Update\n\nAfter the standard velocity update, apply the Andersen thermostat:\n\nFor each particle, generate a random number r between 0 and 1.\n\nIf r < \\nu \\Delta t, reassign the particle’s velocity \\mathbf{v}(t + \\Delta t) from the Maxwell-Boltzmann distribution.\n\nThe modified velocity update step becomes:\\mathbf{v}(t + \\Delta t) = \\mathbf{v}(t) + \\frac{\\mathbf{F}(t) + \\mathbf{F}(t + \\Delta t)}{2m}\\Delta t + \\mathbf{R}(t)\n\nwhere \\mathbf{R}(t) is a random vector that is non-zero only if a collision occurs.","type":"content","url":"/lecture-22-thermostatting#velocity-verlet-with-andersen-thermostat","position":11},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Drawing Random Velocities from a Maxwell-Boltzmann Velocity Distribution","lvl2":"Velocity Verlet with Andersen Thermostat"},"type":"lvl3","url":"/lecture-22-thermostatting#drawing-random-velocities-from-a-maxwell-boltzmann-velocity-distribution","position":12},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Drawing Random Velocities from a Maxwell-Boltzmann Velocity Distribution","lvl2":"Velocity Verlet with Andersen Thermostat"},"content":"To draw random velocities from a Maxwell-Boltzmann velocity distribution at the desired temperature, we can use numpy.random.normal to generate random numbers from a normal distribution and then scale them to have the desired temperature. The Maxwell-Boltzmann velocity distribution for a single dimension is given byf_v(v_x) = \\sqrt{\\frac{m}{2\\pi k_\\text{B}T}}\\exp\\left(-\\frac{mv_x^2}{2k_\\text{B}T}\\right)\n\nwhere m is the mass of the particle, k_\\text{B} is the Boltzmann constant, and T is the temperature. The random velocities can be drawn from this distribution by generating random numbers from a normal distribution with mean 0 and standard deviation \\sqrt{k_\\text{B}T/m}.","type":"content","url":"/lecture-22-thermostatting#drawing-random-velocities-from-a-maxwell-boltzmann-velocity-distribution","position":13},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl2":"Example: Andersen Thermostat in 2D"},"type":"lvl2","url":"/lecture-22-thermostatting#example-andersen-thermostat-in-2d","position":14},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl2":"Example: Andersen Thermostat in 2D"},"content":"Let’s implement the Andersen thermostat in 2D for a system of particles in a box. We will use the Lennard-Jones potential to calculate the forces between the particles.","type":"content","url":"/lecture-22-thermostatting#example-andersen-thermostat-in-2d","position":15},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Initialization","lvl2":"Example: Andersen Thermostat in 2D"},"type":"lvl3","url":"/lecture-22-thermostatting#initialization","position":16},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Initialization","lvl2":"Example: Andersen Thermostat in 2D"},"content":"First, we will define a function to initialize the positions and velocities of the particles.\n\nimport numpy as np\nnp.random.seed(5)\n\ndef initialize_particles(n_particles, box_size, temperature):\n    # Place the particles on a square lattice\n    n_side = int(np.ceil(np.sqrt(n_particles)))\n    positions = np.zeros((n_particles, 2))\n    for i in range(n_particles):\n        x = i % n_side + 0.5\n        y = i // n_side + 0.5\n        positions[i] = [x, y]\n    positions = positions * box_size / n_side\n\n    # Give the particles random velocities\n    velocities = np.random.uniform(-0.5, 0.5, (n_particles, 2))\n\n    # Calculate the center of mass velocity\n    v_com = np.sum(velocities, axis=0) / n_particles\n\n    # Subtract the center of mass velocity from the velocities\n    velocities -= v_com\n\n    # Scale the velocities to have the desired temperature\n    v2 = np.sum(velocities**2)\n    scale_factor = np.sqrt(2 * n_particles * temperature / v2)\n    velocities *= scale_factor\n\n    return positions, velocities\n\n","type":"content","url":"/lecture-22-thermostatting#initialization","position":17},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Plotting the Initial Particle Positions and Velocities","lvl2":"Example: Andersen Thermostat in 2D"},"type":"lvl3","url":"/lecture-22-thermostatting#plotting-the-initial-particle-positions-and-velocities","position":18},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Plotting the Initial Particle Positions and Velocities","lvl2":"Example: Andersen Thermostat in 2D"},"content":"Let’s plot the initial positions and velocities of the particles.\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\n\ndef plot_positions(positions, ax, title):\n    for i in range(len(positions)):\n        ax.add_patch(Circle(positions[i], 0.5, fc='cyan', ec='black'))\n    ax.set_title(title)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_xlim(0, box_size)\n    ax.set_ylim(0, box_size)\n    ax.set_aspect('equal')\n    return ax\n\ndef plot_vectors(positions, vectors, ax, title):\n    for i in range(len(positions)):\n        ax.arrow(positions[i, 0], positions[i, 1], vectors[i, 0], vectors[i, 1], head_width=0.2, head_length=0.2, fc='cyan', ec='black')\n    ax.set_title(title)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_xlim(0, box_size)\n    ax.set_ylim(0, box_size)\n    ax.set_aspect('equal')\n    return ax\n\nn_particles = 16\nbox_size = 10\ntemperature = 0.1\n\npositions, velocities = initialize_particles(n_particles, box_size, temperature)\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\nplot_positions(positions, axs[0], 'Initial Particle Positions')\nplot_vectors(positions, velocities, axs[1], 'Initial Particle Velocities')\n\nplt.show()\n\n","type":"content","url":"/lecture-22-thermostatting#plotting-the-initial-particle-positions-and-velocities","position":19},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Calculating the Forces","lvl2":"Example: Andersen Thermostat in 2D"},"type":"lvl3","url":"/lecture-22-thermostatting#calculating-the-forces","position":20},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Calculating the Forces","lvl2":"Example: Andersen Thermostat in 2D"},"content":"Next, we will define functions to calculate the Lennard-Jones potential and forces between the particles.\n\n# Lennard-Jones potential\ndef U(r, epsilon=1.0, sigma=1.0):\n    return 4 * epsilon * ((sigma / r)**12 - (sigma / r)**6)\n\n# Lennard-Jones force\ndef F(r, epsilon=1.0, sigma=1.0):\n    return 24 * epsilon * (2 * (sigma / r)**12 - (sigma / r)**6) / r\n\n# Calculate the forces between the particles\ndef calculate_forces(positions, box_size, epsilon=1.0, sigma=1.0):\n    n_particles = len(positions)\n    forces = np.zeros((n_particles, 2))\n    potential_energy = 0\n\n    for i in range(n_particles):\n        for j in range(i + 1, n_particles):\n            r = np.linalg.norm(positions[i] - positions[j])\n            potential_energy += U(r, epsilon, sigma)\n            f = F(r, epsilon, sigma) * (positions[i] - positions[j]) / r\n            forces[i] += f\n            forces[j] -= f\n\n    return forces, potential_energy\n\n","type":"content","url":"/lecture-22-thermostatting#calculating-the-forces","position":21},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Plotting the Initial Forces","lvl2":"Example: Andersen Thermostat in 2D"},"type":"lvl3","url":"/lecture-22-thermostatting#plotting-the-initial-forces","position":22},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Plotting the Initial Forces","lvl2":"Example: Andersen Thermostat in 2D"},"content":"Let’s calculate the forces between the particles and plot the initial forces.\n\nforces, _ = calculate_forces(positions, box_size)\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nplot_vectors(positions, forces, ax, 'Initial Forces')\n\nplt.show()\n\n","type":"content","url":"/lecture-22-thermostatting#plotting-the-initial-forces","position":23},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Velocity Verlet Algorithm with Andersen Thermostat","lvl2":"Example: Andersen Thermostat in 2D"},"type":"lvl3","url":"/lecture-22-thermostatting#velocity-verlet-algorithm-with-andersen-thermostat","position":24},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Velocity Verlet Algorithm with Andersen Thermostat","lvl2":"Example: Andersen Thermostat in 2D"},"content":"Now, we will implement the velocity Verlet algorithm with the Andersen thermostat.\n\ndef velocity_verlet(positions, velocities, forces, mass, box_size, temperature, dt, coupling_constant, epsilon=1.0, sigma=1.0, k_B=1.0):\n    # Update positions based on current velocities and forces\n    positions += velocities * dt + forces / (2 * mass) * dt**2\n\n    # Apply periodic boundary conditions or walls (if applicable)\n    # Here, we use walls by reflecting particles that cross the boundaries\n    for i in range(len(positions)):\n        # Check x-coordinate boundaries\n        if positions[i, 0] < sigma / 2 or positions[i, 0] > box_size - sigma / 2:\n            # Reflect position\n            positions[i, 0] = np.clip(positions[i, 0], sigma / 2, box_size - sigma / 2)\n            # Reverse velocity component\n            velocities[i, 0] *= -1\n        # Check y-coordinate boundaries\n        if positions[i, 1] < sigma / 2 or positions[i, 1] > box_size - sigma / 2:\n            positions[i, 1] = np.clip(positions[i, 1], sigma / 2, box_size - sigma / 2)\n            velocities[i, 1] *= -1\n\n    # Compute new forces based on updated positions\n    forces, potential_energy = calculate_forces(positions, box_size, epsilon, sigma)\n\n    # Update velocities with average of old and new forces\n    velocities += forces / mass * dt\n\n    # Apply the Andersen thermostat\n    for i in range(len(velocities)):\n        if np.random.rand() < coupling_constant:\n            # Reassign velocity from Maxwell-Boltzmann distribution\n            velocities[i] = np.random.normal(0, np.sqrt(k_B * temperature / mass), 2)\n\n    # Calculate kinetic energy for diagnostics\n    kinetic_energy = 0.5 * mass * np.sum(velocities**2)\n\n    return positions, velocities, forces, potential_energy, kinetic_energy\n\n","type":"content","url":"/lecture-22-thermostatting#velocity-verlet-algorithm-with-andersen-thermostat","position":25},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Running the Simulation","lvl2":"Example: Andersen Thermostat in 2D"},"type":"lvl3","url":"/lecture-22-thermostatting#running-the-simulation","position":26},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Running the Simulation","lvl2":"Example: Andersen Thermostat in 2D"},"content":"Finally, we will run the simulation and plot the final particle positions and velocities.\n\nParameter Selection\n\nCoupling Constant (\\nu \\Delta t): Determines the frequency of collisions with the heat bath. A higher value leads to more frequent temperature adjustments but can overly randomize particle motions.\n\nTime Step (\\Delta t): Must be small enough to accurately capture particle dynamics. Typically, \\Delta t is chosen so that the maximum displacement per step is much smaller than the interaction range \\sigma.\n\nNumber of Particles (N): Affects statistical accuracy and computational cost. Larger systems provide better sampling of properties but require more computational resources.\n\nUnits and Constants\n\nMass (m): Set to 1 (arbitrary units) for simplicity.\n\nDistance (\\sigma): The characteristic length scale in the Lennard-Jones potential, set to 1.\n\nEnergy (\\epsilon): The depth of the potential well in the Lennard-Jones potential, set to 1.\n\nBoltzmann Constant (k_B): Set to 1 for reduced units.\n\nTemperature (T): Expressed in units where k_B = 1.\n\n# Simulation parameters\nmass = 1\ndt = 0.01\nn_steps = 30000\ncoupling_constant = 0.1\n\n# Initialize the particles\npositions, velocities = initialize_particles(n_particles, box_size, temperature)\n\n# Calculate the forces\nforces, _ = calculate_forces(positions, box_size)\n\n# Arrays to store the history of the simulation\npositions_history = np.zeros((n_steps, n_particles, 2))\npotential_energy_history = np.zeros(n_steps)\nkinetic_energy_history = np.zeros(n_steps)\n\n# Run the simulation\nfor i in range(n_steps):\n    positions, velocities, forces, potential_energy, kinetic_energy = velocity_verlet(positions, velocities, forces, mass, box_size, temperature, dt, coupling_constant)\n    positions_history[i] = positions\n    potential_energy_history[i] = potential_energy\n    kinetic_energy_history[i] = kinetic_energy\n\n","type":"content","url":"/lecture-22-thermostatting#running-the-simulation","position":27},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Plotting the Final Particle Positions and Velocities","lvl2":"Example: Andersen Thermostat in 2D"},"type":"lvl3","url":"/lecture-22-thermostatting#plotting-the-final-particle-positions-and-velocities","position":28},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Plotting the Final Particle Positions and Velocities","lvl2":"Example: Andersen Thermostat in 2D"},"content":"\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\nplot_positions(positions, axs[0], 'Final Particle Positions')\nplot_vectors(positions, velocities, axs[1], 'Final Particle Velocities')\n\nplt.show()\n\n","type":"content","url":"/lecture-22-thermostatting#plotting-the-final-particle-positions-and-velocities","position":29},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Plotting and Analyzing Energy Conservation and Temperature Control","lvl2":"Example: Andersen Thermostat in 2D"},"type":"lvl3","url":"/lecture-22-thermostatting#plotting-and-analyzing-energy-conservation-and-temperature-control","position":30},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Plotting and Analyzing Energy Conservation and Temperature Control","lvl2":"Example: Andersen Thermostat in 2D"},"content":"\n\ntotal_energy_history = potential_energy_history + kinetic_energy_history\ntemperature_history = 2 * kinetic_energy_history / (2 * n_particles - 2)\n\nfig, axs = plt.subplot_mosaic([[0, 1], [2, 3]], figsize=(12, 12))\n\naxs[0].plot(np.arange(n_steps - 2000, n_steps), potential_energy_history[-2000:], 'tab:gray', label='Potential Energy', alpha=0.5)\naxs[0].set_title('Potential Energy')\naxs[0].set_xlabel('Time Step')\naxs[0].set_ylabel('Energy')\navg_potential_energy = np.mean(potential_energy_history[-2000:])\naxs[0].axhline(y=avg_potential_energy, color='r', linestyle='--', label='Average Potential Energy')\naxs[0].text(\n    0.5, 0.9, \n    f'Average Potential Energy: {avg_potential_energy:.2f}', \n    ha='center', \n    va='center', \n    transform=axs[0].transAxes,\n    color='red',\n    bbox=dict(facecolor='white', alpha=0.5, edgecolor='none')\n)\n\naxs[1].plot(np.arange(n_steps - 2000, n_steps), kinetic_energy_history[-2000:], 'tab:gray', label='Kinetic Energy', alpha=0.5)\naxs[1].set_title('Kinetic Energy')\naxs[1].set_xlabel('Time Step')\naxs[1].set_ylabel('Energy')\navg_kinetic_energy = np.mean(kinetic_energy_history[-2000:])\naxs[1].axhline(y=avg_kinetic_energy, color='r', linestyle='--', label='Average Kinetic Energy')\naxs[1].text(\n    0.5, 0.9, \n    f'Average Kinetic Energy: {avg_kinetic_energy:.2f}', \n    ha='center', \n    va='center', \n    transform=axs[1].transAxes,\n    color='red',\n    bbox=dict(facecolor='white', alpha=0.5, edgecolor='none')\n)\n\naxs[2].plot(np.arange(n_steps - 2000, n_steps), total_energy_history[-2000:], 'tab:gray', label='Total Energy', alpha=0.5)\naxs[2].set_title('Total Energy')\naxs[2].set_xlabel('Time Step')\naxs[2].set_ylabel('Energy')\navg_total_energy = np.mean(total_energy_history[-2000:])\naxs[2].axhline(y=avg_total_energy, color='r', linestyle='--', label='Average Total Energy')\naxs[2].text(\n    0.5, 0.9, \n    f'Average Total Energy: {avg_total_energy:.2f}', \n    ha='center', \n    va='center', \n    transform=axs[2].transAxes,\n    color='red',\n    bbox=dict(facecolor='white', alpha=0.5, edgecolor='none')\n)\n\naxs[3].plot(np.arange(n_steps - 2000, n_steps), temperature_history[-2000:], 'tab:gray', label='Temperature', alpha=0.5)\naxs[3].set_title('Temperature')\naxs[3].set_xlabel('Time Step')\naxs[3].set_ylabel('Temperature')\navg_temperature = np.mean(temperature_history[-2000:])\naxs[3].axhline(y=avg_temperature, color='tab:red', linestyle='--', label='Average Temperature')\naxs[3].text(\n    0.5, 0.9, \n    f'Average Temperature: {avg_temperature:.2f}', \n    ha='center', \n    va='center', \n    transform=axs[3].transAxes,\n    color='red',\n    bbox=dict(facecolor='white', alpha=0.5, edgecolor='none')\n)\naxs[3].axhline(y=temperature, color='tab:blue', linestyle='--', label='Desired Temperature')\naxs[3].text(\n    0.5, 0.8, \n    f'Desired Temperature: {temperature}', \n    ha='center', \n    va='center', \n    transform=axs[3].transAxes,\n    color='tab:blue',\n    bbox=dict(facecolor='white', alpha=0.5, edgecolor='none')\n)\n\nplt.show()\n\nThe plots show that while the kinetic and potential energies fluctuate due to stochastic collisions, the total energy remains relatively stable. This indicates that the Andersen thermostat maintains the system’s temperature without causing significant energy drift.\n\nThe temperature plot demonstrates that the system’s temperature converges to the desired value (T = 0.1) over time. The fluctuations around the mean temperature are expected due to the finite size of the system and stochastic nature of the thermostat.","type":"content","url":"/lecture-22-thermostatting#plotting-and-analyzing-energy-conservation-and-temperature-control","position":31},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Plotting and Analyzing Particle Dynamics","lvl2":"Example: Andersen Thermostat in 2D"},"type":"lvl3","url":"/lecture-22-thermostatting#plotting-and-analyzing-particle-dynamics","position":32},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl3":"Plotting and Analyzing Particle Dynamics","lvl2":"Example: Andersen Thermostat in 2D"},"content":"\n\nfig, axs = plt.subplots(3, 3, figsize=(18, 18))\n\nfor i, ax in enumerate(axs.flat):\n    plot_positions(positions_history[i * n_steps // 9], ax, f'Time Step {i * n_steps // 9}')\n\nplt.show()\n\nThe snapshots illustrate how particles move and interact under the influence of the Lennard-Jones potential and the Andersen thermostat. Over time, the particles begin to form clusters because their initial separation distances are greater than the equilibrium distance of the Lennard-Jones potential, and the \n\ntemperature is relatively low. As the simulation progresses, the attractive forces dominate, drawing the particles closer together to minimize the system’s potential energy.\n\nIf the simulation were run for a longer duration, the particles would eventually coalesce into a single cluster. This aggregation is driven by the particles seeking the potential energy minimum at the equilibrium separation distance. Notice how the particles arrange themselves in a hexagonally close-packed structure, which is a common configuration in 2D systems with spherically symmetric interactions like the Lennard-Jones potential.\n\nLimitations of the Andersen Thermostat\n\nMomentum Conservation: As previously noted, the Andersen thermostat does not conserve total momentum, which can impact transport properties.\n\nDynamics Alteration: Frequent velocity reassignment can disrupt the natural dynamics of the system, potentially affecting time-correlation functions.\n\nAlternatives\n\nNosé-Hoover Thermostat: A deterministic thermostat that maintains momentum conservation and better preserves dynamic properties.\n\nLangevin Dynamics: Incorporates friction and random forces, suitable for modeling systems in contact with a heat bath while maintaining some dynamical features.","type":"content","url":"/lecture-22-thermostatting#plotting-and-analyzing-particle-dynamics","position":33},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl2":"Summary"},"type":"lvl2","url":"/lecture-22-thermostatting#summary","position":34},{"hierarchy":{"lvl1":"Chapter 21: Thermostatting","lvl2":"Summary"},"content":"In this lecture, we explored the concept of thermostatting in molecular dynamics simulations, focusing on the Andersen thermostat. We discussed how the Andersen thermostat maintains constant temperature by randomly reassigning particle velocities, simulating collisions with a heat bath.\n\nWe implemented the Andersen thermostat in combination with the velocity Verlet algorithm for a 2D system of particles interacting via the Lennard-Jones potential. Through the simulation, we observed how the thermostat controls the temperature and affects energy conservation.\n\nThe results demonstrated effective temperature regulation and provided insights into the behavior of particles under thermostatting. We also discussed the limitations of the Andersen thermostat and briefly introduced alternative methods.","type":"content","url":"/lecture-22-thermostatting#summary","position":35},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)"},"type":"lvl1","url":"/lecture-23-ase","position":0},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)"},"content":"","type":"content","url":"/lecture-23-ase","position":1},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-23-ase#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you should be able to:\n\nInstall and set up ASE for molecular simulations.\n\nCreate, visualize, and manipulate molecular structures using ASE.\n\nPerform basic computational tasks such as optimizing molecular geometries and calculating energies.\n\nUse ASE in conjunction with machine learning calculators like MACE.\n\nModel adsorption phenomena on surfaces and perform molecular dynamics simulations.","type":"content","url":"/lecture-23-ase#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Introduction"},"type":"lvl2","url":"/lecture-23-ase#introduction","position":4},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Introduction"},"content":"The Atomic Simulation Environment (ASE) is a powerful Python library for setting up, manipulating, and analyzing atomistic simulations. ASE provides tools to create and visualize molecular structures, perform geometry optimizations, calculate energies and forces, and run molecular dynamics simulations. It serves as an interface to various computational chemistry codes and can be extended with custom calculators, making it a versatile tool for computational materials science and chemistry.\n\nIn this lecture, we’ll explore how to use ASE for common tasks in computational chemistry, such as creating molecules, optimizing structures, and simulating adsorption on surfaces. We’ll also see how ASE integrates with machine learning calculators like MACE to accelerate simulations.","type":"content","url":"/lecture-23-ase#introduction","position":5},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Installing ASE"},"type":"lvl2","url":"/lecture-23-ase#installing-ase","position":6},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Installing ASE"},"content":"ASE can be installed using pip:pip install ase\n\nAlternatively, if you’re using Anaconda, you can install it via conda:conda install -c conda-forge ase","type":"content","url":"/lecture-23-ase#installing-ase","position":7},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Creating a Molecule"},"type":"lvl2","url":"/lecture-23-ase#creating-a-molecule","position":8},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Creating a Molecule"},"content":"Let’s create a simple molecule using ASE. We’ll start by creating a carbon monoxide (CO) molecule.\n\nfrom ase import Atoms\n\n# Create a CO molecule with specified positions\natoms = Atoms('CO', positions=[(0, 0, 0), (1.2, 0, 0)])\n\n# Print the molecule's information\nprint(atoms)\n\n","type":"content","url":"/lecture-23-ase#creating-a-molecule","position":9},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Visualizing a Molecule"},"type":"lvl2","url":"/lecture-23-ase#visualizing-a-molecule","position":10},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Visualizing a Molecule"},"content":"ASE can visualize molecules using Matplotlib. Let’s visualize the CO molecule we created.\n\nimport matplotlib.pyplot as plt\nfrom ase.visualize.plot import plot_atoms\n\n# Plot the molecule\nfig, ax = plt.subplots(figsize=(6, 6))\nplot_atoms(atoms, ax, radii=0.5)\nax.set_title('CO Molecule')\nplt.show()\n\n","type":"content","url":"/lecture-23-ase#visualizing-a-molecule","position":11},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Writing a Molecule to a File"},"type":"lvl2","url":"/lecture-23-ase#writing-a-molecule-to-a-file","position":12},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Writing a Molecule to a File"},"content":"We can write the molecule to a file in various formats. Here, we’ll write it to an XYZ file.\n\nfrom ase.io import write\n\n# Write the molecule to an XYZ file\nwrite('CO.xyz', atoms)\n\nCO.xyz2\nProperties=species:S:1:pos:R:3\nC        0.00000000       0.00000000       0.00000000\nO        1.20000000       0.00000000       0.00000000","type":"content","url":"/lecture-23-ase#writing-a-molecule-to-a-file","position":13},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Reading a Molecule from a File"},"type":"lvl2","url":"/lecture-23-ase#reading-a-molecule-from-a-file","position":14},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Reading a Molecule from a File"},"content":"We can read the molecule back from the file we just created.\n\nfrom ase.io import read\n\n# Read the molecule from the XYZ file\natoms = read('CO.xyz')\n\n# Print the molecule's information\nprint(atoms)\n\n","type":"content","url":"/lecture-23-ase#reading-a-molecule-from-a-file","position":15},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Using a Machine Learning Calculator: MACE"},"type":"lvl2","url":"/lecture-23-ase#using-a-machine-learning-calculator-mace","position":16},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Using a Machine Learning Calculator: MACE"},"content":"MACE is a higher-order equivariant message-passing neural network for fast and accurate force fields. We’ll use MACE as a calculator in ASE.\n\nFirst, install MACE:pip install mace-torch\n\nfrom mace.calculators import mace_mp\n\n# Set up the MACE calculator\nmacemp = mace_mp()\n\n# Attach the calculator to the molecule\natoms.calc = macemp\n\n","type":"content","url":"/lecture-23-ase#using-a-machine-learning-calculator-mace","position":17},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Geometry Optimization"},"type":"lvl2","url":"/lecture-23-ase#geometry-optimization","position":18},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Geometry Optimization"},"content":"We can optimize the geometry of the CO molecule using the BFGS algorithm.\n\nfrom ase.optimize import BFGS\n\n# Optimize the molecule\nopt = BFGS(atoms)\nopt.run(fmax=0.05)\n\n# Print the optimized bond length\nbond_length = atoms.get_distance(0, 1)\nprint(f\"Optimized C–O bond length: {bond_length:.3f} Å\")\n\nThe optimized bond length should be close to the experimental value of approximately 1.128 Å.","type":"content","url":"/lecture-23-ase#geometry-optimization","position":19},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Calculating the Atomization Energy"},"type":"lvl2","url":"/lecture-23-ase#calculating-the-atomization-energy","position":20},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Calculating the Atomization Energy"},"content":"We can calculate the atomization energy of CO by comparing the total energy of the molecule to the energies of isolated atoms.\n\n# Create isolated atoms\nC = Atoms('C', positions=[(0, 0, 0)])\nO = Atoms('O', positions=[(0, 0, 0)])\n\n# Attach the calculator to the atoms\nC.calc = macemp\nO.calc = macemp\n\n# Calculate the energies\nE_CO = atoms.get_potential_energy()\nE_C = C.get_potential_energy()\nE_O = O.get_potential_energy()\n\n# Print the energies\nprint(f\"E_CO: {E_CO:.2f} eV\")\nprint(f\"E_C: {E_C:.2f} eV\")\nprint(f\"E_O: {E_O:.2f} eV\")\n\n# Calculate the atomization energy\natomization_energy = E_C + E_O - E_CO\n\nprint(f\"Atomization Energy of CO: {atomization_energy:.2f} eV\")\n\nThe atomization energy should be close to the experimental value of approximately 11.16 eV.","type":"content","url":"/lecture-23-ase#calculating-the-atomization-energy","position":21},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Example: CO Adsorption on Pt(100)"},"type":"lvl2","url":"/lecture-23-ase#example-co-adsorption-on-pt-100","position":22},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Example: CO Adsorption on Pt(100)"},"content":"Let’s simulate the adsorption of CO on a platinum (Pt) (100) surface using ASE.","type":"content","url":"/lecture-23-ase#example-co-adsorption-on-pt-100","position":23},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl3":"Creating the Pt(100) Surface","lvl2":"Example: CO Adsorption on Pt(100)"},"type":"lvl3","url":"/lecture-23-ase#creating-the-pt-100-surface","position":24},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl3":"Creating the Pt(100) Surface","lvl2":"Example: CO Adsorption on Pt(100)"},"content":"\n\nfrom ase.build import fcc100\n\n# Create the Pt(100) surface with specified size and vacuum\nslab = fcc100('Pt', size=(2, 2, 3), vacuum=10.0)\n\n# Visualize the Pt(100) surface\nfig, axs = plt.subplot_mosaic([['side', 'top']], figsize=(12, 6))\nplot_atoms(slab, axs['side'], radii=0.5, rotation='90x,90y')\nplot_atoms(slab, axs['top'], radii=0.5)\naxs['side'].set_title('Side View')\naxs['top'].set_title('Top View')\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/lecture-23-ase#creating-the-pt-100-surface","position":25},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl3":"Adding CO Adsorbate","lvl2":"Example: CO Adsorption on Pt(100)"},"type":"lvl3","url":"/lecture-23-ase#adding-co-adsorbate","position":26},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl3":"Adding CO Adsorbate","lvl2":"Example: CO Adsorption on Pt(100)"},"content":"\n\nfrom ase.build import molecule\nfrom ase.build.surface import add_adsorbate\n\n# Create the CO molecule\nco_molecule = molecule('CO')\n\n# Adjust the position of CO\nco_molecule.set_distance(0, 1, 1.14)\n\n# Add the CO molecule to the Pt(100) surface\nadd_adsorbate(slab, co_molecule, height=3, position=(3, 3))\n\n# Visualize the slab with CO adsorbed\nfig, axs = plt.subplot_mosaic([['side', 'top']], figsize=(12, 6))\nplot_atoms(slab, axs['side'], radii=0.5, rotation='-90x')\nplot_atoms(slab, axs['top'], radii=0.5)\naxs['side'].set_title('Side View with CO Adsorbed')\naxs['top'].set_title('Top View with CO Adsorbed')\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/lecture-23-ase#adding-co-adsorbate","position":27},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl3":"Optimization of the Adsorbed System","lvl2":"Example: CO Adsorption on Pt(100)"},"type":"lvl3","url":"/lecture-23-ase#optimization-of-the-adsorbed-system","position":28},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl3":"Optimization of the Adsorbed System","lvl2":"Example: CO Adsorption on Pt(100)"},"content":"\n\n# Attach the calculator to the slab\nslab.calc = macemp\n\n# Optimize the slab with CO adsorbed\nopt = BFGS(slab, logfile='Pt100_CO.log')\nopt.run(fmax=0.05)\n\n# Visualize the optimized structure\nfig, axs = plt.subplot_mosaic([['side', 'top']], figsize=(12, 6))\nplot_atoms(slab, axs['side'], radii=0.5, rotation='-90x')\nplot_atoms(slab, axs['top'], radii=0.5)\naxs['side'].set_title('Optimized Side View')\naxs['top'].set_title('Optimized Top View')\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/lecture-23-ase#optimization-of-the-adsorbed-system","position":29},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Calculating the Adsorption Energy"},"type":"lvl2","url":"/lecture-23-ase#calculating-the-adsorption-energy","position":30},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Calculating the Adsorption Energy"},"content":"The adsorption energy can be calculated using the energies of the slab with and without CO, and the energy of the isolated CO molecule.\n\n# Energy of the slab with CO adsorbed\nE_slab_CO = slab.get_potential_energy()\n\n# Create and calculate energy of the clean slab\nslab_clean = fcc100('Pt', size=(2, 2, 3), vacuum=10.0)\nslab_clean.calc = macemp\n\n# Optimize the clean slab\nopt_clean = BFGS(slab_clean)\nopt_clean.run(fmax=0.05)\n\nE_slab = slab_clean.get_potential_energy()\n\n# Recalculate E_CO if needed\nE_CO = atoms.get_potential_energy()\n\n# Calculate the adsorption energy\nadsorption_energy = E_slab_CO - E_slab - E_CO\n\nprint(f\"Adsorption Energy: {adsorption_energy:.2f} eV\")\n\nThe adsorption energy should be negative, indicating that adsorption is energetically favorable. The value should be in the range of approximately -1.73 eV to -1.64 eV, consistent with \n\ncomputational data.","type":"content","url":"/lecture-23-ase#calculating-the-adsorption-energy","position":31},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Example: Molecular Dynamics of CO on Pt(100)"},"type":"lvl2","url":"/lecture-23-ase#example-molecular-dynamics-of-co-on-pt-100","position":32},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Example: Molecular Dynamics of CO on Pt(100)"},"content":"We can perform molecular dynamics (MD) simulations to study the behavior of CO on the Pt(100) surface at finite temperatures.","type":"content","url":"/lecture-23-ase#example-molecular-dynamics-of-co-on-pt-100","position":33},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl3":"Setting Up Molecular Dynamics","lvl2":"Example: Molecular Dynamics of CO on Pt(100)"},"type":"lvl3","url":"/lecture-23-ase#setting-up-molecular-dynamics","position":34},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl3":"Setting Up Molecular Dynamics","lvl2":"Example: Molecular Dynamics of CO on Pt(100)"},"content":"\n\nfrom ase import units\nfrom ase.md.andersen import Andersen\nfrom ase.md.velocitydistribution import MaxwellBoltzmannDistribution\nimport matplotlib.pyplot as plt\n\n# Set the temperature and time step\ntemperature = 300  # Kelvin\ntimestep = 1.0  # fs\n\n# Initialize velocities according to the Maxwell-Boltzmann distribution\nMaxwellBoltzmannDistribution(slab, temperature_K=temperature)\n\n# Set up the Andersen dynamics\ndyn = Andersen(slab, timestep * units.fs, temperature_K=temperature, andersen_prob=0.1)\n\n# Lists to store energies\nkinetic_energies = []\npotential_energies = []\ntotal_energies = []\n\n# Function to store energies\ndef store_energies():\n    kinetic_energy = slab.get_kinetic_energy()\n    potential_energy = slab.get_potential_energy()\n    total_energy = kinetic_energy + potential_energy\n    kinetic_energies.append(kinetic_energy)\n    potential_energies.append(potential_energy)\n    total_energies.append(total_energy)\n\n# Attach the function to the dynamics\ndyn.attach(store_energies, interval=1)\n\n# Run the MD simulation for 100 steps\ndyn.run(200)\n\n# Plot the energy during the simulation\nfig, axs = plt.subplots(1, 3, figsize=(18, 6))\n\naxs[0].set_title('Kinetic Energy')\naxs[0].plot(kinetic_energies)\naxs[0].set_xlabel('Time Step')\naxs[0].set_ylabel('Energy (eV)')\n\naxs[1].set_title('Potential Energy')\naxs[1].plot(potential_energies)\naxs[1].set_xlabel('Time Step')\naxs[1].set_ylabel('Energy (eV)')\n\naxs[2].set_title('Total Energy')\naxs[2].plot(total_energies)\naxs[2].set_xlabel('Time Step')\naxs[2].set_ylabel('Energy (eV)')\n\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/lecture-23-ase#setting-up-molecular-dynamics","position":35},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl3":"Visualizing the MD Simulation","lvl2":"Example: Molecular Dynamics of CO on Pt(100)"},"type":"lvl3","url":"/lecture-23-ase#visualizing-the-md-simulation","position":36},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl3":"Visualizing the MD Simulation","lvl2":"Example: Molecular Dynamics of CO on Pt(100)"},"content":"After the simulation, we can visualize the final configuration.\n\n# Visualize the slab after MD simulation\nfig, axs = plt.subplot_mosaic([['side', 'top']], figsize=(12, 6))\nplot_atoms(slab, axs['side'], radii=0.5, rotation='-90x')\nplot_atoms(slab, axs['top'], radii=0.5)\naxs['side'].set_title('Post-MD Side View')\naxs['top'].set_title('Post-MD Top View')\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/lecture-23-ase#visualizing-the-md-simulation","position":37},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Summary"},"type":"lvl2","url":"/lecture-23-ase#summary","position":38},{"hierarchy":{"lvl1":"Chapter 22: Atomic Simulation Environment (ASE)","lvl2":"Summary"},"content":"In this lecture, we explored the Atomic Simulation Environment (ASE) and its capabilities for molecular modeling and simulations. We learned how to:\n\nInstall and set up ASE for simulations.\n\nCreate and visualize molecular structures.\n\nWrite and read molecular data to and from files.\n\nUse machine learning calculators like MACE for efficient computations.\n\nPerform geometry optimizations and calculate energies, such as atomization and adsorption energies.\n\nModel surface phenomena like CO adsorption on Pt(100).\n\nConduct molecular dynamics simulations to study temperature-dependent behavior.\n\nASE provides a flexible and powerful framework for computational studies in chemistry and materials science, allowing researchers to perform a wide range of simulations with ease.","type":"content","url":"/lecture-23-ase#summary","position":39},{"hierarchy":{"lvl1":"Chapter 23: Radial Distribution Function"},"type":"lvl1","url":"/lecture-24-radial-dist","position":0},{"hierarchy":{"lvl1":"Chapter 23: Radial Distribution Function"},"content":"","type":"content","url":"/lecture-24-radial-dist","position":1},{"hierarchy":{"lvl1":"Chapter 23: Radial Distribution Function","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-24-radial-dist#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 23: Radial Distribution Function","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you will be able to:\n\nDefine the radial distribution function.\n\nCompute the radial distribution function for a given configuration of particles.\n\nUnderstand the physical significance of the radial distribution function.","type":"content","url":"/lecture-24-radial-dist#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 23: Radial Distribution Function","lvl2":"Radial Distribution Function"},"type":"lvl2","url":"/lecture-24-radial-dist#radial-distribution-function","position":4},{"hierarchy":{"lvl1":"Chapter 23: Radial Distribution Function","lvl2":"Radial Distribution Function"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\ncentral_particle = (0, 0)\nnum_particles = 100\nbox_size = 10\nr = 3.0  # Shell radius\ndr = 0.5  # Shell thickness\n\n# Generate random particle positions\nnp.random.seed(42)  # For reproducibility\nparticle_positions = np.random.uniform(-box_size/2, box_size/2, size=(num_particles, 2))\n\n# Plot setup\nfig, ax = plt.subplots(figsize=(8, 8))\nax.set_xlim(-box_size/2, box_size/2)\nax.set_ylim(-box_size/2, box_size/2)\nax.set_aspect('equal', adjustable='datalim')\n\n# Draw central particle\nax.plot(*central_particle, 'o', color='red', label='Central Particle')\n\n# Draw surrounding particles\nfor pos in particle_positions:\n    ax.plot(*pos, 'o', color='blue', markersize=5, alpha=0.7)\n\n# Draw the shell\ncircle_inner = plt.Circle(central_particle, r, color='green', fill=False, linestyle='--', label=f'$r = {r}$')\ncircle_outer = plt.Circle(central_particle, r + dr, color='orange', fill=False, linestyle='--', label=f'$r + \\Delta r = {r + dr}$')\nax.add_artist(circle_inner)\nax.add_artist(circle_outer)\n\n# Annotate particles within the shell\nfor pos in particle_positions:\n    distance = np.linalg.norm(np.array(pos) - np.array(central_particle))\n    if r <= distance < r + dr:\n        ax.plot(*pos, 'o', color='purple', markersize=7)\n\n# Add labels and legend\nax.set_title('Radial Distribution Function Demonstration')\nax.set_xlabel('$x$')\nax.set_ylabel('$y$')\nax.legend()\n\n# Show plot\nplt.show()\n\nThe radial distribution function, denoted by g(r), is a measure of the “structure” of a fluid or solid. It quantifies the average number of particles at a distance r from a central particle relative to the ideal gas case. The radial distribution function is defined asg(r) = \\frac{\\langle N(r) \\rangle}{4 \\pi r^2 \\Delta r \\rho}\n\nwhere \\langle N(r) \\rangle is the average number of particles in a shell of radius r and thickness \\Delta r around a central particle, \\rho is the number density of particles, and r is the distance from the central particle. The radial distribution function provides information about the local structure of a system, such as the presence of short-range order, long-range order, or disorder.","type":"content","url":"/lecture-24-radial-dist#radial-distribution-function","position":5},{"hierarchy":{"lvl1":"Chapter 23: Radial Distribution Function","lvl2":"Computing the Radial Distribution Function"},"type":"lvl2","url":"/lecture-24-radial-dist#computing-the-radial-distribution-function","position":6},{"hierarchy":{"lvl1":"Chapter 23: Radial Distribution Function","lvl2":"Computing the Radial Distribution Function"},"content":"To compute the radial distribution function for a given configuration of particles, we need to follow these steps:\n\nCompute the distance between all pairs of particles in the system.\n\nBin the distances into radial bins.\n\nCompute the radial distribution function using the formula given above.\n\nLet’s consider an example to illustrate how to compute the radial distribution function for a simple system of particles.","type":"content","url":"/lecture-24-radial-dist#computing-the-radial-distribution-function","position":7},{"hierarchy":{"lvl1":"Chapter 23: Radial Distribution Function","lvl2":"Example: Lennard-Jones Fluid"},"type":"lvl2","url":"/lecture-24-radial-dist#example-lennard-jones-fluid","position":8},{"hierarchy":{"lvl1":"Chapter 23: Radial Distribution Function","lvl2":"Example: Lennard-Jones Fluid"},"content":"\n\nConsider a three-dimensional Lennard-Jones fluid (\\epsilon = 1, \\sigma = 1, r_c = 2.5) with periodic boundary conditions and N = 1500 particles in a cubic box of side length L = 20. The final configuration of the particles, after a molecular dynamics simulation at a temperature of T = 0.5, is given in the file \n\nlj.xyz. We want to compute the radial distribution function for this configuration of particles.\n\nLet’s start by loading the configuration of particles from the file lj.xyz.\n\n# Load the configuration of particles from the file lj.xyz\nimport numpy as np\n\ndef read_xyz(filename):\n    with open(filename, 'r') as f:\n        lines = f.readlines()\n        n_atoms = int(lines[0])\n        data = np.zeros((n_atoms, 3))\n        for i in range(2, n_atoms + 2):\n            # Skip the first and second columns (id and species) and extract the x, y, z coordinates\n            data[i - 2] = np.array([float(x) for x in lines[i].split()[2:5]])\n    return data\n\n# Load the configuration of particles from the file lj.xyz\nfilename = 'lj.xyz'\npositions = read_xyz(filename)\nn_atoms = len(positions)\nprint(f'Number of particles: {n_atoms}')\n\nNow that we have loaded the configuration of particles, we can compute the distance between all pairs of particles in the system.\n\n# Compute the distance between all pairs of particles in the system\ndef compute_distance(positions, box_length):\n    n_atoms = len(positions)\n    distances = []\n    for i in range(n_atoms):\n        for j in range(n_atoms):\n            if i >= j:\n                continue\n            dr = positions[i] - positions[j]\n            dr = dr - box_length * np.round(dr / box_length)\n            distance = np.linalg.norm(dr)\n            distances.append(distance)\n    return distances\n\n# Compute the distance between all pairs of particles in the system\nbox_length = 20\ndistances = compute_distance(positions, box_length)\nprint(f'Number of distances: {len(distances)}')\n\n# Compute the statistics of the distances\nprint(f'Minimum distance: {min(distances)}')\nprint(f'Maximum distance: {max(distances)}')\nprint(f'Mean distance: {np.mean(distances)}')\nprint(f'Standard deviation of distance: {np.std(distances)}')\n\nNext, we need to bin the distances into radial bins. We will use a bin width of 0.2 and a maximum distance of 10 for this example.\n\n# Bin the distances into radial bins\ndef bin_distances(distances, bin_width, max_distance):\n    bins = np.arange(bin_width, max_distance + bin_width * 2, bin_width)\n    hist, _ = np.histogram(distances, bins=bins)\n    return hist\n\n# Bin the distances into radial bins\nbin_width = 0.01\nmax_distance = 3.5\nhist = bin_distances(distances, bin_width, max_distance)\nprint(f'Number of bins: {len(hist)}')\n\nFinally, we can compute the radial distribution function using the formula given above.\n\ndef radial_distribution_function(hist, n_atoms, box_length, bin_width):\n    rho = n_atoms / box_length**3\n    r = (np.arange(1, len(hist) + 1) - 0.5) * bin_width\n    shell_volumes = 4 * np.pi * r**2 * bin_width\n    g = hist / (rho * n_atoms * shell_volumes)\n    return r, g\n\n# Compute the radial distribution function\nr, g = radial_distribution_function(hist, n_atoms, box_length, bin_width)\n\nNow that we have computed the radial distribution function, we can plot it to visualize the structure of the fluid.\n\nimport matplotlib.pyplot as plt\n\n# Plot the radial distribution function\nplt.plot(r, g)\nplt.xlabel('r')\nplt.ylabel('g(r)')\nplt.title('Radial Distribution Function')\n\n# Annotate the first peak\nfirst_peak_label = 'First Nearest Neighbor Peak'\nplt.annotate(\n    first_peak_label,\n    xy=(1.2, 3),\n    xytext=(1.5, 4),\n    arrowprops=dict(facecolor='black', shrink=0.05))\n\n# Annotate the second peak\nsecond_peak_label = 'Second Nearest Neighbor Peak'\nplt.annotate(\n    second_peak_label,\n    xy=(2.2, 1.5),\n    xytext=(2.5, 2),\n    arrowprops=dict(facecolor='black', shrink=0.05))\n\n# Annotate the third peak\nthird_peak_label = 'Third Nearest Neighbor Peak'\nplt.annotate(\n    third_peak_label,\n    xy=(3.2, 1),\n    xytext=(3.5, 1.5),\n    arrowprops=dict(facecolor='black', shrink=0.05))\n\n# Horizontal line at g(r) = 1\nplt.axhline(y=1, color='r', linestyle='--')\nplt.text(0, 1.1, 'Ideal Gas', color='red')\n\nplt.show()\n\n","type":"content","url":"/lecture-24-radial-dist#example-lennard-jones-fluid","position":9},{"hierarchy":{"lvl1":"Chapter 23: Radial Distribution Function","lvl2":"Interpretation of the Radial Distribution Function"},"type":"lvl2","url":"/lecture-24-radial-dist#interpretation-of-the-radial-distribution-function","position":10},{"hierarchy":{"lvl1":"Chapter 23: Radial Distribution Function","lvl2":"Interpretation of the Radial Distribution Function"},"content":"The radial distribution function provides information about the local structure of a system. Here are some key points to keep in mind when interpreting the radial distribution function:\n\ng(r) = 1 for an ideal gas, indicating no correlation between particles at different distances.\n\ng(r) > 1 for attractive interactions, indicating clustering of particles at certain distances.\n\ng(r) < 1 for repulsive interactions, indicating exclusion of particles at certain distances.\n\nPeaks in g(r) correspond to the average number of particles at specific distances from a central particle.\n\nThe first peak in g(r) corresponds to the first nearest neighbor distance, the second peak to the second nearest neighbor distance, and so on.\n\nThe height and width of the peaks in g(r) provide information about the strength and range of interactions between particles. Higher, narrower peaks indicate strong, short-range interactions, while broader peaks indicate weaker, longer-range interactions.","type":"content","url":"/lecture-24-radial-dist#interpretation-of-the-radial-distribution-function","position":11},{"hierarchy":{"lvl1":"Chapter 23: Radial Distribution Function","lvl2":"Summary"},"type":"lvl2","url":"/lecture-24-radial-dist#summary","position":12},{"hierarchy":{"lvl1":"Chapter 23: Radial Distribution Function","lvl2":"Summary"},"content":"In this lecture, we introduced the radial distribution function as a measure of the local structure of a fluid or solid. We discussed how to compute the radial distribution function for a given configuration of particles and interpret the results. The radial distribution function provides valuable insights into the interactions between particles in a system and will helps us understand the structure and properties of bead-spring polymers in the next lecture and project.","type":"content","url":"/lecture-24-radial-dist#summary","position":13},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain"},"type":"lvl1","url":"/lecture-25-project-2","position":0},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain"},"content":"","type":"content","url":"/lecture-25-project-2","position":1},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Introduction"},"type":"lvl2","url":"/lecture-25-project-2#introduction","position":2},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Introduction"},"content":"Polymer chains exhibit fascinating behaviors, including temperature-driven phase transitions between folded (globular) and unfolded (extended) states. Understanding these transitions is crucial in fields like biophysics, material science, and nanotechnology. MD simulations provide a powerful tool to study such phenomena at the molecular level.\n\nIn this project, you will write a Python program that performs MD simulations of a polymer chain at constant temperature and volume. You will model the polymer as a chain of beads connected by springs and subject to various interaction potentials. By varying parameters such as temperature and interaction strengths, you will simulate and observe the phase transition between the folded and unfolded states of the polymer.","type":"content","url":"/lecture-25-project-2#introduction","position":3},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Polymer Chain Model"},"type":"lvl2","url":"/lecture-25-project-2#polymer-chain-model","position":4},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Polymer Chain Model"},"content":"","type":"content","url":"/lecture-25-project-2#polymer-chain-model","position":5},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Bead-Spring Model","lvl2":"Polymer Chain Model"},"type":"lvl3","url":"/lecture-25-project-2#bead-spring-model","position":6},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Bead-Spring Model","lvl2":"Polymer Chain Model"},"content":"In the bead-spring model, a polymer chain is represented as a series of beads or monomers (red spheres in the figure below) connected by harmonic springs (gray lines). This simple yet effective model captures the essential features of polymer dynamics.\n\n\n\nFigure 1:Schematic of a polymer chain modeled as beads or monomers (red spheres) connected by harmonic springs (gray lines).","type":"content","url":"/lecture-25-project-2#bead-spring-model","position":7},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Interactions in the Polymer Chain","lvl2":"Polymer Chain Model"},"type":"lvl3","url":"/lecture-25-project-2#interactions-in-the-polymer-chain","position":8},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Interactions in the Polymer Chain","lvl2":"Polymer Chain Model"},"content":"The interactions in the polymer chain can be categorized as:\n\nBonded Interactions: Harmonic potentials between adjacent beads.\n\nNon-Bonded Interactions:\n\nExcluded Volume Effects: Repulsive Lennard-Jones (LJ) potential between beads separated by one spacer.\n\nAttractive Interactions: LJ potential between beads separated by more than one spacer.","type":"content","url":"/lecture-25-project-2#interactions-in-the-polymer-chain","position":9},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Potential Energy Components","lvl2":"Polymer Chain Model"},"type":"lvl3","url":"/lecture-25-project-2#potential-energy-components","position":10},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Potential Energy Components","lvl2":"Polymer Chain Model"},"content":"","type":"content","url":"/lecture-25-project-2#potential-energy-components","position":11},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Harmonic Bond Potential and Forces","lvl3":"Potential Energy Components","lvl2":"Polymer Chain Model"},"type":"lvl4","url":"/lecture-25-project-2#harmonic-bond-potential-and-forces","position":12},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Harmonic Bond Potential and Forces","lvl3":"Potential Energy Components","lvl2":"Polymer Chain Model"},"content":"The harmonic bond potential between adjacent beads is given byU_{\\text{bond}}(r) = \\frac{1}{2} k (r - r_0)^2\n\nwhere k is the spring constant, r is the distance between two adjacent beads, and r_0 is the equilibrium bond length.\n\nThe force due to the harmonic bond potential is\\mathbf{F}_{\\text{bond}} = -\\frac{dU_{\\text{bond}}}{dr} = -k (r - r_0) \\hat{\\mathbf{r}}\n\nwhere \\hat{\\mathbf{r}} is the unit vector along the bond direction.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Harmonic bond potential\ndef harmonic_bond_potential(r, k, r0):\n    return 0.5 * k * (r - r0)**2\n\n# Harmonic bond force\ndef harmonic_bond_force(r, k, r0):\n    return -k * (r - r0)\n\n# Parameters\nk = 1.0\nr0 = 1.0\nr_values = np.linspace(0.5, 1.5, 100)\nU_values = harmonic_bond_potential(r_values, k, r0)\nF_values = harmonic_bond_force(r_values, k, r0)\n\n# Plot\nfig, axs = plt.subplots(1, 2)\naxs[0].plot(r_values, U_values)\naxs[0].axvline(x=r0, color='r', linestyle='--', label='Equilibrium Length')\naxs[0].set_xlabel('$r$')\naxs[0].set_ylabel('$U_{\\text{bond}}(r)$')\naxs[0].set_title('Harmonic Bond Potential')\naxs[0].legend()\naxs[1].plot(r_values, F_values)\naxs[1].axvline(x=r0, color='r', linestyle='--', label='Equilibrium Length')\naxs[1].set_xlabel('$r$')\naxs[1].set_ylabel('$F_{\\text{bond}}(r)$')\naxs[1].set_title('Harmonic Bond Force')\naxs[1].legend()\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/lecture-25-project-2#harmonic-bond-potential-and-forces","position":13},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Lennard-Jones Potential","lvl3":"Potential Energy Components","lvl2":"Polymer Chain Model"},"type":"lvl4","url":"/lecture-25-project-2#lennard-jones-potential","position":14},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Lennard-Jones Potential","lvl3":"Potential Energy Components","lvl2":"Polymer Chain Model"},"content":"The repulsive LJ potential models excluded volume effects between beads separated by one spacer (i, i+2)U_{\\text{LJ,rep}}(r) =\n\\begin{cases}\n4\\epsilon_{\\text{rep}} \\left[ \\left( \\frac{\\sigma}{r} \\right)^{12} - \\left( \\frac{\\sigma}{r} \\right)^6 + \\frac{1}{4} \\right], & r < 2^{1/6} \\sigma, \\\\\n0, & r \\geq 2^{1/6} \\sigma.\n\\end{cases}\n\nThe attractive LJ potential models interactions between beads separated by more than one spacer (|i - j| > 2)U_{\\text{LJ,att}}(r) = 4\\epsilon_{\\text{att}} \\left[ \\left( \\frac{\\sigma}{r} \\right)^{12} - \\left( \\frac{\\sigma}{r} \\right)^6 \\right]\n\nwhere \\epsilon_{\\text{rep}} and \\epsilon_{\\text{att}} are the depth of the repulsive and attractive potentials, respectively, and \\sigma is the LJ potential parameter.\n\n# Lennard-Jones potentials\ndef lj_repulsive_potential(r, epsilon, sigma):\n    potential = np.zeros_like(r)\n    mask = r < 2**(1/6) * sigma\n    potential[mask] = 4 * epsilon * ((sigma / r[mask])**12 - (sigma / r[mask])**6 + 0.25)\n    return potential\n\ndef lj_attractive_potential(r, epsilon, sigma):\n    return 4 * epsilon * ((sigma / r)**12 - (sigma / r)**6)\n\n# Parameters\nepsilon_repulsive = 1.0\nepsilon_attractive = 0.5\nsigma = 1.0\nr_values = np.linspace(0.9, 2.0, 100)\nU_repulsive_values = lj_repulsive_potential(r_values, epsilon_repulsive, sigma)\nU_attractive_values = lj_attractive_potential(r_values, epsilon_attractive, sigma)\n\n# Plot\nplt.figure()\nplt.plot(r_values, U_repulsive_values, label='Repulsive LJ Potential ($\\epsilon_{\\\\text{rep}} = 1.0$)')\nplt.plot(r_values, U_attractive_values, label='Attractive LJ Potential ($\\epsilon_{\\\\text{att}} = 0.5$)')\nplt.axvline(x=2**(1/6) * sigma, color='r', linestyle='--', label='Cutoff Distance ($2^{1/6} \\sigma$)')\nplt.xlabel('$r$')\nplt.ylabel('$U(r)$')\nplt.title('Lennard-Jones Potentials')\nplt.legend()\nplt.show()\n\n","type":"content","url":"/lecture-25-project-2#lennard-jones-potential","position":15},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Total Potential Energy","lvl2":"Polymer Chain Model"},"type":"lvl3","url":"/lecture-25-project-2#total-potential-energy","position":16},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Total Potential Energy","lvl2":"Polymer Chain Model"},"content":"The total potential energy of the system is:U_{\\text{total}} = \\sum_{\\text{bonds}} U_{\\text{bond}}(r_{i,i+1}) + \\sum_{\\substack{i,j \\\\ |i-j|=2}} U_{\\text{LJ,rep}}(r_{ij}) + \\sum_{\\substack{i,j \\\\ |i-j|>2}} U_{\\text{LJ,att}}(r_{ij})\n\nwhere r_{ij} is the distance between beads i and j and U_{\\text{LJ,rep}} and U_{\\text{LJ,att}} are the repulsive and attractive LJ potentials, respectively.\n\nplt.figure()\n\n# Plot beads as red circles\nplt.plot([0, 1, 2, 3, 4], [0, 0, 0, 0, 0], 'ro', markersize=10, label='Beads')\n\n# Plot harmonic bonds as gray lines\nplt.plot([0, 1], [0, 0], 'C7-', label='Harmonic Bond', zorder=0)\nplt.plot([1, 2], [0, 0], 'C7-', zorder=0)\nplt.plot([2, 3], [0, 0], 'C7-', zorder=0)\nplt.plot([3, 4], [0, 0], 'C7-', zorder=0)\n\n# Plot repulsive LJ interactions as blue curves\nx02 = np.linspace(0, 2, 100)\nx13 = np.linspace(1, 3, 100)\nx24 = np.linspace(2, 4, 100)\ncurve = np.sin(x02 * np.pi / 2)\nplt.plot(x02, curve, 'b-', label='Repulsive LJ', zorder=1)\nplt.plot(x13, curve, 'b-', zorder=1)\nplt.plot(x24, curve, 'b-', zorder=1)\n\n# Plot attractive LJ interactions as green curves\nx03 = np.linspace(0, 3, 100)\nx14 = np.linspace(1, 4, 100)\ncurve = -np.sin(x03 * np.pi / 3)\nplt.plot(x03, curve, 'g-', label='Attractive LJ', zorder=1)\nplt.plot(x14, curve, 'g-', zorder=1)\n\nplt.axis('off')\nplt.legend()\nplt.show()\n\n","type":"content","url":"/lecture-25-project-2#total-potential-energy","position":17},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Temperature Control"},"type":"lvl2","url":"/lecture-25-project-2#temperature-control","position":18},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Temperature Control"},"content":"To simulate at constant temperature, a simple velocity rescaling thermostat is used.\n\nCompute instantaneous temperature:T_{\\text{inst}} = \\frac{2 K}{3 N k_B}\n\nwhere K is the total kinetic energy, N is the number of particles, and k_B is the Boltzmann constant.\n\nRescale velocities:\\mathbf{v}_i \\leftarrow \\mathbf{v}_i \\sqrt{\\frac{T_{\\text{target}}}{T_{\\text{inst}}}}\n\nWarning\n\nThe velocity rescaling thermostat does not provide true canonical ensemble sampling. More advanced thermostats like Nosé-Hoover dynamics are used for more accurate temperature control.","type":"content","url":"/lecture-25-project-2#temperature-control","position":19},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"type":"lvl2","url":"/lecture-25-project-2#pseudocode-for-md-simulation-of-polymer-chain","position":20},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"content":"","type":"content","url":"/lecture-25-project-2#pseudocode-for-md-simulation-of-polymer-chain","position":21},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Initialize Positions and Velocities","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"type":"lvl3","url":"/lecture-25-project-2#initialize-positions-and-velocities","position":22},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Initialize Positions and Velocities","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"content":"FUNCTION initialize_chain(n_particles, box_size, r0):\n    positions = array of zeros with shape (n_particles, 3)\n    current_position = [box_size/2, box_size/2, box_size/2]\n    positions[0] = current_position\n    FOR i FROM 1 TO n_particles - 1:\n        direction = random unit vector\n        next_position = current_position + r0 * direction\n        positions[i] = apply_pbc(next_position, box_size)\n        current_position = positions[i]\n    RETURN positions\n\nFUNCTION initialize_velocities(n_particles, target_temperature, mass):\n    velocities = random velocities from Maxwell-Boltzmann distribution\n    velocities -= mean(velocities)  # Remove net momentum\n    RETURN velocities\n\nThe initialize_chain function works like placing segments of a “Snake” in the classic video game (see image below), starting from the center of the screen (simulation box) and extending outward. Each segment of the snake is added by moving a fixed distance (r0, like the snake’s body length) in a random direction from the previous segment. If the snake crosses the screen boundaries, it wraps around to the opposite side, mimicking periodic boundary conditions. This ensures the snake stays within the screen while maintaining its shape and continuity. The result is a randomly oriented snake with evenly spaced segments, ready for dynamic movement in the simulation.\n\n\n\nFigure 2:Classic Snake Game","type":"content","url":"/lecture-25-project-2#initialize-positions-and-velocities","position":23},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Apply Periodic Boundary Conditions","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"type":"lvl3","url":"/lecture-25-project-2#apply-periodic-boundary-conditions","position":24},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Apply Periodic Boundary Conditions","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"content":"FUNCTION apply_pbc(position, box_size):\n    RETURN position modulo box_size","type":"content","url":"/lecture-25-project-2#apply-periodic-boundary-conditions","position":25},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Compute Forces","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"type":"lvl3","url":"/lecture-25-project-2#compute-forces","position":26},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Compute Forces","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"content":"","type":"content","url":"/lecture-25-project-2#compute-forces","position":27},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Harmonic Forces","lvl3":"Compute Forces","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"type":"lvl4","url":"/lecture-25-project-2#harmonic-forces","position":28},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Harmonic Forces","lvl3":"Compute Forces","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"content":"FUNCTION compute_harmonic_forces(positions, k, r0, box_size):\n    forces = zeros_like(positions)\n    FOR i FROM 0 TO n_particles - 2:\n        displacement = positions[i+1] - positions[i]\n        displacement = minimum_image(displacement, box_size)\n        distance = norm(displacement)\n        force_magnitude = -k * (distance - r0)\n        force = force_magnitude * (displacement / distance)\n        forces[i] -= force\n        forces[i+1] += force\n    RETURN forces","type":"content","url":"/lecture-25-project-2#harmonic-forces","position":29},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Lennard-Jones Forces","lvl3":"Compute Forces","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"type":"lvl4","url":"/lecture-25-project-2#lennard-jones-forces","position":30},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Lennard-Jones Forces","lvl3":"Compute Forces","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"content":"FUNCTION compute_lennard_jones_forces(positions, epsilon, sigma, box_size, interaction_type):\n    forces = zeros_like(positions)\n    FOR i FROM 0 TO n_particles - 1:\n        FOR j FROM i+1 TO n_particles - 1:\n            IF interaction_type == 'repulsive' AND |i - j| == 2:\n                USE epsilon_repulsive\n            ELSE IF interaction_type == 'attractive' AND |i - j| > 2:\n                USE epsilon_attractive\n            ELSE:\n                CONTINUE\n            displacement = positions[j] - positions[i]\n            displacement = minimum_image(displacement, box_size)\n            distance = norm(displacement)\n            IF distance < cutoff:\n                force_magnitude = 24 * epsilon * [ (sigma / distance)^{12} - 0.5 * (sigma / distance)^6 ] / distance\n                force = force_magnitude * (displacement / distance)\n                forces[i] -= force\n                forces[j] += force\n    RETURN forces","type":"content","url":"/lecture-25-project-2#lennard-jones-forces","position":31},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Velocity Verlet Integration","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"type":"lvl3","url":"/lecture-25-project-2#velocity-verlet-integration","position":32},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Velocity Verlet Integration","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"content":"FUNCTION velocity_verlet(positions, velocities, forces, dt, mass):\n    velocities += 0.5 * forces / mass * dt\n    positions += velocities * dt\n    positions = apply_pbc(positions, box_size)\n    forces_new = compute_forces(positions)\n    velocities += 0.5 * forces_new / mass * dt\n    RETURN positions, velocities, forces_new","type":"content","url":"/lecture-25-project-2#velocity-verlet-integration","position":33},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Velocity Rescaling Thermostat","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"type":"lvl3","url":"/lecture-25-project-2#velocity-rescaling-thermostat","position":34},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Velocity Rescaling Thermostat","lvl2":"Pseudocode for MD Simulation of Polymer Chain"},"content":"FUNCTION rescale_velocities(velocities, target_temperature, mass):\n    kinetic_energy = 0.5 * mass * sum(norm(velocities, axis=1)^2)\n    current_temperature = (2/3) * kinetic_energy / (n_particles * k_B)\n    scaling_factor = sqrt(target_temperature / current_temperature)\n    velocities *= scaling_factor\n    RETURN velocities","type":"content","url":"/lecture-25-project-2#velocity-rescaling-thermostat","position":35},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Example Simulation"},"type":"lvl2","url":"/lecture-25-project-2#example-simulation","position":36},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Example Simulation"},"content":"Here is an example Python code snippet that performs an MD simulation of a polymer chain using the bead-spring model with Lennard-Jones interactions.# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulation parameters\ndt = 0.01  # Time step\ntotal_steps = 10000  # Number of steps\nbox_size = 100.0  # Size of the cubic box\nk = 1.0  # Spring constant\nmass = 1.0  # Particle mass\nr0 = 1.0  # Equilibrium bond length\ntarget_temperature = 0.1  # Target temperature\nrescale_interval = 100  # Steps between velocity rescaling\nn_particles = 20  # Number of particles\nepsilon_repulsive = 1.0  # Depth of repulsive LJ potential\nepsilon_attractive = 0.5  # Depth of attractive LJ potential\nsigma = 1.0  # LJ potential parameter\n\n# Initialize positions and velocities\npositions = initialize_chain(n_particles, box_size, r0)\nvelocities = initialize_velocities(n_particles, target_temperature, mass)\n\n# Simulation loop\nfor step in range(total_steps):\n    # Compute forces\n    forces_harmonic = compute_harmonic_forces(positions, k, r0, box_size)\n    forces_repulsive = compute_lennard_jones_forces(positions, epsilon_repulsive, sigma, box_size, 'repulsive')\n    forces_attractive = compute_lennard_jones_forces(positions, epsilon_attractive, sigma, box_size, 'attractive')\n    total_forces = forces_harmonic + forces_repulsive + forces_attractive\n    \n    # Integrate equations of motion\n    positions, velocities, total_forces = velocity_verlet(positions, velocities, total_forces, dt, mass)\n    \n    # Apply thermostat\n    if step % rescale_interval == 0:\n        velocities = rescale_velocities(velocities, target_temperature, mass)\n    \n    # (Optional) Store data for analysis\n    # ...\n\n# Plot results\n# (Plotting code goes here)","type":"content","url":"/lecture-25-project-2#example-simulation","position":37},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Project Description"},"type":"lvl2","url":"/lecture-25-project-2#project-description","position":38},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Project Description"},"content":"","type":"content","url":"/lecture-25-project-2#project-description","position":39},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Scenario","lvl2":"Project Description"},"type":"lvl3","url":"/lecture-25-project-2#scenario","position":40},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Scenario","lvl2":"Project Description"},"content":"You are a chemical scientist at a space technology company dedicated to developing advanced materials for spacecraft. One of the challenges in space is the extreme temperatures, which can cause polymer materials to fold or become brittle, compromising their mechanical properties and reliability. Your team aims to design polymer materials that remain unfolded and maintain their structural integrity at the low temperatures encountered in space environments.\n\nYour task is to simulate a polymer chain using molecular dynamics to understand how temperature affects its conformational behavior, with a focus on preventing folding at low temperatures. You will model the polymer chain as a series of beads connected by harmonic springs and include non-bonded interactions using Lennard-Jones potentials. By performing simulations at various space-relevant temperatures, you will analyze properties such as the radius of gyration, end-to-end distance, and potential energy to assess the linearity and stability of the polymer chain. Your findings will contribute to the design of polymers suitable for use in space technology.\n\nFinally, you will write a report summarizing your findings and discussing the implications for material design in the context of space applications.","type":"content","url":"/lecture-25-project-2#scenario","position":41},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Tasks","lvl2":"Project Description"},"type":"lvl3","url":"/lecture-25-project-2#tasks","position":42},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Tasks","lvl2":"Project Description"},"content":"Implement an MD simulation of a polymer chain using the bead-spring model.\n\nInclude harmonic bond potentials and Lennard-Jones non-bonded interactions.\n\nSimulate the polymer at different temperatures.\n\nCalculate and analyze properties such as radius of gyration, end-to-end distance, and potential energy.\n\nObserve and characterize the phase transition between folded and unfolded states.\n\nWrite a report summarizing your findings.","type":"content","url":"/lecture-25-project-2#tasks","position":43},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Requirements","lvl2":"Project Description"},"type":"lvl3","url":"/lecture-25-project-2#requirements","position":44},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Requirements","lvl2":"Project Description"},"content":"","type":"content","url":"/lecture-25-project-2#requirements","position":45},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Implementation","lvl3":"Requirements","lvl2":"Project Description"},"type":"lvl4","url":"/lecture-25-project-2#implementation","position":46},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Implementation","lvl3":"Requirements","lvl2":"Project Description"},"content":"Initialize Positions: Generate an initial configuration of the polymer chain without overlaps, applying periodic boundary conditions.\n\nCompute Forces: Implement functions to calculate harmonic bond forces and Lennard-Jones forces (both repulsive and attractive).\n\nIntegrate Equations of Motion: Use the velocity Verlet algorithm to update positions and velocities.\n\nTemperature Control: Implement a thermostat using velocity rescaling to maintain constant temperature.\n\nData Collection: Store trajectories and compute properties for analysis.","type":"content","url":"/lecture-25-project-2#implementation","position":47},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Simulation Parameters","lvl3":"Requirements","lvl2":"Project Description"},"type":"lvl4","url":"/lecture-25-project-2#simulation-parameters","position":48},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Simulation Parameters","lvl3":"Requirements","lvl2":"Project Description"},"content":"Polymer Length: Use at least N = 20 beads.\n\nTemperature Range: Simulate at temperatures ranging from low to high (e.g., T = 0.1 to T = 1.0).\n\nInteraction Strengths: Identify values of k and \\epsilon_{\\text{repulsive}} that prevent folding at low temperatures. Use \\epsilon_{\\text{attractive}} = 0.5 and \\sigma = 1.0.","type":"content","url":"/lecture-25-project-2#simulation-parameters","position":49},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Analysis","lvl3":"Requirements","lvl2":"Project Description"},"type":"lvl4","url":"/lecture-25-project-2#analysis","position":50},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Analysis","lvl3":"Requirements","lvl2":"Project Description"},"content":"Radius of Gyration (R_g):R_g = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (\\mathbf{r}_i - \\mathbf{r}_{\\text{cm}})^2}\n\nwhere \\mathbf{r}_{\\text{cm}} is the center of mass of the polymer.\n\nEnd-to-End Distance (R_{\\text{ee}}):R_{\\text{ee}} = |\\mathbf{r}_N - \\mathbf{r}_1|\n\nPotential Energy: Analyze how the potential energy changes with temperature.\n\nPhase Transition: Identify the temperature at which the polymer transitions from folded to unfolded state.","type":"content","url":"/lecture-25-project-2#analysis","position":51},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Visualization","lvl3":"Requirements","lvl2":"Project Description"},"type":"lvl4","url":"/lecture-25-project-2#visualization","position":52},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Visualization","lvl3":"Requirements","lvl2":"Project Description"},"content":"Plot R_g, R_{\\text{ee}}, and potential energy as functions of temperature.\n\nVisualize the polymer configurations at different temperatures.\n\nOptionally, create animations of the polymer dynamics using, e.g., ASE or Matplotlib.","type":"content","url":"/lecture-25-project-2#visualization","position":53},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Report","lvl3":"Requirements","lvl2":"Project Description"},"type":"lvl4","url":"/lecture-25-project-2#report","position":54},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl4":"Report","lvl3":"Requirements","lvl2":"Project Description"},"content":"Introduction: Briefly explain the significance of polymer folding and the purpose of your simulation.\n\nMethods: Describe your simulation setup, including models and algorithms used.\n\nResults: Present your findings with plots and figures.\n\nDiscussion: Interpret your results, discussing the phase transition and its implications.\n\nConclusion: Summarize your study and suggest future work.","type":"content","url":"/lecture-25-project-2#report","position":55},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Optional Enhancements (Five Bonus Points Each)","lvl2":"Project Description"},"type":"lvl3","url":"/lecture-25-project-2#optional-enhancements-five-bonus-points-each","position":56},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Optional Enhancements (Five Bonus Points Each)","lvl2":"Project Description"},"content":"Chain Length Effects: Investigate how the length of the polymer chain affects the folding behavior.\n\nEnergy Minimization: Perform energy minimization before starting the MD simulation to find a stable initial configuration.","type":"content","url":"/lecture-25-project-2#optional-enhancements-five-bonus-points-each","position":57},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Pseudocode for Analysis Functions"},"type":"lvl2","url":"/lecture-25-project-2#pseudocode-for-analysis-functions","position":58},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Pseudocode for Analysis Functions"},"content":"","type":"content","url":"/lecture-25-project-2#pseudocode-for-analysis-functions","position":59},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Calculate Radius of Gyration","lvl2":"Pseudocode for Analysis Functions"},"type":"lvl3","url":"/lecture-25-project-2#calculate-radius-of-gyration","position":60},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Calculate Radius of Gyration","lvl2":"Pseudocode for Analysis Functions"},"content":"FUNCTION calculate_radius_of_gyration(positions):\n    center_of_mass = mean(positions, axis=0)\n    Rg_squared = mean(sum((positions - center_of_mass)^2, axis=1))\n    Rg = sqrt(Rg_squared)\n    RETURN Rg","type":"content","url":"/lecture-25-project-2#calculate-radius-of-gyration","position":61},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Calculate End-to-End Distance","lvl2":"Pseudocode for Analysis Functions"},"type":"lvl3","url":"/lecture-25-project-2#calculate-end-to-end-distance","position":62},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl3":"Calculate End-to-End Distance","lvl2":"Pseudocode for Analysis Functions"},"content":"FUNCTION calculate_end_to_end_distance(positions):\n    Ree = norm(positions[-1] - positions[0])\n    RETURN Ree","type":"content","url":"/lecture-25-project-2#calculate-end-to-end-distance","position":63},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Example Analysis Code"},"type":"lvl2","url":"/lecture-25-project-2#example-analysis-code","position":64},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Example Analysis Code"},"content":"# Arrays to store properties\ntemperatures = np.linspace(0.1, 1.0, 10)\nRg_values = []\nRee_values = []\npotential_energies = []\n\nfor T in temperatures:\n    # Set target temperature\n    target_temperature = T\n    # (Re-initialize positions and velocities)\n    # (Run simulation)\n    # Compute properties\n    Rg = calculate_radius_of_gyration(positions)\n    Ree = calculate_end_to_end_distance(positions)\n    Rg_values.append(Rg)\n    Ree_values.append(Ree)\n    potential_energies.append(np.mean(potential_energy_array))\n\n# Plotting\nplt.figure()\nplt.plot(temperatures, Rg_values, label='Radius of Gyration')\nplt.xlabel('Temperature')\nplt.ylabel('Radius of Gyration')\nplt.title('Radius of Gyration vs Temperature')\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(temperatures, Ree_values, label='End-to-End Distance')\nplt.xlabel('Temperature')\nplt.ylabel('End-to-End Distance')\nplt.title('End-to-End Distance vs Temperature')\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(temperatures, potential_energies, label='Potential Energy')\nplt.xlabel('Temperature')\nplt.ylabel('Potential Energy')\nplt.title('Potential Energy vs Temperature')\nplt.legend()\nplt.show()","type":"content","url":"/lecture-25-project-2#example-analysis-code","position":65},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Project Submission"},"type":"lvl2","url":"/lecture-25-project-2#project-submission","position":66},{"hierarchy":{"lvl1":"Project 2: Molecular Dynamics Simulations of a Polymer Chain","lvl2":"Project Submission"},"content":"Code: Submit your Python code implementing the MD simulation and analysis.\n\nReport: Submit a PDF report containing your findings, plots, and discussions.\n\nRepository: Push your code and report to the project repository on GitHub.\n\nSubmission Link: Provide the repository link to the course portal.","type":"content","url":"/lecture-25-project-2#project-submission","position":67},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method"},"type":"lvl1","url":"/lecture-26-newton-raphson","position":0},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method"},"content":"","type":"content","url":"/lecture-26-newton-raphson","position":1},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-26-newton-raphson#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you will be able to:\n\nUnderstand the Newton-Raphson method.\n\nImplement the Newton-Raphson method to find the roots of a function.\n\nApply the Newton-Raphson method to solve nonlinear equations.","type":"content","url":"/lecture-26-newton-raphson#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl2":"Newton-Raphson Method"},"type":"lvl2","url":"/lecture-26-newton-raphson#newton-raphson-method","position":4},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl2":"Newton-Raphson Method"},"content":"The Newton-Raphson method is an iterative technique for finding the roots of a real-valued function f(x). The method starts with an initial guess x_0 and iteratively refines the guess using the formula:x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n\nwhere f'(x_n) is the derivative of the function f(x) evaluated at x_n. The process is repeated until the difference between successive approximations is less than a specified tolerance.","type":"content","url":"/lecture-26-newton-raphson#newton-raphson-method","position":5},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl2":"Geometric Interpretation"},"type":"lvl2","url":"/lecture-26-newton-raphson#geometric-interpretation","position":6},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl2":"Geometric Interpretation"},"content":"The Newton-Raphson method can be interpreted geometrically as follows. Given a function f(x), the tangent line to the curve at the point (x_n, f(x_n)) is given by:y = f'(x_n)(x - x_n) + f(x_n)\n\nThe intersection of this tangent line with the x-axis gives the next approximation x_{n+1}:0 = f'(x_n)(x_{n+1} - x_n) + f(x_n)\n\nSolving for x_{n+1}, we get:x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n\nThis is the same formula as the one derived algebraically.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function f(x) = x^3 - 2x - 5\ndef f(x):\n    return x**3 - 2*x - 5\n\n# Define the derivative f'(x) = 3x^2 - 2\ndef f_prime(x):\n    return 3*x**2 - 2\n\n# Define the tangent line at x = 2\ndef tangent_line(x):\n    return f_prime(2)*(x - 2) + f(2)\n\n# Plot the function f(x) and the tangent line at x = 2\nx = np.linspace(0, 4, 100)\ny = f(x)\ntangent = tangent_line(x)\n\n# Calculate the next approximation using Newton-Raphson method\nx_new = 2 - f(2) / f_prime(2)\n\n# Plot the function and the tangent line\nplt.figure(figsize=(8, 6))\nplt.plot(x, y, label='$f(x) = x^3 - 2x - 5$')\nplt.plot(x, tangent, label='Tangent line at $x = 2$')\nplt.axvline(x=2, color='r', linestyle='--', label='$x_0 = 2$')\nplt.axvline(x=x_new, color='g', linestyle='--', label='$x_1 = 2 - f(2)/f\\'(2) = {:.1f}$'.format(x_new))\nplt.xlabel('$x$')\nplt.ylabel('$f(x)$')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n","type":"content","url":"/lecture-26-newton-raphson#geometric-interpretation","position":7},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl2":"Implementation of Newton-Raphson Method"},"type":"lvl2","url":"/lecture-26-newton-raphson#implementation-of-newton-raphson-method","position":8},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl2":"Implementation of Newton-Raphson Method"},"content":"Let’s implement the Newton-Raphson method in Python to find the root of a function f(x). We will define the function f(x) and its derivative f'(x), choose an initial guess x_0, and iterate until the convergence criterion is met.\n\nimport numpy as np\n\ndef f(x):\n    return x**3 - 2*x - 5\n\ndef f_prime(x):\n    return 3*x**2 - 2\n\ndef newton_raphson(f, f_prime, x0, tol=1e-6, max_iter=100):\n    x = x0\n    for i in range(max_iter):\n        x_new = x - f(x) / f_prime(x)\n        if np.abs(x_new - x) < tol:\n            return x_new\n        x = x_new\n    return None\n\n# Initial guess\nx0 = 2.0\n\n# Find the root using Newton-Raphson method\nroot = newton_raphson(f, f_prime, x0)\nprint(f\"Root of the function: {root}\")\n\nIn this example, we define a function f(x) = x^3 - 2x - 5 and its derivative f'(x) = 3x^2 - 2. We choose an initial guess x_0 = 2.0 and apply the Newton-Raphson method to find the root of the function. The result is printed as the output.","type":"content","url":"/lecture-26-newton-raphson#implementation-of-newton-raphson-method","position":9},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl2":"Multivariate Newton-Raphson Method"},"type":"lvl2","url":"/lecture-26-newton-raphson#multivariate-newton-raphson-method","position":10},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl2":"Multivariate Newton-Raphson Method"},"content":"The Newton-Raphson method can be extended to find the roots of a system of nonlinear equations. Given a system of equations f(x) = 0, where f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n, the Newton-Raphson method iteratively refines the guess x_n using the formula:x_{n+1} = x_n - J^{-1}(x_n) f(x_n)\n\nwhere J(x_n) is the Jacobian matrix of f(x) evaluated at x_n. The process is repeated until the difference between successive approximations is less than a specified tolerance.","type":"content","url":"/lecture-26-newton-raphson#multivariate-newton-raphson-method","position":11},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl2":"Implementation of Multivariate Newton-Raphson Method"},"type":"lvl2","url":"/lecture-26-newton-raphson#implementation-of-multivariate-newton-raphson-method","position":12},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl2":"Implementation of Multivariate Newton-Raphson Method"},"content":"Let’s implement the multivariate Newton-Raphson method in Python to solve a chemical equilibrium problem. Consider a chemical system with two species, A and B, in equilibrium:\\text{A} \\rightleftharpoons 2\\text{B}\n\nwith the equilibrium constant K = 100.\n\nWe aim to find the equilibrium concentrations of A ([\\text{A}]) and B ([\\text{B}]) starting from initial concentrations:[\\text{A}]_0 = 1.0 \\, \\text{M}, \\quad [\\text{B}]_0 = 0.5 \\, \\text{M}","type":"content","url":"/lecture-26-newton-raphson#implementation-of-multivariate-newton-raphson-method","position":13},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl3":"Equations to Solve","lvl2":"Implementation of Multivariate Newton-Raphson Method"},"type":"lvl3","url":"/lecture-26-newton-raphson#equations-to-solve","position":14},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl3":"Equations to Solve","lvl2":"Implementation of Multivariate Newton-Raphson Method"},"content":"The equilibrium constant relation:K = \\frac{[\\text{B}]^2}{[\\text{A}]}\n\nRearrange to:f_1([\\text{A}], [\\text{B}]) = [\\text{B}]^2 - K[\\text{A}] = 0\n\nConservation of mass:[\\text{A}] + [\\text{B}] = \\text{constant (initial total)} = 1.5\n\nRearrange to:f_2([\\text{A}], [\\text{B}]) = [\\text{A}] + [\\text{B}] - 1.5 = 0\n\nThese form a nonlinear system of equations:\\mathbf{F}([\\text{A}], [\\text{B}]) =\n\\begin{bmatrix}\nf_1([\\text{A}], [\\text{B}]) \\\\\nf_2([\\text{A}], [\\text{B}])\n\\end{bmatrix} =\n\\begin{bmatrix}\n[\\text{B}]^2 - K[\\text{A}] \\\\\n[\\text{A}] + [\\text{B}] - 1.5\n\\end{bmatrix} =\n\\mathbf{0}","type":"content","url":"/lecture-26-newton-raphson#equations-to-solve","position":15},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl3":"Newton-Raphson Algorithm","lvl2":"Implementation of Multivariate Newton-Raphson Method"},"type":"lvl3","url":"/lecture-26-newton-raphson#newton-raphson-algorithm","position":16},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl3":"Newton-Raphson Algorithm","lvl2":"Implementation of Multivariate Newton-Raphson Method"},"content":"Start with an initial guess:\\mathbf{x}_0 = \\begin{bmatrix} [\\text{A}]_0 \\\\ [\\text{B}]_0 \\end{bmatrix} = \\begin{bmatrix} 1.0 \\\\ 0.5 \\end{bmatrix}\n\nAt each iteration, compute:\\mathbf{x}_{n+1} = \\mathbf{x}_n - \\mathbf{J}^{-1}(\\mathbf{x}_n) \\cdot \\mathbf{F}(\\mathbf{x}_n)\n\nwhere \\mathbf{J} is the Jacobian matrix of partial derivatives:\\mathbf{J} =\n\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial [\\text{A}]} & \\frac{\\partial f_1}{\\partial [\\text{B}]} \\\\\n\\frac{\\partial f_2}{\\partial [\\text{A}]} & \\frac{\\partial f_2}{\\partial [\\text{B}]}\n\\end{bmatrix}\n\nCompute the partial derivatives:\\frac{\\partial f_1}{\\partial [\\text{A}]} = -100, \\quad \\frac{\\partial f_1}{\\partial [\\text{B}]} = 2[\\text{B}]\\frac{\\partial f_2}{\\partial [\\text{A}]} = 1, \\quad \\frac{\\partial f_2}{\\partial [\\text{B}]} = 1\n\nSo:\\mathbf{J} =\n\\begin{bmatrix}\n-100 & 2[\\text{B}] \\\\\n1 & 1\n\\end{bmatrix}\n\nIterate until convergence (when \\|\\mathbf{F}(\\mathbf{x}_n)\\| is sufficiently small).","type":"content","url":"/lecture-26-newton-raphson#newton-raphson-algorithm","position":17},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl3":"Python Implementation","lvl2":"Implementation of Multivariate Newton-Raphson Method"},"type":"lvl3","url":"/lecture-26-newton-raphson#python-implementation","position":18},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl3":"Python Implementation","lvl2":"Implementation of Multivariate Newton-Raphson Method"},"content":"Here’s Python code to implement this:\n\nimport numpy as np\n\n# Define the functions\ndef F(x):\n    A, B = x\n    return np.array([\n        B**2 - 100 * A,  # f1\n        A + B - 1.5      # f2\n    ])\n\n# Define the Jacobian\ndef J(x):\n    A, B = x\n    return np.array([\n        [-100, 2 * B],  # Partial derivatives of f1\n        [1, 1]          # Partial derivatives of f2\n    ])\n\n# Initial guess\nx0 = np.array([1.0, 0.5])\n\n# Newton-Raphson iteration\ntolerance = 1e-6\nmax_iter = 100\nfor i in range(max_iter):\n    Fx = F(x0)\n    Jx = J(x0)\n    dx = np.linalg.solve(Jx, -Fx)  # Solve J * dx = -F\n    x0 = x0 + dx\n    \n    if np.linalg.norm(Fx, ord=2) < tolerance:\n        print(f\"Converged in {i+1} iterations.\")\n        break\nelse:\n    print(\"Did not converge.\")\n\n# Results\nprint(f\"Equilibrium concentrations: [A] = {x0[0]:.6f}, [B] = {x0[1]:.6f}\")\n\nIn this code, we define the functions f_1([\\text{A}], [\\text{B}]) and f_2([\\text{A}], [\\text{B}]) and their Jacobian matrix \\mathbf{J}. We choose an initial guess \\mathbf{x}_0 = [1.0, 0.5] and apply the Newton-Raphson method to solve the system of equations. The equilibrium concentrations of A and B are printed as the output. The analytical solution is [\\text{A}] = 0.02 M and [\\text{B}] = 1.48 M.","type":"content","url":"/lecture-26-newton-raphson#python-implementation","position":19},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl2":"Summary"},"type":"lvl2","url":"/lecture-26-newton-raphson#summary","position":20},{"hierarchy":{"lvl1":"Chapter 24: Newton-Raphson Method","lvl2":"Summary"},"content":"In this lecture, we learned about the Newton-Raphson method for finding the roots of a function. We discussed the geometric interpretation of the method and implemented it in Python. We also extended the method to solve a system of nonlinear equations using the multivariate Newton-Raphson method.","type":"content","url":"/lecture-26-newton-raphson#summary","position":21},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo"},"type":"lvl1","url":"/lecture-27-kinetic-mc","position":0},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo"},"content":"","type":"content","url":"/lecture-27-kinetic-mc","position":1},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-27-kinetic-mc#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you will be able to:\n\nUnderstand the concept of Kinetic Monte Carlo (KMC) simulations.\n\nImplement a simple KMC simulation in Python.\n\nApply KMC simulations to model the dynamics of a system.","type":"content","url":"/lecture-27-kinetic-mc#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Introduction to Kinetic Monte Carlo"},"type":"lvl2","url":"/lecture-27-kinetic-mc#introduction-to-kinetic-monte-carlo","position":4},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Introduction to Kinetic Monte Carlo"},"content":"Phenomena such as diffusion, nucleation of a new phase, structural transformations, and chemical reactions often involve processes that occur over timescales much longer than those accessible via molecular dynamics (MD) simulations. MD simulations are limited by computational resources, making it challenging to simulate processes that occur over milliseconds or longer. Despite their extended timescales, these processes are kinetic and time-dependent.\n\nA common aspect of these processes is that they proceed through transitions between various configurations or states, each occurring with a certain frequency. These frequencies determine the time evolution of the entire process. However, they are not outcomes of the kinetic Monte Carlo (KMC) method itself; rather, they must be known or calculated beforehand. This means that prior to performing a KMC simulation, the possible transitions involved in the kinetic process, along with their corresponding frequencies, must be identified and determined independently of the KMC calculation. These frequencies can be calculated using methods such as transition state theory or harmonic approximation. While they can also be calculated on the fly during the simulation, such calculations are separate from the KMC process itself.\n\nKMC is a computational algorithm that simulates the time evolution of a system by probabilistically selecting and executing transitions based on their frequencies. To illustrate the KMC method in practice, we will apply it to model the diffusion process in a solid.","type":"content","url":"/lecture-27-kinetic-mc#introduction-to-kinetic-monte-carlo","position":5},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Diffusion Studied by Kinetic Monte Carlo"},"type":"lvl2","url":"/lecture-27-kinetic-mc#diffusion-studied-by-kinetic-monte-carlo","position":6},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Diffusion Studied by Kinetic Monte Carlo"},"content":"We consider diffusion via the movement of vacancies in a crystal structure. For example, in the cubic structure of cerium dioxide (CeO₂), which crystallizes in this form, a vacancy can occur at one of the sites occupied by the majority atoms (oxygen in CeO₂). As shown in Figure 3, this vacancy can move to several neighboring sites, indicated by the arrows. Each such jump occurs with a specific frequency.\n\nVacancies facilitate atomic movement within the lattice, making them crucial for understanding diffusion mechanisms in solids. There are N possible positions for the vacancy within the crystal lattice. If the vacancy is at position i, it can jump to M_i different neighboring positions with frequencies \\nu_{ij}, where j indexes the possible destinations from position i. When it moves to a new position j, it can again jump to M_j positions with frequencies \\nu_{jk}, and so on. Thus, the diffusion process is characterized by the set of frequencies \\{ \\nu_{ij} \\}, forming an N \\times N matrix where each element represents the frequency of a jump from position i to position j. Note that some of these frequencies may be zero if a direct jump from i to j is not possible.\n\nThe probability that a jump from position i to position j occurs is given by:p_{ij} = \\frac{\\nu_{ij}}{\\sum_{k=1}^{N} \\nu_{ik}}\n\nHere, \\sum_{k=1}^{N} \\nu_{ik} is the total frequency of all possible jumps from position i. Obviously, 0 \\leq p_{ij} \\leq 1, and \\sum_{j=1}^{N} p_{ij} = 1. These probabilities are commonly referred to as transition probabilities, while the frequencies \\nu_{ij} are known as rate constants.\n\nThe frequencies \\nu_{ij} can be calculated using methods such as transition state theory or derived from experimental data. In KMC simulations, these transition probabilities are used to stochastically determine the sequence of vacancy movements over time.\n\nIn defining these jump frequencies, we assume that when the vacancy moves from position i to position j, it completely “loses the memory” of how it arrived at state i. Therefore, the probability p_{ij} is entirely independent of the previous jumps that led to position i. This property characterizes the process as a Markov process, which is fundamental to the KMC method.\n\n\n\nFigure 1:Crystal structure of cerium dioxide (CeO₂) illustrating an oxygen vacancy (highlighted). The vacancy resides on an oxygen site within the fluorite structure. The arrows indicate potential pathways for oxygen ion diffusion via vacancy migration. Axes (a, b, c) are provided for reference.","type":"content","url":"/lecture-27-kinetic-mc#diffusion-studied-by-kinetic-monte-carlo","position":7},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Kinetic Monte Carlo Procedure"},"type":"lvl2","url":"/lecture-27-kinetic-mc#kinetic-monte-carlo-procedure","position":8},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Kinetic Monte Carlo Procedure"},"content":"We start with a vacanccy at a position marked i and select randomly a possible jump into a position j. This jump is then made with probability p_{ij}. To do this we generate a random number \\xi such that 0 \\leq \\xi < 1 and make the jump i \\rightarrow j if p_{ij} \\geq \\xi. However, if p_{ij} < \\xi we do not make any jump and repeat the process starting again by selecting randomly another possible jump.. After a jump occurred we start with a new vacancy position that is associated with new jump frequencies (new rate constants) and the process is repeated. In this way the vacancy travels and attains various positions but to determine the rate at which it moves we have to associate a time with each step of this random walk.\n\nIf the vacancy is at a site i then the frequency \\nu_\\text{tot}^i with which it will leave the site i is equal to the sum of the frequencies of all possible jumps away from the site i:\\nu_\\text{tot}^i = \\sum_{k=1}^{M_i} \\nu_{ik}\n\nHence the time the vacancy stays at the site i ist_i = \\frac{1}{\\nu_\\text{tot}^i}\n\nIn this way we have associated with every vacancy position i the time t_i during which the vacancy waits at this position. This time is usually dominated by one of the frequencies that is much higher than all the other frequencies. During the KMC process we associate with every state i into which the vacancy got during the process, the time t_i during which the vacancy remains at the position i, determined by the previous equation. The time associated with the process consisting of K steps of the KMC is then\\Delta t^{(K)} = \\sum_{k \\text{ corresponding to all states attained}} t_k","type":"content","url":"/lecture-27-kinetic-mc#kinetic-monte-carlo-procedure","position":9},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Transition (Reaction) Rate Theory"},"type":"lvl2","url":"/lecture-27-kinetic-mc#transition-reaction-rate-theory","position":10},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Transition (Reaction) Rate Theory"},"content":"We investigate a process during which the potential energy of a system varies as depicted in the figure below. The initial and final states are metastable, corresponding to minima in the potential energy (or enthalpy, if external forces are performing work). Between these states, the energy rises to a maximum at an intermediate stage, representing the activated state or transition state.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef add_double_arrow(x, y_min, y_max, arrow_length_offset=0, head_width=0.0125, head_length=0.05, color='black'):\n    \"\"\"\n    Add a double-sided arrow to a plot.\n\n    Parameters:\n        x (float): The x-coordinate of the arrow base.\n        y_min (float): The starting y-coordinate (bottom) of the arrow.\n        y_max (float): The ending y-coordinate (top) of the arrow.\n        arrow_length_offset (float): Adjustment to the arrow length (positive to make shorter).\n        head_width (float): Width of the arrowhead.\n        head_length (float): Length of the arrowhead.\n        color (str): Color of the arrows.\n    \"\"\"\n    arrow_length = y_max - y_min - arrow_length_offset\n    \n    # Downward arrow\n    plt.arrow(x, y_min, 0, arrow_length, head_width=head_width, head_length=head_length, fc=color, ec=color)\n    \n    # Upward arrow\n    plt.arrow(x, y_min + arrow_length, 0, -arrow_length + arrow_length_offset, head_width=head_width, head_length=head_length, fc=color, ec=color)\n\n# Generate data for the potential energy curve\nx = np.linspace(0, 1, 100)\ny = -np.sin(2 * np.pi * x) ** 2 + x\n\n# Define key points for the ground, activated, and final states\nx_gs, x_as, x_fs = 0.25, 0.5, 0.75\ny_gs = np.min(y)\ny_as = np.max(y[y.shape[0] // 4:3 * y.shape[0] // 4])\ny_fs = np.min(y[y.shape[0] // 2:])\n\n# Plot the potential energy curve\nplt.figure(figsize=(8, 6))\nplt.plot(x, y, label='Potential Energy Curve')\nplt.xlabel('Reaction Coordinate')\nplt.ylabel('Potential Energy')\nplt.xlim(0.2, 0.8)\nplt.xticks([x_gs, x_as, x_fs], ['Ground State', 'Activated State', 'Final State'])\nplt.yticks([y_gs, y_as, y_fs], ['$E_\\\\text{gs}$', '$E_\\\\text{as} = E_\\\\text{gs} + E_a$', '$E_\\\\text{fs} = E_\\\\text{gs} + \\\\Delta E$'])\n\n# Add dashed lines for energy levels\nplt.axhline(y=y_gs, color='C1', linestyle='--', label='Ground State Level')\nplt.axhline(y=y_fs, xmin=0.75, xmax=1, color='C2', linestyle='--', label='Final State Level')\nplt.axhline(y=y_as, xmin=0.375, xmax=0.625, color='C3', linestyle='--', label='Activated State Level')\n\n# Add double-sided arrows for activation energy and reaction energy\nadd_double_arrow(x=0.5, y_min=y_gs, y_max=y_as, arrow_length_offset=0.05)\nadd_double_arrow(x=0.75, y_min=y_gs, y_max=y_fs, arrow_length_offset=0.05)\n\n# Add text annotations for energy levels\nplt.text(0.5, (y_gs + y_as) / 2, '$E_a$', ha='center', va='center', bbox=dict(facecolor='white', edgecolor='white'))\nplt.text(0.75, (y_gs + y_fs) / 2, '$\\\\Delta E$', ha='center', va='center', bbox=dict(facecolor='white', edgecolor='white'))\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nAs illustrated in the figure above, the potential energy profile shows two minima separated by an energy barrier of height E_a. The “reaction coordinate” axis represents the pathway along which the reaction occurs, encompassing the structural changes the system undergoes during the transformation. During this process, the system temporarily acquires an amount of energy equal to E_a to overcome the transition state.\n\nWe now pose the following question: Assuming we know the reaction path and the associated energy barrier of height E_a, what is the frequency with which the transition from one equilibrium state to the other occurs?\n\nThis question can be answered to a good approximation within the framework of classical transition state theory (also known as reaction rate theory), which was first thoroughly presented in the book by Glasstone, Laidler, and Eyring:\n\nGlasstone, S., Laidler, K. J., and Eyring, H., The Theory of Rate Processes, McGraw-Hill, 1941.\n\nTransition state theory is fundamental in chemical kinetics as it allows us to predict reaction rates based on the energy barrier and temperature. It provides a method for calculating reaction rates by considering the activated complex at the top of the energy barrier. In the following sections, we summarize the main aspects of this theory and derive an expression for the reaction rate constant k that depends on E_a and the temperature T.","type":"content","url":"/lecture-27-kinetic-mc#transition-reaction-rate-theory","position":11},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Basic Assumptions of Transition State Theory"},"type":"lvl2","url":"/lecture-27-kinetic-mc#basic-assumptions-of-transition-state-theory","position":12},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Basic Assumptions of Transition State Theory"},"content":"Stable Initial and Final States:\n\nThe reaction involves initial and final configurations that are stable states. The transition between these states occurs along a specific pathway known as the reaction coordinate or reaction path.\n\nThe reaction coordinate represents a path on the potential energy surface (PES), which maps the energy changes as the system progresses from reactants to products.\n\nEnergy Barrier and Activated State:\n\nThere is an energy barrier between the initial and final states when moving along the reaction path. The most favorable path is the one with the lowest energy barrier. The point at which the energy reaches a maximum along this path is called the activated state or transition state.\n\nAlthough the transition state represents a maximum in energy, it is a transient configuration that cannot be isolated.\n\nQuasi-Equilibrium of the Activated State:\n\nIt is assumed that the system reaches a quasi-equilibrium where the activated state is in thermodynamic equilibrium with the initial state. This allows for the use of equilibrium thermodynamics to describe the population of the activated state.\n\nIn the activated state, the system vibrates in all directions perpendicular to the reaction coordinate but moves translationally along the reaction path. In contrast, the stable initial and final states have vibrational degrees of freedom in all directions, including along the reaction coordinate.\n\nThe validity of these assumptions depends on the specific reaction mechanism. These are approximations, and experimental observations ultimately determine whether the conclusions drawn from the theory are accurate. In cases where quantum tunneling or non-equilibrium dynamics play a significant role, the assumptions may not hold.","type":"content","url":"/lecture-27-kinetic-mc#basic-assumptions-of-transition-state-theory","position":13},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Main Results of Transition State Theory"},"type":"lvl2","url":"/lecture-27-kinetic-mc#main-results-of-transition-state-theory","position":14},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Main Results of Transition State Theory"},"content":"In transition state theory, the reaction rate (or frequency of activations) is given by:\\nu = \\nu_0 \\exp\\left(-\\frac{E_a}{k_\\text{B} T}\\right)\n\nwhere:\n\n\\nu is the reaction rate or frequency of successful transitions.\n\n\\nu_0 is the pre-exponential factor or attempt frequency, representing the frequency of attempts to overcome the energy barrier.\n\nE_a is the activation energy—the height of the energy barrier between the initial and final states.\n\nk_\\text{B} is the Boltzmann constant.\n\nT is the absolute temperature.\n\nBecause of the exponential dependence on the activation energy E_a, the actual reaction frequency \\nu is much smaller than the attempt frequency \\nu_0; the difference between these two frequencies can span several orders of magnitude.\n\nTip\n\nTo test whether a process is well described by transition state theory, you can produce an Arrhenius plot. Plot the logarithm of a quantity proportional to the reaction rate (e.g., \\ln \\nu or \\ln k) versus the inverse of the temperature 1/T. If the process follows transition state theory, the plot should be a straight line, indicating an exponential dependence on E_a.","type":"content","url":"/lecture-27-kinetic-mc#main-results-of-transition-state-theory","position":15},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Application of the Kinetic Monte Carlo to Oxygen Vacancy Diffusion in CeO₂"},"type":"lvl2","url":"/lecture-27-kinetic-mc#application-of-the-kinetic-monte-carlo-to-oxygen-vacancy-diffusion-in-ceo","position":16},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Application of the Kinetic Monte Carlo to Oxygen Vacancy Diffusion in CeO₂"},"content":"We will now apply the KMC method to model the diffusion of oxygen vacancies in CeO₂. The vacancy can move to neighboring sites with specific frequencies, and we will simulate the time evolution of the system by stochastically selecting and executing these jumps. The frequencies of the jumps are determined by the transition probabilities, which are calculated based on the rate constants.\n\nIn this example, we will consider a cubic lattice with vacancies that can move to neighboring sites in three dimensions. We will define the lattice as a 3D array, where each site can be occupied by an oxygen atom or a vacancy. The vacancy will move to neighboring sites with specific frequencies, and we will simulate the diffusion process using the KMC method.\n\nLet’s start by defining the cubic lattice parameters.\n\n# Define the cubic lattice parameters\nlattice_size = 10  # 10x10x10 lattice\nnum_steps = 10000  # Number of KMC steps\ntemperature = 1000  # Temperature in Kelvin\nactivation_energy = 1.0  # Activation energy in eV\nattempt_frequency = 1e13  # Attempt frequency in Hz\nboltzmann_constant = 8.617333262e-5  # Boltzmann constant in eV/K\n\nNext, we will precompute the rate constants for the vacancy jumps between neighboring sites. We will assume that the vacancy can move to the six neighboring sites in 3D space with equal frequencies. The rate constants will be calculated based on the activation energy and temperature.\n\n# Precompute rate constant\nrate_constant = attempt_frequency * np.exp(-activation_energy / (boltzmann_constant * temperature))\nprint(f\"Rate constant: {rate_constant:.2e} Hz\")\n\n# Plot the rate constant as a function of temperature\ntemperatures = np.linspace(300, 2000, 100)\nrate_constants = attempt_frequency * np.exp(-activation_energy / (boltzmann_constant * temperatures))\n\nplt.figure(figsize=(8, 6))\nplt.plot(temperatures, rate_constants)\nplt.axvline(x=temperature, color='red', linestyle='--')\nplt.axhline(y=rate_constant, color='green', linestyle='--')\nplt.yscale('log')\nplt.xlabel('Temperature (K)')\nplt.ylabel('Rate Constant (Hz)')\nplt.tight_layout()\nplt.grid(True)\nplt.show()\n\nNow, we will initialize the lattice with a single vacancy at the center and simulate the diffusion process using the KMC method. We will randomly select a neighboring site to move the vacancy to based on the transition probabilities calculated from the rate constants.\n\n# Initialize the lattice with a single vacancy\nlattice = np.zeros((lattice_size, lattice_size, lattice_size), dtype=int)\nvacancy_position = [lattice_size // 2] * 3  # Start at the center of the lattice\n# vacancy_position = [0, 0, 0]\n# vacancy_position = [lattice_size - 1] * 3\nlattice[tuple(vacancy_position)] = 1  # Mark the vacancy position\n\n# Plot the initial lattice configuration\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot the lattice sites\nfor i in range(lattice_size):\n    for j in range(lattice_size):\n        for k in range(lattice_size):\n            if lattice[i, j, k] == 1:\n                ax.scatter(i, j, k, color='red', s=100, edgecolors='black')\n            else:\n                ax.scatter(i, j, k, color='blue', s=50, edgecolors='black', alpha=0.5)\n\nax.set_xlabel('$x$')\nax.set_ylabel('$y$')\nax.set_zlabel('$z$')\nax.set_title('Initial Lattice Configuration')\nplt.tight_layout()\nplt.show()\n\nNext, we will define the neighbor offsets for a cubic lattice, where the vacancy can move to six neighboring sites in 3D space.\n\n# Define neighbor offsets for a cubic lattice (6 neighbors)\nneighbor_offsets = [\n    (-1, 0, 0), (1, 0, 0),  # +/- x direction\n    (0, -1, 0), (0, 1, 0),  # +/- y direction\n    (0, 0, -1), (0, 0, 1)   # +/- z direction\n]\n\n# Plot the neighbor offsets in 3D space as arrows from the vacancy position (center) to the neighbors (6 directions)\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot a subset of the lattice sites\nfor i in range(lattice_size // 2 - 1, lattice_size // 2 + 2):\n    for j in range(lattice_size // 2 - 1, lattice_size // 2 + 2):\n        for k in range(lattice_size // 2 - 1, lattice_size // 2 + 2):\n            if (i, j, k) == tuple(vacancy_position):\n                ax.scatter(i, j, k, color='red', s=100, edgecolors='black')\n            else:\n                ax.scatter(i, j, k, color='blue', s=50, edgecolors='black', alpha=0.5)\n\n# Plot the neighbor offsets as arrows\nfor offset in neighbor_offsets:\n    ax.quiver(vacancy_position[0], vacancy_position[1], vacancy_position[2], offset[0], offset[1], offset[2], color='green', arrow_length_ratio=0.1)\n\nax.set_xlabel('$x$')\nax.set_ylabel('$y$')\nax.set_zlabel('$z$')\nax.set_title('Neighbor Offsets for Vacancy Movement')\nplt.tight_layout()\nplt.show()\n\nNow, we will keep track of the vacancy position and time during the KMC simulation. We will randomly select a neighboring site to move the vacancy to based on the transition probabilities calculated from the rate constants. The simulation will proceed for a specified number of steps.\n\n# Keep track of the vacancy position and time\nvacancy_positions = [tuple(vacancy_position)]\ntime = 0\ntime_steps = [time]\n\nNext, we will perform the KMC simulation by randomly selecting a neighboring site to move the vacancy to based on the transition probabilities. We will update the vacancy position and time for each step.\n\n# Run the KMC simulation\nnp.random.seed(42)  # Set random seed for reproducibility\nfor step in range(num_steps):\n    while True:\n        # Determine possible moves\n        possible_moves = []\n        for offset in neighbor_offsets:\n            # Calculate the neighbor position (wrap around for periodic boundary conditions)\n            neighbor = [\n                (vacancy_position[i] + offset[i]) % lattice_size\n                for i in range(3)\n            ]\n            possible_moves.append(neighbor)\n        \n        # Assign probabilities to each move (uniform here)\n        probabilities = [rate_constant for _ in possible_moves]\n        probabilities /= np.sum(probabilities)\n        \n        # Randomly select a possible move\n        chosen_move_idx = np.random.choice(len(possible_moves))\n        new_position = possible_moves[chosen_move_idx]\n\n        # Generate a random number for acceptance\n        xi = np.random.random()\n\n        # Check the acceptance criterion\n        if xi < probabilities[chosen_move_idx]:\n            # Move is accepted, exit the loop\n            break\n\n    # Update the lattice and the vacancy position\n    lattice[tuple(vacancy_position)] = 0\n    lattice[tuple(new_position)] = 1\n    vacancy_position = new_position\n    vacancy_positions.append(tuple(vacancy_position))\n\n    # Update time\n    total_rate = np.sum([rate_constant for _ in possible_moves])\n    time += -np.log(np.random.random()) / total_rate\n    time_steps.append(time)\n\nprint(f\"Total simulation time: {time:.2e} s\")\n\n# Convert the vacancy positions to a list of ASE atoms for visualization\nfrom ase import Atoms\nfrom ase.visualize import view\n\nlist_atoms = []\nfor position in vacancy_positions:\n    # Create a list of chemical symbols for the lattice sites\n    chemical_symbols = []\n    for i in range(lattice_size):\n        for j in range(lattice_size):\n            for k in range(lattice_size):\n                if (i, j, k) == position:\n                    chemical_symbols.append('F')  # Vacancy\n                else:\n                    chemical_symbols.append('O')  # Oxygen atom\n\n    # Create an ASE Atoms object for the lattice configuration\n    atoms = Atoms(chemical_symbols, positions=[(i, j, k) for i in range(lattice_size) for j in range(lattice_size) for k in range(lattice_size)], pbc=True, cell=[lattice_size, lattice_size, lattice_size])\n    list_atoms.append(atoms)\n\n# Write the lattice configurations to a trajectory file for visualization\nfrom ase.io import write\nwrite('vacancy_diffusion.extxyz', list_atoms)\n\n\n\nFigure 2:Visualization of the vacancy diffusion process in a cubic lattice. The vacancy (highlighted in blue) moves to neighboring sites over time, simulating the diffusion process. The trajectory lines indicate the path taken by the vacancy during the simulation.","type":"content","url":"/lecture-27-kinetic-mc#application-of-the-kinetic-monte-carlo-to-oxygen-vacancy-diffusion-in-ceo","position":17},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Diffusion Constant"},"type":"lvl2","url":"/lecture-27-kinetic-mc#diffusion-constant","position":18},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Diffusion Constant"},"content":"The diffusion constant of the vacancy, D_v, can now be determined using the Einstein formulaD_v = \\lim_{\\tau \\to \\infty} \\frac{1}{6 \\tau} \\langle \\left[ r(\\tau) - r(0) \\right]^2 \\rangle\n\nexpressed asD_v = \\lim_{K \\to \\infty} \\frac{1}{6 \\Delta t^{(K)}} \\langle \\left[ r(\\Delta t^{(K)}) - r(0) \\right]^2 \\rangle\n\nwhere r(\\Delta t^{(K)}) is the position of the vacancy after K steps and r(0) the starting position. Averaging has to be taken over a large number of KMC paths that start with different original positions of the vacancy.","type":"content","url":"/lecture-27-kinetic-mc#diffusion-constant","position":19},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Summary"},"type":"lvl2","url":"/lecture-27-kinetic-mc#summary","position":20},{"hierarchy":{"lvl1":"Chapter 25: Kinetic Monte Carlo","lvl2":"Summary"},"content":"In this lecture, we introduced the concept of KMC simulations and applied it to model the diffusion of oxygen vacancies in a solid. We precomputed the rate constants for vacancy jumps between neighboring sites and simulated the time evolution of the system by stochastically selecting and executing these jumps. The KMC method allowed us to model the dynamics of the system over time and observe the diffusion process in action.","type":"content","url":"/lecture-27-kinetic-mc#summary","position":21},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn"},"type":"lvl1","url":"/lecture-28-scikit-learn","position":0},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn"},"content":"","type":"content","url":"/lecture-28-scikit-learn","position":1},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lecture-28-scikit-learn#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl2":"Learning Objectives"},"content":"By the end of this lecture, you will be able to:\n\nUse scikit-learn to perform supervised learning\n\nUnderstand the difference between classification and regression\n\nTrain and evaluate classification models\n\nTrain and evaluate regression models","type":"content","url":"/lecture-28-scikit-learn#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl2":"scikit-learn"},"type":"lvl2","url":"/lecture-28-scikit-learn#scikit-learn","position":4},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl2":"scikit-learn"},"content":"scikit-learn is a Python package that provides simple and efficient tools for data analysis. It is built on numpy, scipy, and matplotlib. It is open source and commercially usable under the BSD license. It is a great tool for machine learning in Python.","type":"content","url":"/lecture-28-scikit-learn#scikit-learn","position":5},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl3":"Installation","lvl2":"scikit-learn"},"type":"lvl3","url":"/lecture-28-scikit-learn#installation","position":6},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl3":"Installation","lvl2":"scikit-learn"},"content":"To install scikit-learn, you can follow the instructions on the \n\nofficial website. You can install it using pip:pip install -U scikit-learn","type":"content","url":"/lecture-28-scikit-learn#installation","position":7},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl2":"Supervised Learning"},"type":"lvl2","url":"/lecture-28-scikit-learn#supervised-learning","position":8},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl2":"Supervised Learning"},"content":"In supervised learning, we have a dataset consisting of both input features and output labels. The goal is to learn a mapping from the input to the output. We have two types of supervised learning:\n\nClassification: The output is a category.\n\nRegression: The output is a continuous value.","type":"content","url":"/lecture-28-scikit-learn#supervised-learning","position":9},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl3":"Classification","lvl2":"Supervised Learning"},"type":"lvl3","url":"/lecture-28-scikit-learn#classification","position":10},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl3":"Classification","lvl2":"Supervised Learning"},"content":"In classification, we have a dataset consisting of input features and output labels. The goal is to learn a mapping from the input features to the output labels. We can use the scikit-learn library to perform classification.","type":"content","url":"/lecture-28-scikit-learn#classification","position":11},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl4":"Machine Learning by Example: Wine Classification","lvl3":"Classification","lvl2":"Supervised Learning"},"type":"lvl4","url":"/lecture-28-scikit-learn#machine-learning-by-example-wine-classification","position":12},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl4":"Machine Learning by Example: Wine Classification","lvl3":"Classification","lvl2":"Supervised Learning"},"content":"Let’s consider an example of wine classification. We have a dataset of wines with different features such as alcohol content, acidity, etc. We want to classify the wines into different categories based on these features.","type":"content","url":"/lecture-28-scikit-learn#machine-learning-by-example-wine-classification","position":13},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 1: Get the Data","lvl4":"Machine Learning by Example: Wine Classification","lvl3":"Classification","lvl2":"Supervised Learning"},"type":"lvl5","url":"/lecture-28-scikit-learn#step-1-get-the-data","position":14},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 1: Get the Data","lvl4":"Machine Learning by Example: Wine Classification","lvl3":"Classification","lvl2":"Supervised Learning"},"content":"First, we need to load the dataset. We can use the load_wine function from sklearn.datasets to load the wine dataset.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_wine\n\ndata = load_wine()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf['target'] = data.target\ndf.head()\n\nWine Recognition Dataset\n\nThe wine recognition dataset is a classic dataset for classification. It contains 178 samples of wine with 13 features each. The features are the chemical composition of the wines, and the target is the class of the wine (0, 1, or 2). You can find more information about the dataset \n\nhere.\n\nYour Data\n\nIf pandas can read your data, you can swap out the load_wine function with pd.read_csv or any other method you prefer to load your data.","type":"content","url":"/lecture-28-scikit-learn#step-1-get-the-data","position":15},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 2: Explore and Visualize the Data","lvl4":"Machine Learning by Example: Wine Classification","lvl3":"Classification","lvl2":"Supervised Learning"},"type":"lvl5","url":"/lecture-28-scikit-learn#step-2-explore-and-visualize-the-data","position":16},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 2: Explore and Visualize the Data","lvl4":"Machine Learning by Example: Wine Classification","lvl3":"Classification","lvl2":"Supervised Learning"},"content":"Next, we need to explore and visualize the data to understand its structure and characteristics. We can use pandas to explore the data and seaborn to visualize it.\n\ndf.describe()\n\ndf['target'].value_counts()\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.pairplot(df, hue='target')\n\nplt.show()\n\n","type":"content","url":"/lecture-28-scikit-learn#step-2-explore-and-visualize-the-data","position":17},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 3: Preprocess the Data","lvl4":"Machine Learning by Example: Wine Classification","lvl3":"Classification","lvl2":"Supervised Learning"},"type":"lvl5","url":"/lecture-28-scikit-learn#step-3-preprocess-the-data","position":18},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 3: Preprocess the Data","lvl4":"Machine Learning by Example: Wine Classification","lvl3":"Classification","lvl2":"Supervised Learning"},"content":"Before training the model, we need to preprocess the data. This involves splitting the data into input features and output labels, normalizing the input features, and splitting the data into training and testing sets.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX = df.drop('target', axis=1)\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nWhy Split the Data?\n\nSplitting the data into training and testing sets allows us to train the model on one set and evaluate it on another set. This helps us assess the model’s performance on unseen data. You can also use \n\ncross-validation to evaluate the model’s performance more robustly.\n\nWhy Scale or “Standardize” the Data?\n\nStandardizing the data (e.g., using StandardScaler) ensures that each feature has a mean of 0 and a standard deviation of 1. This can help improve the performance of some machine learning algorithms, especially those that are sensitive to the scale of the input features.","type":"content","url":"/lecture-28-scikit-learn#step-3-preprocess-the-data","position":19},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 4: Train a Model","lvl4":"Machine Learning by Example: Wine Classification","lvl3":"Classification","lvl2":"Supervised Learning"},"type":"lvl5","url":"/lecture-28-scikit-learn#step-4-train-a-model","position":20},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 4: Train a Model","lvl4":"Machine Learning by Example: Wine Classification","lvl3":"Classification","lvl2":"Supervised Learning"},"content":"Now that we have preprocessed the data, we can train a classification model. We will use the LogisticRegression and RandomForestClassifier models from scikit-learn.\n\nLogistic Regression\n\nLogistic regression is a linear model used for binary classification. It models the probability of the output being in a particular category. You can find more information about logistic regression \n\nhere.\n\nCanley, CC BY-SA 4.0 \n\nhttps://​creativecommons​.org​/licenses​/by​-sa​/4.0, via Wikimedia Commons\n\nRandom Forest\n\nRandom forests are an ensemble learning method that builds multiple decision trees during training and outputs the mode of the classes (i.e., the most frequent class) as the prediction. You can find more information about random forests \n\nhere.\n\nDataCamp\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\n# Train the Logistic Regression model\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\n\n# Train the Random Forest model\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\n\n# Plot the confusion matrix\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\nConfusionMatrixDisplay.from_estimator(lr, X_test, y_test, ax=ax[0])\nax[0].set_title('Logistic Regression')\nConfusionMatrixDisplay.from_estimator(rf, X_test, y_test, ax=ax[1])\nax[1].set_title('Random Forest')\n\nplt.show()\n\nConfusion Matrix\n\nA confusion matrix is a table that is often used to describe the performance of a classification model. It shows the number of true positives, true negatives, false positives, and false negatives. You can find more information about confusion matrices \n\nhere.","type":"content","url":"/lecture-28-scikit-learn#step-4-train-a-model","position":21},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 5: Evaluate the Model","lvl4":"Machine Learning by Example: Wine Classification","lvl3":"Classification","lvl2":"Supervised Learning"},"type":"lvl5","url":"/lecture-28-scikit-learn#step-5-evaluate-the-model","position":22},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 5: Evaluate the Model","lvl4":"Machine Learning by Example: Wine Classification","lvl3":"Classification","lvl2":"Supervised Learning"},"content":"Finally, we need to evaluate the model’s performance. We can use metrics such as accuracy, precision, recall, and F1 score to evaluate the model.\n\nfrom sklearn.metrics import classification_report\n\nprint('Logistic Regression:')\nprint(classification_report(y_test, y_pred_lr))\n\nprint('Random Forest:')\nprint(classification_report(y_test, y_pred_rf))\n\nClassification Report\n\nA classification report shows the precision, recall, F1 score, and support for each class in the classification model. Precision is the ratio of true positives to the sum of true positives and false positives\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n\nRecall is the ratio of true positives to the sum of true positives and false negatives\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n\nThe F1 score is the harmonic mean of precision and recallF1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\nSupport is the number of occurrences of each class in the dataset.","type":"content","url":"/lecture-28-scikit-learn#step-5-evaluate-the-model","position":23},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 6: Plot and Interpret the Coefficients","lvl4":"Machine Learning by Example: Wine Classification","lvl3":"Classification","lvl2":"Supervised Learning"},"type":"lvl5","url":"/lecture-28-scikit-learn#step-6-plot-and-interpret-the-coefficients","position":24},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 6: Plot and Interpret the Coefficients","lvl4":"Machine Learning by Example: Wine Classification","lvl3":"Classification","lvl2":"Supervised Learning"},"content":"For the logistic regression model, we can plot and interpret the coefficients to understand the importance of each feature in the classification.\n\nimport numpy as np\n\n# Ensure feature names are a NumPy array\nfeature_names = np.array(data.feature_names)\n\n# Sort the coefficients\nsorted_idx = lr.coef_[0].argsort()\n\n# Plot the coefficients\nplt.figure(figsize=(12, 6))\nplt.barh(feature_names[sorted_idx], lr.coef_[0][sorted_idx])\nplt.xlabel('Coefficient Value')\nplt.ylabel('Feature Name')\nplt.title('Logistic Regression Coefficients')\nplt.show()\n\nThe plot above shows the coefficients of the logistic regression model. The features with the largest coefficients (in absolute value) are the most important for the classification. The sign of the coefficient indicates the direction of the relationship between the feature and the target. The two features with the largest coefficients are proline and alcalinity_of_ash.\n\nproline is the amount of proline in the wine. Proline is an amino acid that is found in high concentrations in red wines. The coefficient for proline is positive, indicating that wines with higher proline content are more likely to be classified as class 2.\n\n\n\nFigure 1:The chemical structure of proline. By Qohelet12, CC0, via Wikimedia Commons\n\nalcalinity_of_ash is the amount of ash in the wine. Ash is the inorganic residue remaining after the water and organic matter have been removed by heating. The coefficient for alcalinity_of_ash is negative, indicating that wines with lower ash content are more likely to be classified as class 2.\n\nYour Data\n\nYou can swap out the wine dataset with your own dataset to perform classification on your data. Make sure to preprocess the data, train the model, and evaluate the model as shown in the example above.","type":"content","url":"/lecture-28-scikit-learn#step-6-plot-and-interpret-the-coefficients","position":25},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl3":"Regression","lvl2":"Supervised Learning"},"type":"lvl3","url":"/lecture-28-scikit-learn#regression","position":26},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl3":"Regression","lvl2":"Supervised Learning"},"content":"In regression, we have a dataset consisting of input features and continuous output values. The goal is to learn a mapping from the input features to the output values. We can use the scikit-learn library to perform regression.","type":"content","url":"/lecture-28-scikit-learn#regression","position":27},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"type":"lvl4","url":"/lecture-28-scikit-learn#machine-learning-by-example-oxygen-vacancy-formation-energy-prediction","position":28},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"content":"Let’s consider an example of regression for predicting the oxygen vacancy formation energy in materials. We have an \n\nExcel file containing the features of the materials and the oxygen vacancy formation energy. We want to train a regression model to predict the oxygen vacancy formation energy based on the features of the materials.","type":"content","url":"/lecture-28-scikit-learn#machine-learning-by-example-oxygen-vacancy-formation-energy-prediction","position":29},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 1: Use pip or conda to Install openpyxl","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"type":"lvl5","url":"/lecture-28-scikit-learn#step-1-use-pip-or-conda-to-install-openpyxl","position":30},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 1: Use pip or conda to Install openpyxl","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"content":"Before we can read the Excel file, we need to install the openpyxl library. You can install it using pip:pip install openpyxl","type":"content","url":"/lecture-28-scikit-learn#step-1-use-pip-or-conda-to-install-openpyxl","position":31},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 2: Get the Data","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"type":"lvl5","url":"/lecture-28-scikit-learn#step-2-get-the-data","position":32},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 2: Get the Data","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"content":"First, we need to load the dataset. We can use the pd.read_excel function from pandas to load the Excel file.\n\ndf = pd.read_excel('ovfe-deml.xlsx')\ndf.head()\n\nOxygen Vacancy Formation Energy Dataset\n\nThe oxygen vacancy formation energy dataset contains the features of materials and the oxygen vacancy formation energy. The features include the crystal structure (xtal_str), the composition (comp), the standard enthalpy of formation (dHf), the measured and calculated band gaps (Eg_exp, Eg_GW, and Eg_DFTU), the valence band maximum (O2p_min_VBM), the difference between the electronegativity of the cation and anion (dEN), and the energy above the convex hull (Ehull_MP). The energy above the convex hull is a measure of the thermodynamic stability of the material. The higher the energy above the convex hull, the less stable the material. The target is the oxygen vacancy formation energy (OVFE_calc). You can find more information about the dataset \n\nhere.\n\nYour Data\n\nIf pandas can read your data, you can swap out the pd.read_excel function with pd.read_csv or any other method you prefer to load your data.","type":"content","url":"/lecture-28-scikit-learn#step-2-get-the-data","position":33},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 3: Explore and Visualize the Data","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"type":"lvl5","url":"/lecture-28-scikit-learn#step-3-explore-and-visualize-the-data","position":34},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 3: Explore and Visualize the Data","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"content":"Next, we need to explore and visualize the data to understand its structure and characteristics. We can use pandas to explore the data and seaborn to visualize it.","type":"content","url":"/lecture-28-scikit-learn#step-3-explore-and-visualize-the-data","position":35},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl6":"Missing Values","lvl5":"Step 3: Explore and Visualize the Data","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"type":"lvl6","url":"/lecture-28-scikit-learn#missing-values","position":36},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl6":"Missing Values","lvl5":"Step 3: Explore and Visualize the Data","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"content":"Before exploring the data, we need to check for missing values and handle them if necessary.\n\ndf.isnull().sum()\n\nThe measured band gap (Eg_exp) and the energy above the convex hull (Ehull_MP) have nine and two missing values, respectively. We can drop these columns or impute the missing values with the mean, median, or mode of the column. Let’s drop the columns for now.\n\ndf.drop(['Eg_exp', 'Ehull_MP'], axis=1, inplace=True)\ndf.head()\n\nMissing Values\n\nMissing values can affect the performance of machine learning models. It is important to handle missing values appropriately by imputing them or dropping the corresponding rows or columns. You can find more information about handling missing values \n\nhere.","type":"content","url":"/lecture-28-scikit-learn#missing-values","position":37},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl6":"Data Exploration","lvl5":"Step 3: Explore and Visualize the Data","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"type":"lvl6","url":"/lecture-28-scikit-learn#data-exploration","position":38},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl6":"Data Exploration","lvl5":"Step 3: Explore and Visualize the Data","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"content":"Now, let’s explore the data to understand its structure and characteristics.\n\ndf.describe()\n\nsns.pairplot(df, kind='reg', diag_kind='kde')\nplt.show()\n\n","type":"content","url":"/lecture-28-scikit-learn#data-exploration","position":39},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 4: Preprocess the Data","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"type":"lvl5","url":"/lecture-28-scikit-learn#step-4-preprocess-the-data","position":40},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 4: Preprocess the Data","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"content":"Before training the model, we need to preprocess the data. This involves splitting the data into input features and output labels, normalizing the input features, and splitting the data into training and testing sets.\n\nX = df.drop(['xtal_str', 'comp', 'OVFE_calc', 'OVFE_reg_GW', 'OVFE_reg_DFTU'], axis=1)\ny = df['OVFE_calc']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n","type":"content","url":"/lecture-28-scikit-learn#step-4-preprocess-the-data","position":41},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 5: Train a Model","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"type":"lvl5","url":"/lecture-28-scikit-learn#step-5-train-a-model","position":42},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 5: Train a Model","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"content":"Now that we have preprocessed the data, we can train a regression model. We will use the RidgeCV and Perceptron models from scikit-learn.\n\nRidge regression\n\nRidge regression is a linear model used for regression. It is similar to ordinary least squares regression, but it adds a penalty term to the loss function to prevent overfitting by shrinking the coefficients. You can find more information about ridge regression \n\nhere.\n\nMulti-layer Perceptron (MLP)\n\nThe multi-layer perceptron (MLP) is a type of artificial neural network that consists of multiple layers of nodes. It is a powerful model that can learn complex patterns in the data. You can find more information about MLPs \n\nhere.\n\n\n\nFigure 2:A multi-layer perceptron (MLP) neural network. \n\nSource\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.neural_network import MLPRegressor\n\n# Train the Ridge regression model\nridge = RidgeCV()\nridge.fit(X_train, y_train)\ny_pred_ridge = ridge.predict(X_test)\n\n# Train the MLPRegressor model\nmlp = MLPRegressor(\n    hidden_layer_sizes=(100, 50),\n    activation='relu',\n    solver='adam',\n    max_iter=1000,\n    random_state=42\n)\nmlp.fit(X_train, y_train)\ny_pred_mlp = mlp.predict(X_test)\n\n# Plot the predicted vs. actual values\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test, y_pred_ridge, label='Ridge')\nplt.scatter(y_test, y_pred_mlp, label='MLP')\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--')\nplt.xlabel('Actual Oxygen Vacancy Formation Energy (eV)')\nplt.ylabel('Predicted Oxygen Vacancy Formation Energy (eV)')\nplt.legend()\nplt.show()\n\n","type":"content","url":"/lecture-28-scikit-learn#step-5-train-a-model","position":43},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 6: Evaluate the Model","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"type":"lvl5","url":"/lecture-28-scikit-learn#step-6-evaluate-the-model","position":44},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 6: Evaluate the Model","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"content":"Finally, we need to evaluate the model’s performance. We can use metrics such as mean squared error (MSE), mean absolute error (MAE), and R^2 score to evaluate the model.\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nprint('Ridge Regression:')\nprint('MSE:', mean_squared_error(y_test, y_pred_ridge))\nprint('MAE:', mean_absolute_error(y_test, y_pred_ridge))\nprint('R^2:', r2_score(y_test, y_pred_ridge))\n\nprint('MLPRegressor:')\nprint('MSE:', mean_squared_error(y_test, y_pred_mlp))\nprint('MAE:', mean_absolute_error(y_test, y_pred_mlp))\nprint('R^2:', r2_score(y_test, y_pred_mlp))\n\nRegression Metrics\n\nMean squared error (MSE) is the average of the squared differences between the predicted and actual values\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\nMean absolute error (MAE) is the average of the absolute differences between the predicted and actual values\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n\nThe R^2 score is the coefficient of determination and represents the proportion of the variance in the dependent variable that is predictable from the independent variablesR^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n\nYour Data\n\nYou can swap out the oxygen vacancy formation energy dataset with your own dataset to perform regression on your data. Make sure to preprocess the data, train the model, and evaluate the model as shown in the example above.","type":"content","url":"/lecture-28-scikit-learn#step-6-evaluate-the-model","position":45},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 7: Plot and Interpret the Coefficients","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"type":"lvl5","url":"/lecture-28-scikit-learn#step-7-plot-and-interpret-the-coefficients","position":46},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl5":"Step 7: Plot and Interpret the Coefficients","lvl4":"Machine Learning by Example: Oxygen Vacancy Formation Energy Prediction","lvl3":"Regression","lvl2":"Supervised Learning"},"content":"For the Ridge regression model, we can plot and interpret the coefficients to understand the importance of each feature in the regression.\n\n# Ensure feature names are a NumPy array\nfeature_names = np.array(X.columns)\n\n# Sort the coefficients\nsorted_idx = ridge.coef_.argsort()\n\n# Plot the coefficients\nplt.figure(figsize=(12, 6))\nplt.barh(feature_names[sorted_idx], ridge.coef_[sorted_idx])\nplt.xlabel('Coefficient Value')\nplt.ylabel('Feature Name')\nplt.title('Ridge Regression Coefficients')\nplt.show()\n\nThe plot above shows the coefficients of the Ridge regression model. The features with the largest coefficients (in absolute value) are the most important for the regression. The sign of the coefficient indicates the direction of the relationship between the feature and the target. The feature with the largest coefficient is dHf.\n\nInterpretation\n\ndHf is the standard enthalpy of formation of the material. The formation reaction of a metal oxide (MO_x) can be represented as\\text{M} + \\frac{x}{2} \\text{O}_2 \\rightarrow \\text{MO}_x\n\nThis reaction can be thought of as an oxidation reaction, where the metal is oxidized to form the metal oxide, and its standard enthalpy change can be thought of as an enthalpy of oxidation. Since oxygen vacancy formation is a reduction reaction, the oxygen vacancy formation energy is inversely related to the standard enthalpy of formation. The coefficient for dHf is negative, indicating that materials with lower standard enthalpies of formation have higher oxygen vacancy formation energies.\n\nMaterials Design\n\nThis relationship is exciting because it suggests that we can predict the oxygen vacancy formation energy of metal oxides, which is challenging to measure experimentally, based on their standard enthalpies of formation, which are readily available from databases like the \n\nMaterials Project, \n\nAFLOW, and \n\nOQMD. This prediction can help guide the design of materials with desired properties for applications like solid oxide fuel cells, oxygen separation membranes, catalysts, and \n\nthermochemical water and carbon dioxide splitting.","type":"content","url":"/lecture-28-scikit-learn#step-7-plot-and-interpret-the-coefficients","position":47},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl3":"Summary","lvl2":"Supervised Learning"},"type":"lvl3","url":"/lecture-28-scikit-learn#summary","position":48},{"hierarchy":{"lvl1":"Chapter 26: scikit-learn","lvl3":"Summary","lvl2":"Supervised Learning"},"content":"In this lecture, we learned how to use scikit-learn to perform supervised learning. We covered classification and regression and trained models on the wine recognition dataset and the oxygen vacancy formation energy dataset. We explored the data, preprocessed it, trained the models, evaluated the models, and interpreted the results. We used logistic regression and random forests for classification and ridge regression and MLPRegressor for regression. We also visualized the data, plotted the confusion matrix, and interpreted the coefficients.","type":"content","url":"/lecture-28-scikit-learn#summary","position":49},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling"},"type":"lvl1","url":"/lecture-29-nested-sampling","position":0},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling"},"content":"In this lecture, we will discuss the nested sampling algorithm. Nested sampling is a method that can be used to estimate the partition function of a chemical system.","type":"content","url":"/lecture-29-nested-sampling","position":1},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Lecture Objectives"},"type":"lvl2","url":"/lecture-29-nested-sampling#lecture-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Lecture Objectives"},"content":"By the end of this lecture, you will be able to:\n\nExplain the nested sampling algorithm.\n\nImplement the nested sampling algorithm to estimate the partition function of a simple chemical system.","type":"content","url":"/lecture-29-nested-sampling#lecture-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Partition Function"},"type":"lvl2","url":"/lecture-29-nested-sampling#partition-function","position":4},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Partition Function"},"content":"In the lecture on statistical thermodynamics, we discussed the partition function of a chemical system. The partition function is a sum over all possible states of the system, weighted by the Boltzmann factor. The partition function of a classical system is given by:Z = \\int e^{-\\beta H(\\mathbf{q}, \\mathbf{p})} d\\mathbf{q} d\\mathbf{p}\n\nwhere H(\\mathbf{q}, \\mathbf{p}) is the Hamiltonian of the system, \\mathbf{q} are the positions of the particles, \\mathbf{p} are the momenta of the particles, and \\beta = 1/kT is the inverse temperature.","type":"content","url":"/lecture-29-nested-sampling#partition-function","position":5},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Configuration Integral"},"type":"lvl2","url":"/lecture-29-nested-sampling#configuration-integral","position":6},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Configuration Integral"},"content":"The integral over the momenta can be performed analytically, leading to:Z = \\frac{1}{N! h^{3N}} \\left( \\frac{2\\pi m}{\\beta} \\right)^{3N/2} \\int e^{-\\beta V(\\mathbf{q})} d\\mathbf{q}\n\nwhere N is the number of particles, h is the Planck constant, m is the mass of the particles, and V(\\mathbf{q}) is the potential energy of the system. The integral over the positions is known as the configuration integral.","type":"content","url":"/lecture-29-nested-sampling#configuration-integral","position":7},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Nested Sampling"},"type":"lvl2","url":"/lecture-29-nested-sampling#nested-sampling","position":8},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Nested Sampling"},"content":"The configuration integral is a high-dimensional integral, which can be difficult to compute. The basic idea behind nested sampling is to transform the high-dimensional integral into a one-dimensional integral, which can be computed using Monte Carlo methods:Z_{\\text{config}} = \\int_{-\\infty}^{\\infty} e^{-\\beta E} g(E) dE\n\nwhere E is the energy of the system, and g(E) is the density of states at energy E. Z_{\\text{config}} can also be written as:Z_{\\text{config}} = \\int_{0}^{\\infty} e^{-\\beta E(\\chi)} d\\chi \\approx \\sum_{i=1}^{M} e^{-\\beta E(\\chi_i)} \\Delta \\chi_i\n\nwhere \\chi is the cumulative density of states, and M is the number of points used to estimate the integral.\n\nNote\n\nThe cumulative density of states \\chi is defined as:\\chi(E) = \\int_{0}^{E} g(E') dE'\n\nThe cumulative density of states is a monotonically increasing function of energy because as the energy increases, the number of accessible states increases.\n\nConsider an anharmonic oscillator. The number of accessible states is lower at lower energies because the oscillator does not have enough energy to sample past the potential energy minimum. As the energy increases, the number of accessible states increases because the oscillator can sample past the potential energy minimum. At very high energies, the number of accessible states increases more quickly because the oscillator can sample dissociation.","type":"content","url":"/lecture-29-nested-sampling#nested-sampling","position":9},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Nested Sampling Algorithm"},"type":"lvl2","url":"/lecture-29-nested-sampling#nested-sampling-algorithm","position":10},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Nested Sampling Algorithm"},"content":"Note\n\nAn infinite cumulative density of states corresponds to a system that can sample all possible states. A cumulative density of states of zero corresponds to a system that is stuck in a single state.\n\nThe nested sampling algorithm is a Monte Carlo method that can be used to estimate the configuration integral by carrying out the sum over the cumulative density of states from \\infty to 0. The algorithm proceeds as follows:\n\nCreate an initial set of K configurations that uniformly sample the configuration space. Each configuration is called a “live point” or “walker”. The set of live points or walkers is called the “live set”.\n\nCompute the energy of each live point and sort the live points by energy.\n\nCull the live point with the highest energy and replace it with a new live point that is sampled from the uniform distribution bounded by the energy of the culled live point.\n\nRepeat steps 2 and 3 until the change in the energy of the culled live point is less than a specified tolerance.\n\nThe partition function can be estimated as:Z = \\sum_{i=1}^{M} e^{-\\beta E(\\chi_i)} \\Delta \\chi_i\n\nwhere M is the number of iterations of the nested sampling algorithm. \\Delta \\chi_i is the difference in the cumulative density of states between the i-th and (i-1)-th iteration:\\Delta \\chi_i = \\chi_{i-1} - \\chi_i = \\frac{1}{K + 1} \\left( \\frac{K}{K+1} \\right)^{i}","type":"content","url":"/lecture-29-nested-sampling#nested-sampling-algorithm","position":11},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Example: Harmonic Oscillator"},"type":"lvl2","url":"/lecture-29-nested-sampling#example-harmonic-oscillator","position":12},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Example: Harmonic Oscillator"},"content":"Let’s consider a simple example of a harmonic oscillator. The potential energy of a harmonic oscillator is given by:V(x) = \\frac{1}{2} k x^2\n\nwhere k is the force constant of the oscillator. The energy of the oscillator is given by:E = \\frac{1}{2} k x^2\n\nLet’s implement the nested sampling algorithm to estimate the partition function of a harmonic oscillator.\n\nFirst, we need to import the necessary libraries:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nNext, we define the potential energy of the harmonic oscillator:\n\ndef potential_energy(x, k):\n    return 0.5 * k * x**2\n\nWe also define the number of live points and the force constant of the oscillator:\n\nK = 100\nk = 1.0  # force constant of the oscillator in eV/A^2\n\nWe create an initial set of live points that uniformly sample the configuration space:\n\nx_max = 1.0\nlive_points = np.random.uniform(-x_max, x_max, K)\n\nWe carry out the nested sampling algorithm:\n\nn_iterations = 1000\nenergies = potential_energy(live_points, k)\nenergies_of_culled_live_points = []\n\nfor i in range(n_iterations):\n    # Get the index of the live point with the highest energy\n    idx = np.argmax(energies)\n\n    # Append the energy of the culled live point to the list\n    energies_of_culled_live_points.append(energies[idx])\n\n    # Replace the culled live point with a new live point sampled from the uniform distribution bounded by the energy of the culled live point\n    while True:\n        new_live_point = np.random.uniform(-x_max, x_max)\n        new_energy = potential_energy(new_live_point, k)\n        if new_energy < energies[idx]:\n            live_points[idx] = new_live_point\n            energies[idx] = new_energy\n            break\n\n","type":"content","url":"/lecture-29-nested-sampling#example-harmonic-oscillator","position":13},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Analysis"},"type":"lvl2","url":"/lecture-29-nested-sampling#analysis","position":14},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Analysis"},"content":"Let’s plot the energy of the culled live points as a function of the iteration number:\n\nplt.plot(energies_of_culled_live_points, 'o-')\nplt.xlabel('Iteration')\nplt.ylabel('Energy of Culled Live Point (eV)')\nplt.show()\n\nThe plot shows that the energy of the culled live points decreases with the iteration number.\n\nWe can estimate the partition function of the harmonic oscillator as a function of temperature:\n\nk_B = 8.617333262E-5  # Boltzmann constant in eV/K\n\ndef partition_function(energies, beta, chi_0):\n    Z = 0.0\n    for i, energy in enumerate(energies):\n        delta_chi = (1 / (K + 1)) * ((K / (K + 1)) ** i)\n        Z += np.exp(-beta * energy) * delta_chi\n    return Z\n\ntemperatures = np.linspace(0.1, 10.0, 100)\npartition_functions = []\nchi_0 = 2.0 * x_max\n\nfor T in temperatures:\n    beta = 1 / (k_B * T)  # Boltzmann constant in eV/K\n    partition_functions.append(partition_function(energies_of_culled_live_points, beta, chi_0) * 2)\n\nLet’s plot the partition function of the harmonic oscillator as a function of temperature and compare it to the exact partition function:\n\nfrom scipy.special import erf\n\ndef exact_partition_function(temperature, limit):\n    return np.sqrt(2 * np.pi * k_B * temperature) * erf(limit / np.sqrt(2 * k_B * temperature))\n\nexact_partition_functions = [exact_partition_function(T, x_max) for T in temperatures]\n\nplt.plot(temperatures, partition_functions, label='Nested Sampling')\nplt.plot(temperatures, exact_partition_functions, label='Exact')\nplt.xlabel('Temperature (K)')\nplt.ylabel('Partition Function')\nplt.legend()\nplt.show()\n\nThe plot shows that the partition function estimated using the nested sampling algorithm is in good agreement with the exact partition function. Since the partition function is contains all the information needed to calculate the thermodynamic properties of the system, the nested sampling algorithm can be used to estimate the thermodynamic properties of a chemical system.","type":"content","url":"/lecture-29-nested-sampling#analysis","position":15},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Summary"},"type":"lvl2","url":"/lecture-29-nested-sampling#summary","position":16},{"hierarchy":{"lvl1":"Chapter 27: Nested Sampling","lvl2":"Summary"},"content":"In this lecture, we discussed the nested sampling algorithm. Nested sampling is a method that can be used to estimate the partition function and thermodynamic properties of a chemical system.","type":"content","url":"/lecture-29-nested-sampling#summary","position":17}]}